# Story 7.4: LLM-Driven Fitness Scoring Criteria

## Status

**Draft**

## Story

**As a** user,
**I want** Claude to dynamically identify appropriate fitness scoring criteria,
**so that** scoring adapts to my specific research context rather than using rigid rules.

## Acceptance Criteria

1. LLM analyzes user research profile and available data (FR25)
2. Scoring criteria dynamically identified (not hardcoded)
3. Suggested criteria include but not limited to: research alignment, publication recency, journal reputation, authorship patterns, position availability, website freshness, collaboration activity
4. Criteria weighted based on relevance to user's goals
5. Scoring methodology explained transparently
6. User can review criteria before scoring applied (shown in logs)

## Tasks / Subtasks

- [ ] **Task 1: Prompt LLM to Identify Criteria** (AC: 1, 2)
  - [ ] Provide user profile + available data to LLM
  - [ ] Request scoring criteria identification
  - [ ] Request weighting suggestions

- [ ] **Task 2: Parse LLM Response** (AC: 3, 4)
  - [ ] Extract criteria list
  - [ ] Extract weights
  - [ ] Validate criteria are scoreable

- [ ] **Task 3: Log Scoring Methodology** (AC: 5, 6)
  - [ ] Log identified criteria and weights
  - [ ] Display to user for review

- [ ] **Task 4: Create Scoring Criteria Model** (AC: 1)
  - [ ] Create `src/models/scoring_criteria.py`
  - [ ] Fields:
    ```python
    class ScoringCriteria(BaseModel):
        criteria: list[dict]  # [{name, weight, description}]
        methodology: str
    ```

## Dev Notes

### Source Tree Location
- Create: `src/agents/fitness_scorer.py`
- Create: `src/models/scoring_criteria.py`

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |
