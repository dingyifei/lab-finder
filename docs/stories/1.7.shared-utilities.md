# Story 1.7: Shared Utilities Implementation

## Status

**Draft**

## Story

**As a** developer,
**I want** core shared utility modules (checkpoint_manager, llm_helpers, progress_tracker, logger) implemented before phase agents,
**so that** all subsequent agent implementations can use consistent checkpoint handling, LLM prompting, progress tracking, and logging patterns.

## Acceptance Criteria

1. `src/utils/checkpoint_manager.py` created with save_batch, load_batches, get_resume_point, mark_phase_complete functions
2. `src/utils/llm_helpers.py` created with centralized LLM prompt templates for common operations
3. `src/utils/progress_tracker.py` created wrapping rich library for consistent progress bars
4. `src/utils/logger.py` created with structlog configuration including correlation IDs and phase context
5. All utilities have type hints and pass mypy type checking
6. Unit tests written for all utility modules (70% coverage minimum)
7. Example usage documented in each utility module's docstring

## Tasks / Subtasks

- [ ] **Task 1: Implement Checkpoint Manager** (AC: 1)
  - [ ] Create `src/utils/checkpoint_manager.py`
  - [ ] Implement `save_batch(phase: str, batch_id: int, data: list[BaseModel]) -> None`
    - Write JSONL to `checkpoints/phase-{N}-batch-{M}.jsonl`
    - Use jsonlines library for streaming write
    - Serialize Pydantic models with `.model_dump()`
  - [ ] Implement `load_batches(phase: str) -> list[dict]`
    - Read all `checkpoints/phase-{N}-batch-*.jsonl` files
    - Return aggregated list of records
    - Deduplicate by ID (later batches override earlier)
  - [ ] Implement `get_resume_point(phase: str) -> int`
    - Scan checkpoint directory for completed batches
    - Return first missing batch number
    - Return 0 if no batches exist
  - [ ] Implement `mark_phase_complete(phase: str) -> None`
    - Write entry to `checkpoints/_phase_completion_markers.json`
    - Track which phases are fully complete
  - [ ] Add error handling for corrupted checkpoint files
  - [ ] Add type hints for all functions

- [ ] **Task 2: Implement LLM Helpers** (AC: 2)
  - [ ] Create `src/utils/llm_helpers.py`
  - [ ] Define prompt template for department relevance filtering
    - Template: "Given user research interests: {interests}, is {department_name} relevant? Respond with Yes/No and reasoning."
  - [ ] Define prompt template for professor filtering
    - Template: "Given user profile: {profile}, does professor {name} with research areas {areas} match? Provide confidence score 0-100 and reasoning."
  - [ ] Define prompt template for LinkedIn profile matching
    - Template: "Does LinkedIn profile {profile_data} match lab member {member_name} at {university}? Confidence score 0-100 and reasoning."
  - [ ] Define prompt template for LLM name matching
    - Template: "Are these the same person? Name 1: {name1}, Name 2: {name2}, Context: {context}. Yes/No with confidence."
  - [ ] Define prompt template for abstract relevance scoring
    - Template: "Rate relevance of this paper abstract to user interests {interests}. Abstract: {abstract}. Score 0-100 with reasoning."
  - [ ] Implement `call_llm_with_retry(prompt: str, max_retries: int = 3) -> str`
    - Use tenacity for retry with exponential backoff
    - Log all LLM calls at DEBUG level
    - Return LLM response text
  - [ ] Add type hints and docstrings

- [ ] **Task 3: Implement Progress Tracker** (AC: 3)
  - [ ] Create `src/utils/progress_tracker.py`
  - [ ] Implement `ProgressTracker` class wrapping rich.Progress
  - [ ] Implement `start_phase(phase_name: str, total_items: int) -> None`
    - Initialize rich progress bar
    - Display: "Phase {N}: {name} [0/{total}]"
  - [ ] Implement `update(completed: int) -> None`
    - Increment progress bar
    - Update ETA calculation
  - [ ] Implement `complete_phase() -> None`
    - Mark progress bar as complete
    - Display summary: "Phase {N} complete: {total} items processed"
  - [ ] Support batch-level progress updates
    - Display: "Batch 3/8: Processing professors 21-40"
  - [ ] Add type hints and docstrings

- [ ] **Task 4: Implement Structured Logger** (AC: 4)
  - [ ] Create `src/utils/logger.py`
  - [ ] Configure structlog with JSON output format
  - [ ] Implement `get_logger(correlation_id: str, phase: str, component: str) -> BoundLogger`
    - Bind correlation_id, phase, component to logger context
    - Generate UUID for correlation_id if not provided
  - [ ] Configure log levels:
    - DEBUG: LLM prompts/responses, detailed execution flow
    - INFO: Phase progress, batch completion, checkpoints saved
    - WARNING: Missing data, skipped items, low-confidence matches
    - ERROR: Exceptions, failed retries, resource access failures
    - CRITICAL: Unrecoverable failures requiring user intervention
  - [ ] Configure log output to `logs/lab-finder.log`
  - [ ] Add timestamp, level, message to all log entries
  - [ ] Mask credentials in log output (never log passwords, API keys)
  - [ ] Add type hints and docstrings

- [ ] **Task 5: Add Type Hints and Validation** (AC: 5)
  - [ ] Add type hints to all functions in all utility modules
  - [ ] Run mypy on `src/utils/` and fix all type errors
  - [ ] Add Pydantic models for complex data structures if needed
  - [ ] Verify mypy passes with no errors: `mypy src/utils/`

- [ ] **Task 6: Write Unit Tests** (AC: 6)
  - [ ] Create `tests/unit/test_checkpoint_manager.py`
    - Test save_batch writes JSONL correctly
    - Test load_batches aggregates multiple files
    - Test get_resume_point identifies missing batches
    - Test deduplication by ID
    - Use tmp_path fixture for temporary directories
  - [ ] Create `tests/unit/test_llm_helpers.py`
    - Test prompt template formatting
    - Mock LLM calls with pytest-mock
    - Test retry logic on failures
  - [ ] Create `tests/unit/test_progress_tracker.py`
    - Test progress bar initialization
    - Test progress updates
    - Mock rich.Progress to avoid console output in tests
  - [ ] Create `tests/unit/test_logger.py`
    - Test logger configuration
    - Test context binding (correlation_id, phase, component)
    - Test log levels
    - Verify JSON output format
  - [ ] Run pytest with coverage: `pytest --cov=src/utils --cov-report=term-missing`
  - [ ] Verify 70% coverage minimum achieved

- [ ] **Task 7: Document Usage Examples** (AC: 7)
  - [ ] Add docstrings with examples to all utility modules
  - [ ] Include usage examples in module-level docstrings
  - [ ] Document parameters, return types, and exceptions
  - [ ] Add inline comments for complex logic

## Dev Notes

### Relevant Architecture Information

**Source Tree Location:**
```
src/utils/                       # Shared utilities
├── __init__.py
├── checkpoint_manager.py   # Checkpoint save/load logic
├── progress_tracker.py     # Progress bar wrapper (rich)
├── mcp_client.py           # MCP server client helpers (Story 1.5)
├── web_scraper.py          # Web scraping helpers (Later epic)
├── llm_helpers.py          # LLM prompt templates and wrappers
└── logger.py               # Structlog configuration
```

**All utilities will be used by:**
- `checkpoint_manager.py` - Used by ALL phase agents (Epic 2-8)
- `llm_helpers.py` - Used by all agents doing LLM filtering/matching
- `progress_tracker.py` - Used by CLI coordinator and all agents
- `logger.py` - Used by ALL components

**Checkpoint Manager Design (from Architecture):**

**Purpose:** Save and load JSONL checkpoints; manage batch-level resumability

**Key Interfaces:**
- `save_batch(phase: str, batch_id: int, data: list[BaseModel]) -> None` - Write batch to JSONL
- `load_batches(phase: str) -> list[dict]` - Load all completed batches for phase
- `get_resume_point(phase: str) -> int` - Identify first incomplete batch
- `mark_phase_complete(phase: str) -> None` - Write phase completion marker

**Technology Stack:**
- jsonlines 4.0.0 for streaming read/write
- pydantic models with `.model_dump()` serialization

**Checkpoint Directory Structure:**
```
checkpoints/
├── phase-0-validation.json           # Single JSON (not batched)
├── phase-1-departments.jsonl         # Department records
├── phase-2-professors-batch-1.jsonl  # Batch 1 of professors
├── phase-2-professors-batch-2.jsonl  # Batch 2 of professors
├── phase-2-professors-batch-N.jsonl  # Batch N of professors
└── _phase_completion_markers.json    # Tracks which phases are complete
```

**Checkpoint Loading Strategy:**
1. Resumability: CLI Coordinator checks `_phase_completion_markers.json` to determine last completed phase
2. Batch Loading: For incomplete phases, load all completed batches (e.g., `phase-2-professors-batch-*.jsonl`)
3. Memory Management: Load checkpoints into Python lists of Pydantic models for processing
4. Deduplication: When loading batches, deduplicate by ID (later batches override earlier if IDs collide)

**Logging Standards (from Architecture):**

**Library:** structlog 24.1.0
**Format:** JSON (structured logging for parsing)
**Levels:**
- DEBUG: Detailed execution flow, LLM prompts/responses (verbose)
- INFO: Phase progress, batch completion, checkpoints saved
- WARNING: Missing data, skipped items, low-confidence matches
- ERROR: Exceptions, failed retries, resource access failures
- CRITICAL: Unrecoverable failures requiring user intervention

**Required Context:**
- **Correlation ID:** `correlation_id` - Generated per execution run (UUID)
- **Phase Context:** `phase`, `batch_id`, `component` - Identifies pipeline stage
- **User Context:** `university`, `target_department` - User's analysis target (NO PII)

**Example Log Entry:**
```json
{
  "timestamp": "2024-10-06T10:30:45Z",
  "level": "WARNING",
  "correlation_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "phase": "professor_filter",
  "batch_id": 3,
  "component": "professor_filter_agent",
  "message": "Low confidence match for professor",
  "professor_name": "Dr. John Doe",
  "confidence_score": 68.5,
  "university": "Stanford University"
}
```

**Critical Rules (from Coding Standards):**
- **Never use print() for logging:** Always use `structlog` logger. `print()` breaks structured logging.
- **All LLM calls must use llm_helpers module:** Centralize prompt templates in `src/utils/llm_helpers.py`. No inline LLM prompts scattered in code.
- **Checkpoint saves must be atomic:** Use `checkpoint_manager.save_batch()` - never write JSONL files directly.
- **Always type hint function signatures:** Use `-> ReturnType` and parameter hints. MyPy must pass without errors.
- **Correlation IDs in all log statements:** Always bind correlation_id to logger context.

### Testing

**Test File Locations:**
- `tests/unit/test_checkpoint_manager.py`
- `tests/unit/test_llm_helpers.py`
- `tests/unit/test_progress_tracker.py`
- `tests/unit/test_logger.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Coverage Requirement: 70% minimum
- Mock external dependencies (LLM calls, file I/O where appropriate)
- Use pytest fixtures (tmp_path for temporary directories)

**Test Requirements:**
1. Unit tests for all public functions
2. Test edge cases (empty inputs, None values, missing files)
3. Test error handling (corrupted checkpoints, LLM failures)
4. Use AAA pattern (Arrange, Act, Assert)
5. Mock LLM calls with pytest-mock
6. Use tmp_path fixture for checkpoint file tests

**Example Test Pattern:**
```python
def test_save_batch_creates_jsonl_file(tmp_path):
    # Arrange
    checkpoint_manager = CheckpointManager(checkpoint_dir=tmp_path)
    data = [{"id": "1", "name": "Test"}]

    # Act
    checkpoint_manager.save_batch(phase="test", batch_id=1, data=data)

    # Assert
    checkpoint_file = tmp_path / "phase-test-batch-1.jsonl"
    assert checkpoint_file.exists()
    # Verify JSONL content
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
