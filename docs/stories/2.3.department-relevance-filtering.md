# Story 2.3: Department Relevance Filtering

## Status

**Draft**

## Story

**As a** user,
**I want** irrelevant departments filtered out before professor discovery,
**so that** the system doesn't waste time scraping departments with no relevant labs.

## Acceptance Criteria

1. LLM analyzes each department against user's consolidated research profile (FR9)
2. Departments with ANY potential research overlap are included
3. Obviously unrelated departments excluded (e.g., literature for bioengineering student)
4. Reasoning logged for each filtering decision (NFR17)
5. Filtered departments list saved for user review
6. Remaining departments list passed to next epic
7. Progress shows "Filtered X of Y departments"

## Tasks / Subtasks

- [ ] **Task 1: Load User Research Profile** (AC: 1)
  - [ ] Load consolidated profile from `output/user-profile.md` (from Story 1.4)
  - [ ] Extract research interests section
  - [ ] Extract degree program and background
  - [ ] Pass profile to LLM filtering function

- [ ] **Task 2: Implement LLM-Based Relevance Filtering** (AC: 1, 2, 3)
  - [ ] Use `llm_helpers.py` prompt template for department filtering (Story 1.7)
  - [ ] Prompt template:
    ```
    Given user research interests: {interests}
    Degree program: {degree}
    Background: {background}

    Is department "{department_name}" ({school}) relevant for this user's research?

    Guidelines:
    - Include if ANY potential research overlap exists
    - Include interdisciplinary departments
    - Exclude only obviously unrelated departments

    Respond with:
    - Decision: Yes/No
    - Confidence: 0-100
    - Reasoning: Brief explanation
    ```
  - [ ] Call LLM for each department
  - [ ] Parse LLM response (decision, confidence, reasoning)
  - [ ] Apply decision to filter departments

- [ ] **Task 3: Handle Edge Cases & Interdisciplinary Departments** (AC: 2)
  - [ ] Ensure departments with partial overlap are INCLUDED
  - [ ] Handle interdisciplinary departments (e.g., "Bioengineering & CS")
  - [ ] Handle generic departments (e.g., "Graduate Studies") - include by default
  - [ ] Handle ambiguous department names - favor inclusion over exclusion
  - [ ] Log edge case decisions separately for user review

- [ ] **Task 4: Update Department Models with Relevance Flags** (AC: 1, 6)
  - [ ] Add `is_relevant: bool` field to Department model (from Story 2.1)
  - [ ] Add `relevance_reasoning: str` field to Department model
  - [ ] Set fields based on LLM filtering decision
  - [ ] Preserve all departments (both relevant and filtered) in checkpoint

- [ ] **Task 5: Log All Filtering Decisions** (AC: 4)
  - [ ] Use structured logger from Story 1.7
  - [ ] Log format:
    ```json
    {
      "component": "department_filter",
      "department_id": "dept-001",
      "department_name": "Computer Science",
      "school": "Engineering",
      "decision": "include",
      "confidence": 95,
      "reasoning": "Direct match with user's AI/ML interests"
    }
    ```
  - [ ] Log all decisions at INFO level
  - [ ] Log edge cases at WARNING level
  - [ ] Ensure correlation_id in all logs

- [ ] **Task 6: Save Filtered Departments for User Review** (AC: 5)
  - [ ] Create `output/filtered-departments.md` report
  - [ ] List all EXCLUDED departments with reasoning
  - [ ] Format as markdown table:
    | Department | School | Reason | Confidence |
  - [ ] Include summary: "Filtered out X of Y departments"
  - [ ] Provide instructions: "Review this list to ensure no relevant departments were excluded"

- [ ] **Task 7: Save Relevant Departments to Checkpoint** (AC: 6)
  - [ ] Filter Department list to only `is_relevant == True`
  - [ ] Save to `checkpoints/phase-1-relevant-departments.jsonl`
  - [ ] Use checkpoint_manager.save_batch()
  - [ ] This becomes input for Epic 3 professor discovery
  - [ ] Log count: "X relevant departments identified for professor discovery"

- [ ] **Task 8: Implement Progress Tracking** (AC: 7)
  - [ ] Use progress_tracker from Story 1.7
  - [ ] Display: "Phase 1: Department Filtering [X/Y departments processed]"
  - [ ] Update progress after each department filtered
  - [ ] Display final summary: "Filtered X of Y departments (Z% retained)"

## Dev Notes

### Relevant Architecture Information

**Component:** University Structure Discovery Agent - Department Filter Module (Epic 2)

**Responsibility:** Filter departments based on research relevance; preserve transparency (Epic 2: FR9)

**Key Interfaces:**
- `filter_departments(departments: list[Department], profile: UserProfile) -> list[Department]` - LLM-based relevance filtering

**Dependencies:**
- LLM Helpers for prompt templates and LLM calls (Story 1.7)
- Structured Logger for decision logging (Story 1.7)
- Progress Tracker for status updates (Story 1.7)
- Checkpoint Manager for saving results (Story 1.7)
- User profile from Story 1.4

**Technology Stack:**
- Claude LLM via llm_helpers
- structlog with component context
- Pydantic Department model with relevance fields

**Source Tree Location:**
- Modify: `src/agents/university_discovery.py` (add filter_departments method)
- Modify: `src/models/department.py` (add is_relevant and relevance_reasoning fields)
- Create: `output/filtered-departments.md` (user review report)
- Create: `checkpoints/phase-1-relevant-departments.jsonl` (filtered results)

**Updated Department Model:**
```python
class Department(BaseModel):
    id: str  # Unique identifier (generated)
    name: str  # Department name
    school: Optional[str] = None  # Parent school/college
    division: Optional[str] = None  # Parent division
    url: str  # Department homepage URL
    hierarchy_level: int  # Depth in organizational tree (0=school, 1=division, 2=department)
    is_relevant: bool = False  # Result of relevance filtering (set in Story 2.3)
    relevance_reasoning: str = ""  # LLM explanation (set in Story 2.3)
```

**LLM Prompt Template (from llm_helpers.py):**
```python
DEPARTMENT_RELEVANCE_PROMPT = """
Given user research interests: {interests}
Degree program: {degree}
Background: {background}

Is department "{department_name}" ({school}) relevant for this user's research?

Guidelines:
- Include if ANY potential research overlap exists
- Include interdisciplinary departments
- Exclude only obviously unrelated departments

Respond in JSON format:
{{
  "decision": "include" | "exclude",
  "confidence": 0-100,
  "reasoning": "brief explanation"
}}
"""
```

**Filtering Philosophy:**
- **Inclusive by default:** When in doubt, include the department
- **ANY overlap criterion:** Even tangential research connections should pass filter
- **Exclude only obvious mismatches:** E.g., literature department for bioengineering student
- **Preserve transparency:** All decisions logged and reviewable

**Critical Rules (from Coding Standards):**
- All LLM calls must use llm_helpers module
- Never use print() for logging (use structlog)
- Always type hint function signatures
- Use checkpoint_manager.save_batch() - never write JSONL directly

**Architecture Component Diagram Flow:**
```
CLI Coordinator → University Discovery Agent (filter_departments)
University Discovery Agent → LLM Helpers (department relevance prompt)
University Discovery Agent → Logger (log all decisions)
University Discovery Agent → Checkpoint Manager (save relevant departments)
University Discovery Agent → File System (save filtered-departments.md)
```

### Testing

**Test File Location:** `tests/unit/test_department_filter.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Mock LLM responses with pytest-mock
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Unit test for filter_departments function with mock LLM responses
2. Test inclusive filtering (ANY overlap includes department)
3. Test obvious exclusion (completely unrelated department)
4. Test edge cases (interdisciplinary, ambiguous names)
5. Test logging of all decisions
6. Test checkpoint saving of relevant departments
7. Test filtered-departments.md report generation

**Example Test Pattern:**
```python
def test_filter_departments_includes_partial_overlap(mocker):
    # Arrange
    mock_llm = mocker.patch('src.utils.llm_helpers.call_llm_with_retry')
    mock_llm.return_value = '{"decision": "include", "confidence": 75, "reasoning": "Partial overlap with data science"}'

    departments = [
        Department(id="1", name="Statistics", school="Science",
                   url="https://stats.edu", hierarchy_level=1)
    ]
    profile = UserProfile(research_interests="Machine Learning and AI")

    # Act
    relevant = filter_departments(departments, profile)

    # Assert
    assert len(relevant) == 1
    assert relevant[0].is_relevant == True
    assert relevant[0].relevance_reasoning == "Partial overlap with data science"
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
