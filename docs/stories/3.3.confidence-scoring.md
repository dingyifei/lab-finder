# Story 3.3: Confidence Scoring for Filtering Decisions

## Status

**Draft**

## Dependencies

**Must Be Complete Before Starting:**
- **Story 3.2**: LLM-Based Professor Research Field Filtering (provides Professor model with `relevance_confidence`, `is_relevant`, `relevance_reasoning` fields)
- **Story 3.1**: Multi-Agent Professor Discovery (provides professor records)
- **Story 1.4**: Shared Utilities (provides `llm_helpers`, `checkpoint_manager`, `logger`)

## Story

**As a** user,
**I want** confidence scores provided for each filtering decision,
**so that** I can review borderline cases and understand the system's certainty.

## Acceptance Criteria

1. Confidence score (0-100) generated for each professor filtering decision (Epic 3 FR12)
2. Score reflects LLM's certainty about research field alignment (Epic 3 FR12)
3. Low-confidence decisions (e.g., <70%) flagged for user review (Epic 3 FR12)
4. Confidence scores saved with professor records (Epic 3 FR12)
5. Thresholds configurable in system parameters (Epic 3 FR12)
6. Score rationale includes key matching factors (Epic 3 FR12)

## Tasks / Subtasks

- [ ] **Task 1: Add Confidence Threshold Configuration** (AC: 5)
  - [ ] Update `config/system_params.json` to include:
    ```json
    {
      "filtering_config": {
        "low_confidence_threshold": 70,
        "high_confidence_threshold": 90,
        "borderline_review_enabled": true
      }
    }
    ```
  - [ ] Load thresholds in ProfessorFilterAgent initialization
  - [ ] Validate thresholds: 0 < low < high <= 100

- [ ] **Task 2: Enhance LLM Prompt in `src/utils/llm_helpers.py` for Confidence Scoring** (AC: 1, 2, 6)
  - [ ] Modify existing `PROFESSOR_FILTER_PROMPT` template in `src/utils/llm_helpers.py` (created in Story 3.2)
  - [ ] Update prompt to request detailed confidence explanation
  - [ ] Prompt should ask LLM to explain confidence level:
    ```
    Provide confidence score (0-100) reflecting:
    - Strength of research overlap (strong/moderate/weak)
    - Number of matching research areas
    - Specificity of match (direct vs. tangential)
    - Certainty in interpretation

    Include "key_factors" listing specific matching elements.
    ```
  - [ ] Ensure LLM returns structured confidence with rationale

- [ ] **Task 3: Update PROFESSOR_DATA_QUALITY_FLAGS Constant** (AC: 3)
  - [ ] Update `PROFESSOR_DATA_QUALITY_FLAGS` set in `src/models/professor.py`
  - [ ] Add new flags:
    - `"low_confidence_filter"` - For confidence < threshold
    - `"llm_response_error"` - For malformed LLM responses
    - `"manual_override"` - For user manual overrides

- [ ] **Task 4: Parse and Validate Confidence Scores** (AC: 1)
  - [ ] Parse confidence from LLM JSON response
  - [ ] Validate confidence is integer 0-100
  - [ ] If confidence invalid or missing: Set to 50 (neutral) and flag
  - [ ] Log parsing failures with WARNING level
  - [ ] Store validated confidence in Professor.relevance_confidence field (from Story 3.2)

- [ ] **Task 5: Identify Low-Confidence Decisions** (AC: 3)
  - [ ] Compare each confidence score to configured threshold
  - [ ] If confidence < low_confidence_threshold: Flag as "low_confidence"
  - [ ] Add to Professor.data_quality_flags: `"low_confidence_filter"`
  - [ ] Separate logging for low-confidence decisions (WARNING level)

- [ ] **Task 6: Generate Borderline Cases Review Report** (AC: 3)
  - [ ] Create `output/borderline-professors.md` report
  - [ ] List all professors with confidence below threshold
  - [ ] Include for each:
    - Professor name, department
    - Research areas
    - Filter decision (include/exclude)
    - Confidence score
    - Reasoning
  - [ ] Format as markdown table for easy review
  - [ ] Add instructions: "Review these borderline cases manually"

- [ ] **Task 7: Implement Confidence-Based Statistics** (AC: 1, 2)
  - [ ] Calculate confidence distribution:
    - High confidence (>=90): X professors
    - Medium confidence (70-89): Y professors
    - Low confidence (<70): Z professors
  - [ ] Calculate for both included and excluded professors
  - [ ] Include in filtering summary logs
  - [ ] Display: "Filtering complete: X high-conf, Y med-conf, Z low-conf matches"
  - [ ] Add validation: Log WARNING if >30% of decisions are low-confidence (indicates prompt tuning needed)

- [ ] **Task 8: Add Confidence Visualization to Reports** (AC: 4, 6) *[Depends on Task 7]*
  - [ ] In professor checkpoint, include confidence histogram data
  - [ ] Save confidence stats to `output/filter-confidence-stats.json`:
    ```json
    {
      "total_professors": 150,
      "included": {
        "total": 85,
        "high_confidence": 60,
        "medium_confidence": 20,
        "low_confidence": 5
      },
      "excluded": {
        "total": 65,
        "high_confidence": 50,
        "medium_confidence": 10,
        "low_confidence": 5
      }
    }
    ```
  - [ ] Use in final reports to show filtering quality

- [ ] **Task 9: Enable Manual Override of Low-Confidence Cases** (AC: 3)
  - [ ] Support optional manual review file: `config/manual-overrides.json`
  - [ ] Format:
    ```json
    {
      "professor_overrides": [
        {"professor_id": "prof-123", "decision": "include", "reason": "User manual review"}
      ]
    }
    ```
  - [ ] Apply overrides after LLM filtering
  - [ ] Log overrides: "Applied X manual overrides"
  - [ ] Flag overridden professors with `"manual_override"`

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Filter Agent - Confidence Scoring Module (Epic 3)

**Responsibility:** Provide confidence scores for filtering decisions; flag borderline cases (Epic 3: FR12)

**Key Interfaces:**
- `calculate_confidence_stats(professors: list[Professor]) -> dict` - Compute confidence distribution
- `identify_borderline_cases(professors: list[Professor], threshold: int) -> list[Professor]`

**Dependencies:**
- LLM Helpers for enhanced prompts with confidence (Story 1.4)
- Professor Filter Agent from Story 3.2
- System parameters config for thresholds
- Structured Logger for confidence-based logging

**Technology Stack:**
- Python statistics for distribution calculation
- JSON for confidence stats export
- Pydantic validation for confidence scores

**Source Tree Location:**
- Modify: `src/agents/professor_filter.py` (add confidence logic)
- Modify: `src/models/professor.py` (add new quality flags to PROFESSOR_DATA_QUALITY_FLAGS constant)
- Modify: `src/utils/llm_helpers.py` (enhance PROFESSOR_FILTER_PROMPT with confidence explanation)
- Modify: `config/system_params.json` (add filtering_config)
- Create: `output/borderline-professors.md` (review report)
- Create: `output/filter-confidence-stats.json` (statistics)
- Optional: `config/manual-overrides.json` (user overrides)

**Confidence Scoring Levels:**
- **High (90-100):** Strong, clear research overlap; multiple matching areas
- **Medium (70-89):** Moderate overlap; some matching areas or tangential connection
- **Low (<70):** Weak or unclear overlap; requires manual review

**Expected Confidence Distribution (typical):**
- High confidence (90-100): ~40-50% of decisions
- Medium confidence (70-89): ~30-40% of decisions
- Low confidence (<70): ~10-20% of decisions

*Note: Actual distribution will vary based on user profile specificity and department relevance. Use these as guidelines for validation.*

**Enhanced LLM Response Format:**
```json
{
  "decision": "include" | "exclude",
  "confidence": 85,
  "reasoning": "Strong overlap in machine learning and neural networks",
  "key_factors": [
    "Direct match: Machine Learning",
    "Direct match: Neural Networks",
    "Moderate match: Data Science (related to user's AI interests)"
  ],
  "confidence_explanation": "High confidence due to multiple direct matches in core research areas"
}
```

**Edge Case Handling:**
- **Confidence > 100 or < 0:** Clamp to valid range (0-100) and log warning
- **Non-numeric confidence:** Set to 50 (neutral) and add to data_quality_flags
- **Missing confidence field:** Set to 50 (neutral) and add to data_quality_flags
- **Malformed JSON response:** Log error, set confidence to 50, flag as "llm_response_error"
- **Confidence present but invalid type (string, null, etc.):** Attempt conversion; if fails, set to 50 and flag

*All edge cases should be logged at WARNING level with professor identifier for debugging.*

**Confidence Threshold Logic:**
```python
# From system_params.json
LOW_THRESHOLD = 70
HIGH_THRESHOLD = 90

if confidence >= HIGH_THRESHOLD:
    status = "high_confidence"
elif confidence >= LOW_THRESHOLD:
    status = "medium_confidence"
else:
    status = "low_confidence"
    # Flag for review
    professor.data_quality_flags.append("low_confidence_filter")
```

**Borderline Cases Report Format:**
```markdown
# Borderline Professor Filtering Cases

**Low Confidence Threshold:** 70
**Total Borderline Cases:** 12

## Included Professors (Low Confidence)

| Professor | Department | Research Areas | Confidence | Reasoning |
|-----------|------------|----------------|------------|-----------|
| Dr. Jane Smith | Computer Science | HCI, Visualization | 65 | Tangential overlap with user's AI interests |

## Excluded Professors (Low Confidence)

| Professor | Department | Research Areas | Confidence | Reasoning |
|-----------|------------|----------------|------------|-----------|
| Dr. John Doe | Statistics | Bayesian Methods | 68 | Weak connection to user's deep learning focus |

**Action Required:** Review these borderline cases manually to ensure no relevant professors were filtered incorrectly.
```

**Critical Rules (from Coding Standards):**
- Confidence scores must be validated (0-100 range)
- Always flag low-confidence decisions for user review
- Never use print() for logging (use structlog)
- All confidence thresholds must be configurable

**Architecture Component Diagram Flow:**
```
Professor Filter Agent → LLM (request confidence + reasoning)
Professor Filter Agent → Confidence Validator (validate score)
Professor Filter Agent → Threshold Checker (identify borderline)
Professor Filter Agent → Report Generator (borderline-professors.md)
Professor Filter Agent → Stats Calculator (filter-confidence-stats.json)
```

### Testing

**Test File Location:** `tests/unit/test_confidence_scoring.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Mock LLM responses with various confidence levels
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Test confidence score parsing from LLM response
2. Test confidence validation (valid and invalid scores)
3. Test low-confidence identification
4. Test confidence statistics calculation
5. Test borderline cases report generation
6. Test manual override application
7. Test threshold configuration loading

**Example Test Pattern:**
```python
def test_identify_low_confidence_cases():
    # Arrange
    professors = [
        Professor(id="1", name="Dr. Smith", relevance_confidence=95, is_relevant=True),
        Professor(id="2", name="Dr. Doe", relevance_confidence=65, is_relevant=True),
        Professor(id="3", name="Dr. Lee", relevance_confidence=45, is_relevant=False),
    ]
    threshold = 70
    agent = ProfessorFilterAgent()

    # Act
    borderline = agent.identify_borderline_cases(professors, threshold)

    # Assert
    assert len(borderline) == 2  # Doe and Lee below threshold
    assert all(p.relevance_confidence < threshold for p in borderline)
    assert "low_confidence_filter" in professors[1].data_quality_flags
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |
| 2025-10-07 | 0.2 | Added Professor model reference, expected confidence distribution, and edge case handling documentation | Sarah (PO) |
| 2025-10-07 | 0.3 | Fixed field name to filter_confidence (aligns with architecture), added Task 7 dependency note, added Epic FR cross-references to AC | Sarah (PO) |
| 2025-10-07 | 0.4 | **VALIDATION FIXES:** (1) Added Dependencies section - Story 3.2 must be COMPLETE first; (2) Fixed field name inconsistency: filter_confidence→relevance_confidence (aligns with Story 3.2 v0.5); (3) Fixed data type: float→int for confidence; (4) Added Task 3 to update PROFESSOR_DATA_QUALITY_FLAGS constant; (5) Enhanced Task 2 with specific implementation location (llm_helpers.py PROFESSOR_FILTER_PROMPT); (6) Added distribution validation warning in Task 7; (7) Updated Source Tree Location to reflect actual file modifications; (8) Renumbered tasks 3→9 after adding new Task 3 | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
