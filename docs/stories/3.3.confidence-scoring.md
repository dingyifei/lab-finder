# Story 3.3: Confidence Scoring for Filtering Decisions

## Status

**Draft**

## Story

**As a** user,
**I want** confidence scores provided for each filtering decision,
**so that** I can review borderline cases and understand the system's certainty.

## Acceptance Criteria

1. Confidence score (0-100) generated for each professor filtering decision (FR12)
2. Score reflects LLM's certainty about research field alignment
3. Low-confidence decisions (e.g., <70%) flagged for user review
4. Confidence scores saved with professor records
5. Thresholds configurable in system parameters
6. Score rationale includes key matching factors

## Tasks / Subtasks

- [ ] **Task 1: Add Confidence Threshold Configuration** (AC: 5)
  - [ ] Update `config/system-parameters.json` to include:
    ```json
    {
      "filtering_config": {
        "low_confidence_threshold": 70,
        "high_confidence_threshold": 90,
        "borderline_review_enabled": true
      }
    }
    ```
  - [ ] Load thresholds in ProfessorFilterAgent initialization
  - [ ] Validate thresholds: 0 < low < high <= 100

- [ ] **Task 2: Enhance LLM Prompt for Confidence Scoring** (AC: 1, 2, 6)
  - [ ] Update professor filter prompt to request detailed confidence
  - [ ] Prompt should ask LLM to explain confidence level:
    ```
    Provide confidence score (0-100) reflecting:
    - Strength of research overlap (strong/moderate/weak)
    - Number of matching research areas
    - Specificity of match (direct vs. tangential)
    - Certainty in interpretation

    Include "key_factors" listing specific matching elements.
    ```
  - [ ] Ensure LLM returns structured confidence with rationale

- [ ] **Task 3: Parse and Validate Confidence Scores** (AC: 1)
  - [ ] Parse confidence from LLM JSON response
  - [ ] Validate confidence is integer 0-100
  - [ ] If confidence invalid or missing: Set to 50 (neutral) and flag
  - [ ] Log parsing failures with WARNING level
  - [ ] Store validated confidence in Professor.relevance_confidence field

- [ ] **Task 4: Identify Low-Confidence Decisions** (AC: 3)
  - [ ] Compare each confidence score to configured threshold
  - [ ] If confidence < low_confidence_threshold: Flag as "low_confidence"
  - [ ] Add to Professor.data_quality_flags: `"low_confidence_filter"`
  - [ ] Separate logging for low-confidence decisions (WARNING level)

- [ ] **Task 5: Generate Borderline Cases Review Report** (AC: 3)
  - [ ] Create `output/borderline-professors.md` report
  - [ ] List all professors with confidence below threshold
  - [ ] Include for each:
    - Professor name, department
    - Research areas
    - Filter decision (include/exclude)
    - Confidence score
    - Reasoning
  - [ ] Format as markdown table for easy review
  - [ ] Add instructions: "Review these borderline cases manually"

- [ ] **Task 6: Implement Confidence-Based Statistics** (AC: 1, 2)
  - [ ] Calculate confidence distribution:
    - High confidence (>=90): X professors
    - Medium confidence (70-89): Y professors
    - Low confidence (<70): Z professors
  - [ ] Calculate for both included and excluded professors
  - [ ] Include in filtering summary logs
  - [ ] Display: "Filtering complete: X high-conf, Y med-conf, Z low-conf matches"

- [ ] **Task 7: Add Confidence Visualization to Reports** (AC: 4, 6)
  - [ ] In professor checkpoint, include confidence histogram data
  - [ ] Save confidence stats to `output/filter-confidence-stats.json`:
    ```json
    {
      "total_professors": 150,
      "included": {
        "total": 85,
        "high_confidence": 60,
        "medium_confidence": 20,
        "low_confidence": 5
      },
      "excluded": {
        "total": 65,
        "high_confidence": 50,
        "medium_confidence": 10,
        "low_confidence": 5
      }
    }
    ```
  - [ ] Use in final reports to show filtering quality

- [ ] **Task 8: Enable Manual Override of Low-Confidence Cases** (AC: 3)
  - [ ] Support optional manual review file: `config/manual-overrides.json`
  - [ ] Format:
    ```json
    {
      "professor_overrides": [
        {"professor_id": "prof-123", "decision": "include", "reason": "User manual review"}
      ]
    }
    ```
  - [ ] Apply overrides after LLM filtering
  - [ ] Log overrides: "Applied X manual overrides"
  - [ ] Flag overridden professors with `"manual_override"`

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Filter Agent - Confidence Scoring Module (Epic 3)

**Responsibility:** Provide confidence scores for filtering decisions; flag borderline cases (Epic 3: FR12)

**Key Interfaces:**
- `calculate_confidence_stats(professors: list[Professor]) -> dict` - Compute confidence distribution
- `identify_borderline_cases(professors: list[Professor], threshold: int) -> list[Professor]`

**Dependencies:**
- LLM Helpers for enhanced prompts with confidence (Story 1.7)
- Professor Filter Agent from Story 3.2
- System parameters config for thresholds
- Structured Logger for confidence-based logging

**Technology Stack:**
- Python statistics for distribution calculation
- JSON for confidence stats export
- Pydantic validation for confidence scores

**Source Tree Location:**
- Modify: `src/agents/professor_filter.py` (add confidence logic)
- Modify: `config/system-parameters.json` (add filtering_config)
- Create: `output/borderline-professors.md` (review report)
- Create: `output/filter-confidence-stats.json` (statistics)
- Optional: `config/manual-overrides.json` (user overrides)

**Confidence Scoring Levels:**
- **High (90-100):** Strong, clear research overlap; multiple matching areas
- **Medium (70-89):** Moderate overlap; some matching areas or tangential connection
- **Low (<70):** Weak or unclear overlap; requires manual review

**Enhanced LLM Response Format:**
```json
{
  "decision": "include" | "exclude",
  "confidence": 85,
  "reasoning": "Strong overlap in machine learning and neural networks",
  "key_factors": [
    "Direct match: Machine Learning",
    "Direct match: Neural Networks",
    "Moderate match: Data Science (related to user's AI interests)"
  ],
  "confidence_explanation": "High confidence due to multiple direct matches in core research areas"
}
```

**Confidence Threshold Logic:**
```python
# From system-parameters.json
LOW_THRESHOLD = 70
HIGH_THRESHOLD = 90

if confidence >= HIGH_THRESHOLD:
    status = "high_confidence"
elif confidence >= LOW_THRESHOLD:
    status = "medium_confidence"
else:
    status = "low_confidence"
    # Flag for review
    professor.data_quality_flags.append("low_confidence_filter")
```

**Borderline Cases Report Format:**
```markdown
# Borderline Professor Filtering Cases

**Low Confidence Threshold:** 70
**Total Borderline Cases:** 12

## Included Professors (Low Confidence)

| Professor | Department | Research Areas | Confidence | Reasoning |
|-----------|------------|----------------|------------|-----------|
| Dr. Jane Smith | Computer Science | HCI, Visualization | 65 | Tangential overlap with user's AI interests |

## Excluded Professors (Low Confidence)

| Professor | Department | Research Areas | Confidence | Reasoning |
|-----------|------------|----------------|------------|-----------|
| Dr. John Doe | Statistics | Bayesian Methods | 68 | Weak connection to user's deep learning focus |

**Action Required:** Review these borderline cases manually to ensure no relevant professors were filtered incorrectly.
```

**Critical Rules (from Coding Standards):**
- Confidence scores must be validated (0-100 range)
- Always flag low-confidence decisions for user review
- Never use print() for logging (use structlog)
- All confidence thresholds must be configurable

**Architecture Component Diagram Flow:**
```
Professor Filter Agent → LLM (request confidence + reasoning)
Professor Filter Agent → Confidence Validator (validate score)
Professor Filter Agent → Threshold Checker (identify borderline)
Professor Filter Agent → Report Generator (borderline-professors.md)
Professor Filter Agent → Stats Calculator (filter-confidence-stats.json)
```

### Testing

**Test File Location:** `tests/unit/test_confidence_scoring.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Mock LLM responses with various confidence levels
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Test confidence score parsing from LLM response
2. Test confidence validation (valid and invalid scores)
3. Test low-confidence identification
4. Test confidence statistics calculation
5. Test borderline cases report generation
6. Test manual override application
7. Test threshold configuration loading

**Example Test Pattern:**
```python
def test_identify_low_confidence_cases():
    # Arrange
    professors = [
        Professor(id="1", name="Dr. Smith", relevance_confidence=95, is_relevant=True),
        Professor(id="2", name="Dr. Doe", relevance_confidence=65, is_relevant=True),
        Professor(id="3", name="Dr. Lee", relevance_confidence=45, is_relevant=False),
    ]
    threshold = 70
    agent = ProfessorFilterAgent()

    # Act
    borderline = agent.identify_borderline_cases(professors, threshold)

    # Assert
    assert len(borderline) == 2  # Doe and Lee below threshold
    assert all(p.relevance_confidence < threshold for p in borderline)
    assert "low_confidence_filter" in professors[1].data_quality_flags
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
