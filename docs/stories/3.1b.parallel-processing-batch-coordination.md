# Story 3.1b: Parallel Processing + Batch Coordination

## Status

**Draft**

## Story

**As a** user,
**I want** professor discovery executed in parallel across departments with progress tracking,
**so that** discovery is efficient even for large universities.

## Acceptance Criteria

1. Results aggregated from parallel async tasks into master professor list
2. Parallel processing uses Python asyncio.gather() with configurable concurrency limits
3. Progress tracking shows current batch and department being processed
4. Batch progress displayed: "Processing department X of Y"
5. Failed departments logged but don't block overall processing

## Dependencies

**Must Be Complete Before Starting:**
- **Story 3.1a** - Requires Professor model and `discover_professors_for_department()` function
- **Story 1.4**: Shared Utilities Implementation (provides `progress_tracker`)

**Blocks:**
- **Story 3.1c** - Provides parallel professor discovery results for deduplication

## Tasks / Subtasks

- [ ] **Task 1: Setup Progress Tracking** (AC: 3, 4)
  - [ ] Use progress_tracker from `src/utils/progress_tracker.py`
  - [ ] Initialize at start: `tracker.start_phase("Phase 2: Professor Discovery", total_items=len(departments))`
  - [ ] Update after each department: `tracker.update(completed=i+1)`
  - [ ] Display batch progress: "Batch M: Processing departments A-B"
  - [ ] Final summary: "Phase 2 complete: X professors discovered"

> **Note:** Tasks 2-4 below are organized by responsibility but are integrated in a single `discover_professors_parallel()` function (see Dev Notes implementation example lines 144-230).

- [ ] **Task 2: Implement Parallel Processing with asyncio.gather()** (AC: 1, 2, 5)
  - [ ] Implement `discover_professors_parallel(departments: list[Department], max_concurrent: int) -> list[Professor]`
  - [ ] Create asyncio.Semaphore for concurrency control:
    ```python
    semaphore = asyncio.Semaphore(max_concurrent)

    async def process_with_semaphore(dept: Department) -> list[Professor]:
        async with semaphore:
            correlation_id = f"prof-disc-{uuid.uuid4()}"
            logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
            return await discover_professors_for_department(dept, correlation_id)
    ```
  - [ ] Execute parallel tasks: `results = await asyncio.gather(*tasks, return_exceptions=True)`
  - [ ] Handle exceptions gracefully: log errors, continue with partial results
  - [ ] Get max_concurrent from `system_params.json::batch_sizes.departments`
  - [ ] Log: "Processing {len(departments)} departments with max_concurrent={max_concurrent}"

- [ ] **Task 3: Aggregate Parallel Results** (AC: 1)
  - [ ] Flatten results from all parallel tasks
  - [ ] Filter out Exception objects from results
  - [ ] Track failed departments for logging
  - [ ] Return combined list of all professors

- [ ] **Task 4: Error Handling for Failed Departments** (AC: 5)
  - [ ] If department processing fails: Log error, continue with others
  - [ ] Track failed department count
  - [ ] Include in summary: "X of Y departments failed"
  - [ ] Don't fail entire pipeline for single department errors

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Discovery & Filter Agent (Epic 3)

**Responsibility:** Coordinate parallel professor discovery across departments with progress tracking.

**Key Interfaces:**
- `discover_professors_parallel(departments: list[Department], max_concurrent: int) -> list[Professor]` - Parallel processing coordinator

**Dependencies:**
- `discover_professors_for_department()` from Story 3.1a
- Progress Tracker for status updates (see `src/utils/progress_tracker.py`)
- System parameters config for batch size
- Department data from Epic 2 checkpoints

**Technology Stack:**
- Python asyncio for parallel execution
- asyncio.Semaphore for concurrency control
- structlog with correlation IDs
- Progress Tracker for user feedback

**Parallel Execution Pattern:** Application-level asyncio.gather() with Semaphore for concurrency control (NOT Claude Agent SDK feature)

**System Configuration:**
The story references `system_params.json::batch_sizes.departments` for concurrency limits. See `config/system_params.example.json` for the template configuration.

**SystemParams Model:** Defined in `src/models/config.py` (see Story 2.5 for implementation details)

Example configuration structure:
```json
{
  "batch_sizes": {  // Accessed via system_params.batch_sizes
    "departments": 5,  // system_params.batch_sizes.departments
    "professors": 20,
    "labs": 10
  }
}
```

Configuration loading example:
```python
from src.models.config import SystemParams  # Model defined in Story 2.5
import json

def load_system_params() -> SystemParams:
    with open("config/system_params.json") as f:
        config = json.load(f)
    return SystemParams.model_validate(config)
```

**Source Tree Location:**
- Modify: `src/agents/professor_filter.py` (add parallel processing functions)
- Use: `src/utils/progress_tracker.py`
- Use: `config/system_params.json`
- Use: `src/models/config.py` (SystemParams model from Story 2.5)

**Correlation ID Hierarchy Pattern:**
For traceability across distributed async tasks, use hierarchical correlation IDs:
- Coordinator level: `"prof-discovery-coordinator"`
- Department level: `f"prof-disc-{dept.id}-{uuid.uuid4().hex[:8]}"`
- Batch level (if batching): `f"prof-disc-batch-{batch_num}-{uuid.uuid4().hex[:8]}"`

This enables filtering logs by department or batch while maintaining unique IDs per execution.

> **Design Decision:** This hierarchical pattern is introduced in Story 3.1b to support parallel async processing traceability. It extends the basic correlation ID pattern from Story 1.4 (Shared Utilities) with domain-specific prefixes for professor discovery operations.

**Note on Checkpointing:** This story implements the parallel execution pattern only. Checkpoint persistence of discovered professors is handled by downstream stories:
- Story 3.1c (deduplication + rate limiting) OR
- Story 3.5 (overall batch processing coordination)

The `discover_professors_parallel()` function returns aggregated results to the caller for persistence. This separation allows the parallel processing logic to remain pure and reusable across different orchestration contexts.

### Parallel Processing Implementation

**Pattern: asyncio.gather() with Semaphore**

```python
import asyncio
from typing import List
import uuid
from src.utils.logger import get_logger

async def discover_professors_parallel(
    departments: list[Department],
    max_concurrent: int = 5
) -> list[Professor]:
    """
    Discover professors across multiple departments in parallel using asyncio.

    This is APPLICATION-LEVEL parallel execution, NOT a Claude Agent SDK feature.
    Uses asyncio.Semaphore to control concurrency.
    """
    from src.utils.progress_tracker import ProgressTracker

    logger = get_logger(correlation_id="prof-discovery-coordinator", phase="professor_discovery")
    logger.info(f"Starting parallel discovery", departments_count=len(departments), max_concurrent=max_concurrent)

    # Initialize progress tracker
    tracker = ProgressTracker()
    tracker.start_phase("Phase 2: Professor Discovery", total_items=len(departments))

    # Create semaphore for concurrency control
    semaphore = asyncio.Semaphore(max_concurrent)
    completed_count = 0

    async def process_with_semaphore(dept: Department, index: int) -> list[Professor]:
        """Process single department with semaphore control and progress tracking."""
        nonlocal completed_count

        async with semaphore:
            # Generate unique correlation ID for this department's processing
            correlation_id = f"prof-disc-{dept.id}-{uuid.uuid4().hex[:8]}"

            logger.info(f"Processing department {index+1}/{len(departments)}",
                       department=dept.name,
                       correlation_id=correlation_id)

            try:
                professors = await discover_professors_for_department(dept, correlation_id)

                # Update progress after successful completion
                completed_count += 1
                tracker.update(completed=completed_count)

                return professors
            except Exception as e:
                logger.error("Department processing failed",
                           department=dept.name,
                           error=str(e),
                           correlation_id=correlation_id)

                # Update progress even on failure
                completed_count += 1
                tracker.update(completed=completed_count)

                return []  # Return empty list, continue with others

    # Create tasks for all departments
    tasks = [process_with_semaphore(dept, i) for i, dept in enumerate(departments)]

    # Execute in parallel with asyncio.gather
    # return_exceptions=True ensures one failure doesn't stop others
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Flatten results
    all_professors = []
    failed_count = 0
    for result in results:
        if isinstance(result, Exception):
            failed_count += 1
            logger.error("Task failed with exception", error=str(result))
            continue
        all_professors.extend(result)

    # Complete progress tracking
    tracker.complete_phase()

    logger.info("Parallel discovery complete",
               total_professors=len(all_professors),
               failed_departments=failed_count)

    return all_professors
```

### Progress Tracking Integration

**Use `progress_tracker.py` from Story 1.4:**

```python
from src.utils.progress_tracker import ProgressTracker

tracker = ProgressTracker()

# Start phase
tracker.start_phase("Phase 2: Professor Discovery", total_items=len(departments))

# Within parallel processing - update after each department
for i, dept in enumerate(departments):
    # ... process department ...
    tracker.update(completed=i+1)

# Complete phase
tracker.complete_phase()
```

**Expected Display:**
```
Phase 2: Professor Discovery [3/25 departments]
Batch 1: Processing departments 1-5
```

### Configuration Loading

**Load batch size from system_params.json:**

```python
from src.models.config import SystemParams
import json

def load_system_params() -> SystemParams:
    with open("config/system_params.json") as f:
        config = json.load(f)
    return SystemParams.model_validate(config)

# Usage
system_params = load_system_params()
max_concurrent = system_params.batch_sizes.departments  # e.g., 5
```

### Error Handling Pattern

```python
# Within parallel processing
async def process_with_error_handling(dept: Department) -> list[Professor]:
    try:
        return await discover_professors_for_department(dept, correlation_id)
    except Exception as e:
        logger.error("Department failed", department=dept.name, error=str(e))
        # Don't raise - return empty list and continue
        return []

# After gather()
for i, result in enumerate(results):
    if isinstance(result, Exception):
        # Log exception from gather
        logger.error(f"Department {departments[i].name} raised exception: {result}")
    elif isinstance(result, list):
        # Success - aggregate results
        all_professors.extend(result)
```

### Testing

**Test File Location:** `tests/integration/test_professor_discovery.py`

**Test Organization:** Add parallel processing tests to the existing `test_professor_discovery.py` file created in Story 3.1a. These tests complement the basic discovery tests with parallel execution and concurrency control validation.

**Testing Standards:**
- Framework: pytest 8.4.2
- Async tests: pytest-asyncio
- Coverage requirement: 70% minimum

**Test Requirements:**

1. **Test parallel execution with asyncio.gather**
   ```python
   @pytest.mark.asyncio
   async def test_parallel_discovery_with_multiple_departments(mocker):
       # Mock discover_professors_for_department
       async def mock_discover(dept, corr_id):
           return [Professor(id=f"prof-{dept.id}", name=f"Prof from {dept.name}", profile_url="test")]

       mocker.patch('src.agents.professor_filter.discover_professors_for_department',
                    side_effect=mock_discover)

       depts = [Department(id=f"d{i}", name=f"Dept{i}", url=f"http://d{i}.edu") for i in range(5)]
       professors = await discover_professors_parallel(depts, max_concurrent=3)

       assert len(professors) == 5
   ```

2. **Test concurrency control with Semaphore**
   ```python
   @pytest.mark.asyncio
   async def test_semaphore_limits_concurrent_execution(mocker):
       # Track concurrent executions
       active_count = 0
       max_active = 0

       async def mock_discover_with_tracking(dept, corr_id):
           nonlocal active_count, max_active
           active_count += 1
           max_active = max(max_active, active_count)
           await asyncio.sleep(0.1)  # Simulate work
           active_count -= 1
           return []

       mocker.patch('src.agents.professor_filter.discover_professors_for_department',
                    side_effect=mock_discover_with_tracking)

       depts = [Department(id=f"d{i}", name=f"Dept{i}", url=f"http://d{i}.edu") for i in range(10)]
       await discover_professors_parallel(depts, max_concurrent=3)

       # Verify semaphore limited concurrency to 3
       assert max_active <= 3
   ```

3. **Test progress tracking**
   ```python
   @pytest.mark.asyncio
   async def test_progress_tracking_updates(mocker):
       mock_tracker = mocker.patch('src.utils.progress_tracker.ProgressTracker')

       depts = [Department(id=f"d{i}", name=f"Dept{i}", url="test") for i in range(5)]
       await discover_professors_parallel(depts, max_concurrent=2)

       # Verify progress tracker called
       mock_tracker.return_value.start_phase.assert_called_once()
       assert mock_tracker.return_value.update.call_count == 5
   ```

4. **Test error handling for failed departments**
   ```python
   @pytest.mark.asyncio
   async def test_failed_department_doesnt_stop_processing(mocker):
       # Mock to fail on 2nd department
       async def mock_discover_with_failure(dept, corr_id):
           if dept.id == "d1":
               raise Exception("Department 1 failed")
           return [Professor(id=f"prof-{dept.id}", name=f"Prof", profile_url="test")]

       mocker.patch('src.agents.professor_filter.discover_professors_for_department',
                    side_effect=mock_discover_with_failure)

       depts = [Department(id=f"d{i}", name=f"Dept{i}", url="test") for i in range(3)]
       professors = await discover_professors_parallel(depts, max_concurrent=2)

       # Should get professors from d0 and d2, but not d1
       assert len(professors) == 2
   ```

5. **Test edge case: Empty department list**
   ```python
   @pytest.mark.asyncio
   async def test_empty_department_list(mocker):
       mocker.patch('src.agents.professor_filter.discover_professors_for_department')

       professors = await discover_professors_parallel([], max_concurrent=3)

       # Should return empty list without errors
       assert professors == []
   ```

6. **Test edge case: All departments fail**
   ```python
   @pytest.mark.asyncio
   async def test_all_departments_fail(mocker):
       async def mock_discover_always_fail(dept, corr_id):
           raise Exception("All departments failed")

       mocker.patch('src.agents.professor_filter.discover_professors_for_department',
                    side_effect=mock_discover_always_fail)

       depts = [Department(id=f"d{i}", name=f"Dept{i}", url="test") for i in range(3)]
       professors = await discover_professors_parallel(depts, max_concurrent=2)

       # Should return empty list, not crash
       assert professors == []
   ```

7. **Test edge case: Single department**
   ```python
   @pytest.mark.asyncio
   async def test_single_department(mocker):
       async def mock_discover(dept, corr_id):
           return [Professor(id="prof-1", name="Prof Smith", profile_url="test")]

       mocker.patch('src.agents.professor_filter.discover_professors_for_department',
                    side_effect=mock_discover)

       depts = [Department(id="d1", name="Dept1", url="test")]
       professors = await discover_professors_parallel(depts, max_concurrent=5)

       # Should handle single department correctly
       assert len(professors) == 1
   ```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | 0.1 | Split from Story 3.1 - Parallel processing and batch coordination | Bob (SM) |
| 2025-10-07 | 0.2 | Added checkpoint orchestration clarification in Dev Notes per PO validation recommendation | Sarah (PO) |
| 2025-10-07 | 0.3 | PO validation fixes: Story 1.7→1.4 reference correction, added edge case tests, documented correlation ID pattern, added task integration note | Sarah (PO) |
| 2025-10-07 | 0.4 | PO validation improvements: Enhanced config example with nested path comments, clarified test file organization as additive to Story 3.1a tests | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

**Files Modified in This Story:**
- **MODIFY:** `src/agents/professor_filter.py` - Add parallel processing functions

**Cross-Story File Tracking for `src/agents/professor_filter.py`:**
- **Story 3.1a:** Created file with discovery functions ✅
- **Story 3.1b (this story):** Adds parallel processing functions (discover_professors_parallel)
- **Story 3.1c:** Will add deduplication + rate limiting functions
- **Story 3.2 (future):** Will add filtering functions

**Expected Function Additions:**
- `discover_professors_parallel()` - Parallel async processing with Semaphore and progress tracking

_Dev agent: Update with actual files modified during implementation_

## QA Results

_To be populated by QA agent_
