# Story 4.1: Lab Website Discovery & Scraping

## Status

**Draft**

## Story

**As a** user,
**I want** lab websites discovered and scraped for each relevant professor,
**so that** I can assess lab activity through website maintenance and content.

## Acceptance Criteria

1. Lab website URLs identified from professor directory or search (FR14)
2. Website content scraped using built-in web tools (NFR5)
3. Playwright fallback when built-in tools fail
4. Last modified/update date extracted where available
5. Lab description, research focus, news/updates extracted
6. Current website content saved for analysis
7. Missing websites flagged but don't block processing

## Tasks / Subtasks

- [ ] **Task 1: Create Lab Pydantic Model** (AC: 4, 5, 6)
  - [ ] Create `src/models/lab.py` module
  - [ ] Define Lab model:
    ```python
    class Lab(BaseModel):
        id: str  # Unique identifier
        professor_id: str  # Foreign key to Professor
        professor_name: str  # PI name for display
        department: str  # Department affiliation
        lab_name: str  # Lab name
        lab_url: Optional[str] = None  # Lab website URL
        last_updated: Optional[datetime] = None  # Last website update
        description: str = ""  # Lab description/overview
        research_focus: list[str] = []  # Research themes
        news_updates: list[str] = []  # Recent news items
        website_content: str = ""  # Full text content (for analysis)
        data_quality_flags: list[str] = []  # Quality issues
    ```
  - [ ] Add type hints and validation
  - [ ] Run mypy to verify type correctness

- [ ] **Task 2: Implement Lab Website Discovery** (AC: 1)
  - [ ] Create `src/agents/lab_research.py` module
  - [ ] Implement `discover_lab_website(professor: Professor) -> Optional[str]`
  - [ ] Discovery strategies:
    1. Use professor.lab_url if already extracted in Phase 2
    2. Search professor profile page for lab website link
    3. Search department page for professor's lab link
    4. Try common patterns: `{dept_url}/labs/{professor_lastname}`
  - [ ] Return first valid lab URL found
  - [ ] If no URL found: Return None, flag as missing

- [ ] **Task 3: Implement Lab Website Scraping** (AC: 2, 3, 5, 6)
  - [ ] Implement `scrape_lab_website(lab_url: str) -> dict`
  - [ ] Scraping logic:
    - Try built-in web tools first (NFR5)
    - Fallback to Playwright if built-in fails
    - Extract structured content:
      - Lab description/overview
      - Research focus areas
      - News/updates section
      - Full page text content
  - [ ] Add retry logic with tenacity (3 retries, exponential backoff)
  - [ ] Timeout: 30 seconds per page

- [ ] **Task 4: Extract Last Update Date** (AC: 4)
  - [ ] Check for explicit "Last Updated" metadata
  - [ ] Check HTML meta tags: `<meta name="last-modified">`
  - [ ] Check for "Updated:" or "Last modified:" text patterns
  - [ ] Parse dates to datetime objects
  - [ ] If no date found: Set to None, flag as unknown
  - [ ] Log extracted dates for verification

- [ ] **Task 5: Extract Lab Description and Research Focus** (AC: 5)
  - [ ] Look for lab overview/description sections
  - [ ] Common selectors:
    - `.lab-overview`, `#about`, `.description`
    - `<section>` with "about" or "overview" heading
  - [ ] Extract research focus:
    - Look for "Research Areas:", "Focus:", "Interests:"
    - Parse bulleted lists or paragraphs
  - [ ] Clean and normalize extracted text
  - [ ] Store in Lab.description and Lab.research_focus

- [ ] **Task 6: Extract Recent News/Updates** (AC: 5)
  - [ ] Look for news or updates sections:
    - `.news`, `.updates`, `#latest-news`
    - `<section>` with "news" or "updates" heading
  - [ ] Extract recent items (last 5-10 entries)
  - [ ] Parse dates if available
  - [ ] Store headlines/summaries in Lab.news_updates
  - [ ] Use for website freshness assessment

- [ ] **Task 7: Handle Missing Lab Websites** (AC: 1, 7)
  - [ ] If lab URL not found: Create Lab record with minimal data
  - [ ] Set Lab.lab_url = None
  - [ ] Add flag: `data_quality_flags.append("no_website")`
  - [ ] Don't fail processing - continue with other data sources
  - [ ] Log: "No website found for {professor_name}'s lab"
  - [ ] These labs will rely on publication data (Epic 5)

- [ ] **Task 8: Save Lab Data to Checkpoint** (AC: 6)
  - [ ] Use checkpoint_manager.save_batch()
  - [ ] Save to `checkpoints/phase-3-labs-batch-{N}.jsonl`
  - [ ] Include all Lab model fields
  - [ ] Serialize with `.model_dump()`
  - [ ] Log: "Scraped X lab websites"

- [ ] **Task 9: Integrate Progress Tracking** (AC: 1)
  - [ ] Use progress_tracker from Story 1.7
  - [ ] Display: "Phase 3: Lab Website Discovery [X/Y professors processed]"
  - [ ] Show batch progress
  - [ ] Display summary: "Discovered X lab websites, Y missing"

## Dev Notes

### Relevant Architecture Information

**Component:** Lab Research Agent (Epic 4)

**Responsibility:** Discover and scrape lab websites for content and freshness (Epic 4: FR14)

**Key Interfaces:**
- `discover_lab_website(professor: Professor) -> Optional[str]` - Find lab URL
- `scrape_lab_website(lab_url: str) -> dict` - Extract lab content

**Dependencies:**
- Professor data from Epic 3 (filtered professors)
- Claude Agent SDK built-in web tools (primary)
- Playwright (fallback for JS-heavy pages)
- Checkpoint Manager for saving lab data
- Progress Tracker for status updates

**Technology Stack:**
- Claude Agent SDK web fetch tools
- Playwright 1.40.0 as fallback
- Python datetime for date parsing
- structlog with phase context

**Source Tree Location:**
- Create: `src/agents/lab_research.py`
- Create: `src/models/lab.py`
- Load from: `checkpoints/phase-2-professors-filtered-batch-*.jsonl`
- Save to: `checkpoints/phase-3-labs-batch-{N}.jsonl`

**Lab Model Schema:**
```python
class Lab(BaseModel):
    id: str  # UUID or hash(professor_id + lab_name)
    professor_id: str  # Links to Professor.id
    professor_name: str  # PI name
    department: str
    lab_name: str  # Lab official name
    lab_url: Optional[str] = None  # Website URL
    last_updated: Optional[datetime] = None  # Last site update
    description: str = ""  # Lab overview
    research_focus: list[str] = []  # Research areas
    news_updates: list[str] = []  # Recent news
    website_content: str = ""  # Full text for analysis
    data_quality_flags: list[str] = []  # ["no_website", "scraping_failed", etc.]
```

**Lab Website Discovery Strategies:**
1. **From Professor Record:** Use professor.lab_url if available
2. **From Profile Page:** Search professor's faculty profile for lab link
3. **From Department Page:** Find lab directory link
4. **Pattern Matching:** Try common URL patterns
5. **Search Fallback:** Use department + professor name search

**Common Lab Website Patterns:**
- `{university}/labs/{lab-name}`
- `{department}/research/{professor-name}`
- `{professor-name}.lab.{university}`
- `lab.{professor-name}.{university}`

**Content Extraction Selectors:**
```python
SELECTORS = {
    "description": [".lab-overview", "#about", ".description", "section.about"],
    "research_focus": [".research-areas", "#research", ".focus"],
    "news": [".news", ".updates", "#latest", "section.news"],
    "last_updated": ["meta[name='last-modified']", ".last-updated", "#updated"]
}
```

**Web Scraping Strategy (from Architecture):**
- **Tiered Fallback:** Built-in → Playwright → Skip with Flag
- Try Claude Agent SDK built-in tools first
- Fallback to Playwright for JS-heavy or dynamic pages
- Skip with data quality flag if both fail

**Error Handling Pattern:**
- Retry Policy: Exponential backoff (max 3 retries, 1s initial delay)
- Timeout: 30 seconds per page
- Missing websites: Flag but continue (rely on publications)
- Scraping failures: Flag with "scraping_failed", use partial data

**Critical Rules (from Coding Standards):**
- Web scraping must try built-in tools first
- Always type hint function signatures
- Never use print() for logging (use structlog)
- Use checkpoint_manager.save_batch() - never write JSONL directly

**Architecture Component Diagram Flow:**
```
CLI Coordinator → Lab Research Agent
Lab Research Agent → Website Discovery (find lab URL)
Lab Research Agent → Web Scraping (built-in → Playwright fallback)
Lab Research Agent → Content Extraction (parse structured data)
Lab Research Agent → Checkpoint Manager (save lab data)
Lab Research Agent → Progress Tracker (display progress)
```

### Testing

**Test File Location:** `tests/integration/test_lab_website_scraping.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Integration tests with mock HTML fixtures
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Unit test for Lab model validation
2. Test lab website discovery strategies
3. Test web scraping with mock lab HTML
4. Test fallback (built-in failure → Playwright success)
5. Test last update date extraction
6. Test missing website handling
7. Test checkpoint saving

**Example Test Pattern:**
```python
def test_scrape_lab_website_with_mock_html():
    # Arrange
    mock_html = """
    <div class="lab-overview">
        <h2>About Our Lab</h2>
        <p>We research machine learning and AI.</p>
    </div>
    <div class="research-areas">
        <li>Deep Learning</li>
        <li>Computer Vision</li>
    </div>
    <div class="news">
        <p>2025-10-01: New paper published</p>
    </div>
    """
    # Mock web fetch to return mock_html

    agent = LabResearchAgent()

    # Act
    lab_data = agent.scrape_lab_website("https://lab.edu")

    # Assert
    assert "machine learning" in lab_data["description"].lower()
    assert "Deep Learning" in lab_data["research_focus"]
    assert len(lab_data["news_updates"]) > 0
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
