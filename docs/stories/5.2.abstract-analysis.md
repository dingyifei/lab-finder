# Story 5.2: Abstract & Acknowledgment Analysis

## Status

**Draft**

## Story

**As a** user,
**I want** paper abstracts and acknowledgment sections analyzed for research alignment,
**so that** the system focuses on most relevant content without processing full papers.

## Acceptance Criteria

1. Abstracts extracted from publication data (FR30)
2. Acknowledgment sections extracted where available
3. LLM analyzes abstract content for alignment with user research interests
4. Acknowledgments analyzed for collaboration patterns and funding sources
5. Relevance score generated for each paper
6. Analysis optimized (abstracts only, not full text) for efficiency
7. Missing abstracts handled (use title as fallback)

## Tasks / Subtasks

- [ ] **Task 1: Extract Abstracts** (AC: 1, 7)
  - [ ] Get abstracts from publication metadata
  - [ ] If missing, flag and use title only
  - [ ] Store in Publication.abstract

- [ ] **Task 2: Extract Acknowledgments** (AC: 2)
  - [ ] Look for acknowledgment section in paper metadata
  - [ ] Parse collaborator names and funding sources
  - [ ] Store in Publication.acknowledgments field

- [ ] **Task 3: LLM-Based Abstract Relevance Analysis** (AC: 3, 5)
  - [ ] Prompt LLM with user interests + abstract
  - [ ] Request relevance score (0-100)
  - [ ] Store in Publication.relevance_score

- [ ] **Task 4: Analyze Acknowledgments for Collaborations** (AC: 4)
  - [ ] Extract collaborator names from acknowledgments
  - [ ] Identify funding agencies
  - [ ] Store collaboration data for later analysis

- [ ] **Task 5: Update Publication Model** (AC: 2, 5)
  - [ ] Add fields:
    ```python
    acknowledgments: str = ""
    relevance_score: int = 0  # 0-100
    collaborators: list[str] = []
    ```

## Dependencies

**Depends On:**
- Story 5.1 complete (same agent execution - runs sequentially within `src/agents/publication_retrieval.py`)

**Execution Model:**
- Runs WITHIN `src/agents/publication_retrieval.py` as part of Epic 5A pipeline
- Executes in-order: Story 5.1 → **5.2** → 5.3 → Checkpoint save
- Modifies Publication objects in-place (enrichment pattern)
- NOT a separate agent or file

**Data Flow:**
1. Input: Publication objects from Story 5.1 (with title, authors, journal, year, abstract)
2. Processing: LLM analyzes abstracts for relevance; extracts acknowledgments
3. Output: Enriched Publication objects (+ relevance_score, acknowledgments) to Story 5.3

**Blocks:**
- Story 5.3 (provides analyzed publications for journal weighting)
- Epic 5B (provides analyzed PI publications with relevance scores)

**Reference:**
- Agent Definition: `docs/architecture.md` (lines 827-856, Publication Retrieval Agent)
- Step 2: "Analyze abstract against user research interests"

## Testing

### Test Approach

**Unit Tests:**
- Abstract extraction from Publication objects
- Relevance score calculation (0-100 range validation)
- Acknowledgment section parsing
- Collaborator name extraction logic
- Funding source identification
- Missing abstract fallback to title

**Integration Tests:**
- End-to-end LLM-based abstract analysis
- Relevance score consistency across similar abstracts
- Publication enrichment (fields added correctly)

**Mocking Strategy:**
- Mock LLM responses for relevance scoring using pytest-mock
- Use deterministic scores for unit tests (no actual LLM calls)
- Mock Claude Agent SDK calls in unit tests
- Integration tests use real LLM with small sample set (5-10 abstracts)

**Test Framework:**
- pytest 7.4.4
- pytest-asyncio 0.23.3 for async LLM calls
- pytest-mock 3.12.0 for LLM mocking

### Key Test Scenarios

**1. High Relevance Abstract**
- Input: Abstract on "deep learning for robotics" + User interests "AI and robotics"
- Expected: relevance_score >= 80
- Validates: AC 3 - LLM correctly identifies high alignment

**2. Low Relevance Abstract**
- Input: Abstract on "medieval literature" + User interests "machine learning"
- Expected: relevance_score <= 20
- Validates: AC 3 - LLM correctly identifies low alignment

**3. Missing Abstract Fallback**
- Input: Publication with empty abstract field
- Expected: Title used for relevance analysis, logged warning
- Validates: AC 7 - Missing abstracts handled (use title as fallback)

**4. Acknowledgment Extraction**
- Input: Publication with acknowledgment section
- Expected: Collaborators and funding sources extracted
- Validates: AC 2, 4 - Acknowledgment sections extracted and analyzed

**5. Publication Enrichment**
- Input: Publication object from Story 5.1 (title, authors, journal only)
- Expected: Publication updated with relevance_score, acknowledgments, collaborators fields
- Validates: AC 5 - Fields added to Publication model

**6. Relevance Score Boundary Validation**
- Input: Various abstracts
- Expected: All scores in range 0-100, integer values
- Validates: Score normalization and validation

**7. Batch Abstract Processing**
- Input: 20 publications for single professor
- Expected: All analyzed without errors, scores assigned
- Validates: Pipeline efficiency, no blocking on LLM calls

### Success Criteria

- ✅ Relevance scores in valid range (0-100) for 100% of publications
- ✅ High-relevance abstracts (user domain match) score >70
- ✅ Low-relevance abstracts (unrelated domains) score <30
- ✅ Missing abstracts don't crash processing, use title fallback
- ✅ Acknowledgments extracted when present (>80% extraction rate)
- ✅ Collaborator names parsed from acknowledgments correctly
- ✅ Publication objects successfully enriched with new fields
- ✅ LLM prompts generate consistent scores (variance <15 for same abstract)
- ✅ Test coverage >75% for abstract analysis functions

### Special Testing Considerations

**LLM Response Mocking:**
```python
# Example: Mock LLM relevance scoring
@pytest.fixture
def mock_llm_relevance_response():
    return {
        "relevance_score": 85,
        "rationale": "Abstract discusses deep learning in robotics, "
                     "directly aligned with user's AI research interests."
    }

@pytest.mark.asyncio
async def test_analyze_abstract(mocker, mock_llm_relevance_response):
    mocker.patch('src.agents.publication_retrieval.analyze_abstract_llm',
                 return_value=mock_llm_relevance_response)

    pub = Publication(
        title="Deep Learning for Robotics",
        abstract="We present a novel deep learning approach...",
        authors=["Jane Smith"]
    )
    user_profile = UserProfile(interests=["AI", "robotics"])

    score = await analyze_abstract(pub, user_profile)

    assert score == 85
    assert 0 <= score <= 100
```

**Relevance Score Consistency Testing:**
- Test same abstract multiple times to ensure score variance <15
- Use temperature=0 for LLM calls to reduce randomness
- Log cases where variance exceeds threshold for review

**Edge Cases:**
- Very short abstracts (< 50 characters)
- Very long abstracts (> 2000 characters)
- Non-English abstracts (graceful handling)
- Malformed acknowledgment sections

**Acknowledgment Parsing:**
- Test various acknowledgment formats (NSF grant numbers, collaborator lists)
- Handle both structured and unstructured acknowledgments
- Extract funding agencies: NSF, NIH, DARPA, etc.

**Integration Test Sample Size:**
- Use 5-10 real abstracts for integration tests (LLM calls expensive)
- Cache LLM responses for repeated test runs
- Consider rate limiting for integration test suite

## Dev Notes

### Integration with Epic 5A Pipeline

**IMPORTANT:** This story does NOT create a separate agent or file. Implementation goes in:
- **Modify:** `src/agents/publication_retrieval.py` (created in Story 5.1)
- **Modify:** `src/models/publication.py` (add relevance_score, acknowledgments fields)

**Execution Order within publication_retrieval.py:**
1. Story 5.1: `fetch_publications()` - Query paper-search-mcp
2. **Story 5.2: `analyze_abstracts()` - LLM relevance scoring (THIS STORY)**
3. Story 5.3: `assign_journal_scores()` - SJR lookup
4. Save to checkpoint: `checkpoints/phase-5a-pi-publications-batch-{N}.jsonl`

### Source Tree Location
- Modify: `src/agents/publication_retrieval.py`
- Modify: `src/models/publication.py`

**LLM Relevance Prompt:**
```python
ABSTRACT_RELEVANCE_PROMPT = """
User Research Interests: {interests}

Paper Abstract: {abstract}

Rate relevance (0-100) and explain why.
"""
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |
