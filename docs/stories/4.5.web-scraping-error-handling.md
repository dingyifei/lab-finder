# Story 4.5: Web Scraping Error Handling & Retry Logic

## Status

**Approved**

**Dependencies:** Story 4.1 (modifies `src/agents/lab_research.py`) - COMPLETE (QA score: 95/100)

## Story

**As a** user,
**I want** robust error handling for web scraping failures,
**so that** temporary issues don't derail the entire analysis.

## Acceptance Criteria

1. Retry logic implemented for failed requests (NFR13)
2. Built-in tools tried first, then Playwright fallback (NFR5)
3. Rate limiting prevents being blocked (NFR10, NFR11)
4. robots.txt respected for ethical scraping
5. Timeout handling for slow-loading pages
6. All failures logged with details (NFR17)
7. Graceful degradation: partial data better than no data

## Tasks / Subtasks

**NOTE:** Tasks are ordered by dependency - implement in sequence for best results.

- [ ] **Task 1: Implement Tiered Fallback** (AC: 2)
  - [ ] Implement `built_in_fetch()` using ClaudeSDKClient WebFetch tool
  - [ ] Implement `playwright_fetch()` for JS-heavy pages
  - [ ] Try built-in tools first, fallback to Playwright on failure
  - [ ] Raise exception if both methods fail (will trigger retry logic)

- [ ] **Task 2: Implement Retry Logic** (AC: 1)
  - [ ] Implement `fetch_with_retry()` using tenacity decorator
  - [ ] Use exponential backoff: max 3 retries, 1s initial delay, 10s max delay
  - [ ] Retry on network errors and timeouts
  - [ ] Call Task 1's `built_in_fetch()` and `playwright_fetch()` within retry loop

- [ ] **Task 3: Implement Timeouts** (AC: 5)
  - [ ] Implement `fetch_with_timeout()` wrapper using asyncio.wait_for
  - [ ] 30 second timeout per page
  - [ ] Call Task 2's `fetch_with_retry()` within timeout wrapper
  - [ ] Handle timeout gracefully and log occurrences

- [ ] **Task 4: Add Rate Limiting** (AC: 3)
  - [ ] Implement DomainRateLimiter class using aiolimiter AsyncLimiter
  - [ ] Limit requests per domain: 1 req/second default
  - [ ] Configurable rate limits in system_params.json
  - [ ] Per-domain tracking (different domains have independent limits)

- [ ] **Task 5: Respect robots.txt** (AC: 4)
  - [ ] Implement `check_robots_txt()` using urllib.robotparser
  - [ ] Check robots.txt before scraping
  - [ ] Skip if disallowed, flag accordingly
  - [ ] Gracefully handle missing robots.txt (assume allowed)

- [ ] **Task 6: Implement Comprehensive Error Logging** (AC: 6)
  - [ ] Log all failure types with structured logger
  - [ ] Include failure context (URL, error type, retry count)
  - [ ] Add correlation IDs to all log statements
  - [ ] Log timeout occurrences with duration
  - [ ] Log robots.txt blocking events

- [ ] **Task 7: Implement Graceful Degradation Pattern** (AC: 7)
  - [ ] Continue processing on web scraping failures
  - [ ] Set appropriate data quality flags for failed operations
  - [ ] Return partial Lab records with flags instead of failing
  - [ ] Ensure pipeline continues with degraded data

- [ ] **Task 8: Integrate Error Handling into Lab Research Agent** (AC: all)
  - [ ] Create `src/utils/web_scraper.py` with all utility functions from Tasks 1-7
  - [ ] Import error handling utilities into `src/agents/lab_research.py`
  - [ ] Add `DomainRateLimiter` instance to `LabResearchAgent.__init__()`
  - [ ] Update `scrape_lab_website()` to use `fetch_with_timeout()` instead of direct WebFetch
  - [ ] Add robots.txt check before scraping (call `check_robots_txt()`)
  - [ ] Apply rate limiting via `rate_limiter.acquire()`
  - [ ] Update Lab model with new error-related data quality flags (`robots_txt_blocked`)
  - [ ] Test integration with existing Story 4.1 implementation

## Dev Notes

### Relevant Architecture Information

**Component:** Lab Research Agent - Web Scraping Error Handler (Epic 4)

**Responsibility:** Robust web scraping with retry logic and tiered fallback (Epic 4: NFR5, NFR13)

**Key Interfaces:**
- `fetch_with_retry(url: str, timeout: int = 30) -> str` - Fetch with retry and fallback
- `check_robots_txt(url: str) -> bool` - Verify scraping allowed

**Dependencies:**
- **Story 4.1:** Provides `src/agents/lab_research.py` and `src/models/lab.py` (COMPLETE - QA approved)
- Claude Agent SDK built-in web tools (primary)
- Playwright (fallback)
- tenacity for retry logic
- robotparser for robots.txt compliance

**Technology Stack:**
- tenacity 9.1.2 for exponential backoff (verified in requirements.txt)
- Playwright 1.55.0 as fallback (verified in requirements.txt)
- urllib.robotparser for robots.txt (Python stdlib)
- structlog 25.4.0 for error logging (verified in requirements.txt)
- aiolimiter 1.2.1 for rate limiting (verified in requirements.txt)

**Source Tree Location:**
- Modify: `src/agents/lab_research.py` (integrate error handling into existing Story 4.1 implementation)
- Create: `src/utils/web_scraper.py` (reusable web scraping helpers)

**Checkpoint Strategy:**
- This story operates **within Story 4.1's checkpoint batches** (`phase-4-labs-batch-N.jsonl`)
- No separate checkpoints for Story 4.5 (error handling is integrated into existing batch flow)
- Resumability handled by Story 4.1's checkpoint manager

**Web Scraping Tiered Fallback Strategy (from Architecture):**
```
1. Try Claude Agent SDK built-in web tools (fast, simple)
   ↓ (on failure)
2. Try Playwright (handles JS, authentication)
   ↓ (on failure)
3. Flag with data quality issue, continue with partial data
```

**Claude Agent SDK WebFetch Tool Access Pattern:**

The `built_in_fetch()` function uses Claude Agent SDK's WebFetch tool via `ClaudeSDKClient`:

```python
from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions

async def built_in_fetch(url: str, timeout: int = 30) -> str:
    """Fetch URL using Claude Agent SDK WebFetch tool."""
    options = ClaudeAgentOptions(
        max_turns=1,              # Stateless one-shot
        allowed_tools=["WebFetch"],  # Enable WebFetch tool
        setting_sources=None,     # CRITICAL: Prevent codebase context injection
        system_prompt="You are a web scraping assistant. Extract all page content from the provided URL."
    )

    async with ClaudeSDKClient(options=options) as client:
        await client.query(f"Fetch and extract all content from: {url}")

        # Process response
        async for message in client.receive_response():
            if message.get("type") == "text":
                return message.get("content", "")

    raise Exception(f"WebFetch failed for {url}")
```

**Playwright Invocation via Bash Tool:**

The `playwright_fetch()` function uses Playwright via the Bash tool (per Tech Stack guidance):

```python
async def playwright_fetch(url: str, timeout: int = 30) -> str:
    """Fetch URL using Playwright via Bash tool."""
    from asyncio import create_subprocess_shell, subprocess

    # Playwright script to fetch page content
    playwright_cmd = f"""
    playwright codegen --target python -o /dev/null {url} && \\
    playwright python -c "
from playwright.async_api import async_playwright
import asyncio

async def fetch():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto('{url}', timeout={timeout * 1000})
        content = await page.content()
        await browser.close()
        print(content)

asyncio.run(fetch())
"
    """

    process = await create_subprocess_shell(
        playwright_cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )

    stdout, stderr = await process.communicate()

    if process.returncode != 0:
        raise Exception(f"Playwright failed: {stderr.decode()}")

    return stdout.decode()
```

**Note:** The above Playwright implementation is a reference. Consider using the existing Playwright patterns from Story 4.1 if already implemented.

**Retry Logic with Tenacity:**
```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),
    reraise=True
)
async def fetch_with_retry(url: str, timeout: int = 30) -> str:
    """Fetch URL with retry logic and tiered fallback."""
    logger.debug(f"Fetching {url} with built-in tools")

    # Try built-in tools first (NFR5)
    try:
        content = await built_in_fetch(url, timeout=timeout)
        logger.info(f"Built-in tools succeeded for {url}")
        return content
    except Exception as e:
        logger.warning(f"Built-in tools failed for {url}: {e}")

        # Fallback to Playwright
        try:
            logger.debug(f"Falling back to Playwright for {url}")
            content = await playwright_fetch(url, timeout=timeout)
            logger.info(f"Playwright succeeded for {url}")
            return content
        except Exception as e2:
            logger.error(f"Both methods failed for {url}: {e2}")
            raise  # Will be retried by tenacity
```

**Rate Limiting Implementation:**
```python
from aiolimiter import AsyncLimiter
from urllib.parse import urlparse

class DomainRateLimiter:
    """Per-domain rate limiting to prevent blocking."""

    def __init__(self):
        self.limiters = {}  # domain -> AsyncLimiter
        self.default_rate = 1  # 1 request per second default

    async def acquire(self, url: str):
        """Acquire rate limit token for URL's domain."""
        domain = urlparse(url).netloc

        if domain not in self.limiters:
            # Create limiter for new domain
            self.limiters[domain] = AsyncLimiter(
                max_rate=self.default_rate,
                time_period=1.0  # per second
            )

        await self.limiters[domain].acquire()

# Usage
rate_limiter = DomainRateLimiter()

async def fetch_url(url: str):
    await rate_limiter.acquire(url)  # Respect rate limit
    return await fetch_with_retry(url)
```

**robots.txt Compliance:**
```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin

async def check_robots_txt(url: str, user_agent: str = "LabFinder/1.0") -> bool:
    """Check if URL is allowed by robots.txt."""
    try:
        parser = RobotFileParser()
        robots_url = urljoin(url, "/robots.txt")

        # Fetch robots.txt
        robots_content = await fetch_with_timeout(robots_url, timeout=5)
        parser.parse(robots_content.splitlines())

        # Check if URL is allowed
        allowed = parser.can_fetch(user_agent, url)

        if not allowed:
            logger.warning(f"robots.txt disallows scraping {url}")

        return allowed

    except Exception as e:
        logger.debug(f"Could not check robots.txt for {url}: {e}")
        # If robots.txt unavailable, assume allowed
        return True
```

**Timeout Handling:**
```python
import asyncio

async def fetch_with_timeout(url: str, timeout: int = 30) -> str:
    """Fetch URL with timeout."""
    try:
        return await asyncio.wait_for(
            fetch_with_retry(url),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        logger.warning(f"Timeout after {timeout}s fetching {url}")
        raise
```

**Graceful Degradation Pattern:**
```python
async def scrape_lab_website_safe(lab: Lab) -> Lab:
    """Scrape lab website with comprehensive error handling."""
    if not lab.lab_url:
        lab.data_quality_flags.append("no_website")
        return lab

    # Check robots.txt
    if not await check_robots_txt(lab.lab_url):
        lab.data_quality_flags.append("robots_txt_blocked")
        return lab

    try:
        # Rate limit
        await rate_limiter.acquire(lab.lab_url)

        # Fetch with retry and fallback
        content = await fetch_with_timeout(lab.lab_url, timeout=30)

        # Parse content
        lab.website_content = content
        lab.description = extract_description(content)
        lab.research_focus = extract_research_focus(content)

    except Exception as e:
        logger.error(f"Failed to scrape {lab.lab_url}: {e}")
        lab.data_quality_flags.append("scraping_failed")
        # Continue processing with partial data

    return lab
```

**Integration Points in `lab_research.py` (Task 8):**

The error handling components must be integrated into the existing `LabResearchAgent.scrape_lab_website()` method from Story 4.1:

```python
# In src/agents/lab_research.py

from src.utils.web_scraper import (
    fetch_with_retry,
    check_robots_txt,
    DomainRateLimiter,
    fetch_with_timeout
)

class LabResearchAgent:
    def __init__(self):
        self.rate_limiter = DomainRateLimiter()  # Initialize rate limiter
        # ... other initialization

    async def scrape_lab_website(self, lab: Lab, correlation_id: str) -> Lab:
        """
        Scrape lab website with error handling integration.
        REPLACES/ENHANCES Story 4.1 implementation.
        """
        logger = get_logger(correlation_id=correlation_id, component="lab_research")

        if not lab.lab_url:
            lab.data_quality_flags.append("no_website")
            return lab

        # NEW: Check robots.txt before scraping
        if not await check_robots_txt(lab.lab_url):
            lab.data_quality_flags.append("robots_txt_blocked")
            logger.warning("robots.txt blocks scraping", url=lab.lab_url)
            return lab

        try:
            # NEW: Apply rate limiting
            await self.rate_limiter.acquire(lab.lab_url)

            # NEW: Use fetch_with_retry instead of direct WebFetch
            content = await fetch_with_timeout(lab.lab_url, timeout=30)

            # EXISTING: Parse content (from Story 4.1)
            lab.website_content = content
            lab.description = self._extract_description(content)
            lab.research_focus = self._extract_research_focus(content)
            # ... other parsing

        except Exception as e:
            # NEW: Graceful degradation instead of re-raising
            logger.error("Failed to scrape lab website", url=lab.lab_url, error=str(e))
            lab.data_quality_flags.append("scraping_failed")
            # Continue processing with partial data

        return lab
```

**Key Integration Changes:**
1. Import error handling utilities from `web_scraper.py`
2. Add `DomainRateLimiter` instance to `LabResearchAgent.__init__()`
3. Add robots.txt check before scraping
4. Replace direct WebFetch with `fetch_with_timeout()` (which uses `fetch_with_retry()`)
5. Apply rate limiting via `rate_limiter.acquire()`
6. Change exception handling from re-raise to graceful degradation

**New Data Quality Flags:**

Add the following flags to `src/models/lab.py` constants (if not already present from Story 4.1):

```python
LAB_DATA_QUALITY_FLAGS = [
    "no_website",              # From Story 4.1
    "scraping_failed",         # From Story 4.1
    "playwright_fallback",     # From Story 4.1
    "robots_txt_blocked",      # NEW for Story 4.5
    # ... other existing flags
]
```

**Critical Rules (from Coding Standards):**
- Web scraping must try built-in tools first (NFR5)
- Respect robots.txt for ethical scraping (NFR10)
- Implement per-domain rate limiting (NFR11)
- Never crash on scraping failures (NFR13)
- Never use print() for logging (use structlog)

**Architecture Component Diagram Flow:**
```
Lab Research Agent → robots.txt Checker
  ↓ (if allowed)
Rate Limiter (per domain)
  ↓
Built-in Web Tools (try first)
  ↓ (on failure)
Playwright Fallback
  ↓ (on failure)
Retry Logic (tenacity, 3 attempts)
  ↓ (on final failure)
Graceful Degradation (flag + continue)
  ↓
Lab Model Update (with flags)
  ↓
Checkpoint Manager (save with error flags)
```

### Testing

**Test File Location:** `tests/unit/test_web_scraping_errors.py`

**Testing Standards:**
- Framework: pytest 8.4.2 with pytest-asyncio
- Mock web requests and failures
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Test retry logic with exponential backoff
2. Test tiered fallback (built-in → Playwright → fail)
3. Test rate limiting enforcement per domain
4. Test robots.txt compliance checking
5. Test timeout handling
6. Test graceful degradation (partial data)
7. Test error logging for all failure types
8. Integration test with mock HTTP server

**Example Test Pattern:**
```python
@pytest.mark.asyncio
async def test_fetch_with_retry_fallback(mocker):
    # Arrange
    mock_builtin = mocker.patch('src.utils.web_scraper.built_in_fetch')
    mock_builtin.side_effect = ConnectionError("Built-in failed")

    mock_playwright = mocker.patch('src.utils.web_scraper.playwright_fetch')
    mock_playwright.return_value = "<html>Success via Playwright</html>"

    # Act
    result = await fetch_with_retry("https://test.edu")

    # Assert
    assert "Success via Playwright" in result
    assert mock_builtin.call_count == 3  # Retried 3 times
    assert mock_playwright.call_count == 3  # Fallback tried each time

@pytest.mark.asyncio
async def test_graceful_degradation_on_failure(mocker):
    # Arrange
    mocker.patch('src.utils.web_scraper.fetch_with_timeout',
                 side_effect=Exception("All methods failed"))

    lab = Lab(id="lab-1", lab_url="https://test.edu")

    # Act
    result = await scrape_lab_website_safe(lab)

    # Assert
    assert "scraping_failed" in result.data_quality_flags
    assert result.website_content == ""  # Empty but not None
    # Lab object returned, processing continues
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-09 | 0.4 | **TASK ORDERING FIX** - Reordered tasks to match dependency flow: Task 1 (Tiered Fallback) → Task 2 (Retry Logic) → Task 3 (Timeouts) → Tasks 4-7 (independent) → Task 8 (Integration). Added NOTE about sequential implementation. Enhanced subtask descriptions to clarify dependencies. Updated validation score: 9.6→10/10. Story now APPROVED for implementation. | Sarah (PO) |
| 2025-10-09 | 0.3 | **VALIDATION FIXES** - Applied PO validation corrections: (1) Added Story 4.1 dependency note to Status section; (2) Verified all library versions against requirements.txt (all correct); (3) Added checkpoint strategy clarification in Dev Notes; (4) Added LAB_DATA_QUALITY_FLAGS constant reference; (5) Added template placeholder sections (Dev Agent Record, QA Results); (6) Clarified that Story 4.5 operates within Story 4.1 checkpoint batches. Story now ready for implementation (validation score: 8.5→9.5/10). | Sarah (PO) |
| 2025-10-06 | 0.2 | Added missing Tasks 6-8, corrected version numbers, added SDK usage guidance, clarified integration points | Sarah (PO) |
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by agent_

### Debug Log References

_To be populated by agent_

### Completion Notes List

_To be populated by agent_

### File List

_To be populated by agent_

## QA Results

_To be populated by QA agent_
