# Story 4.5: Web Scraping Error Handling & Retry Logic

## Status

**Approved** (v0.5 - Puppeteer MCP Architecture)

**Dependencies:** Story 4.1 (modifies `src/agents/lab_research.py`) - COMPLETE (QA score: 95/100)

## Story

**As a** user,
**I want** robust error handling for web scraping failures,
**so that** temporary issues don't derail the entire analysis.

## Acceptance Criteria

1. Retry logic implemented for failed requests (NFR13)
2. **Built-in tools tried first, then Puppeteer MCP fallback** (NFR5)
3. Rate limiting prevents being blocked (NFR10, NFR11)
4. robots.txt respected for ethical scraping
5. Timeout handling for slow-loading pages
6. All failures logged with details (NFR17)
7. Graceful degradation: partial data better than no data
8. **Sufficiency evaluation determines data completeness between stages**

## Tasks / Subtasks

**NOTE:** Tasks are ordered by dependency - implement in sequence for best results.

- [ ] **Task 1: Implement Multi-Stage Scraping Pattern with Sufficiency Evaluation** (AC: 2, 8)
  - [ ] Implement `scrape_with_sufficiency()` main orchestrator function
  - [ ] Implement `_scrape_with_sdk()` helper for both WebFetch and Puppeteer MCP stages
  - [ ] Stage 1: Extract data using WebFetch tool
  - [ ] Stage 2: Evaluate sufficiency (calls Task 2's `evaluate_sufficiency()`)
  - [ ] Stage 3: Escalate to Puppeteer MCP if insufficient (`mcp__puppeteer__navigate`, `mcp__puppeteer__evaluate`)
  - [ ] Stage 4: Re-evaluate sufficiency after Puppeteer MCP
  - [ ] Loop up to max_attempts (default: 3), return best effort on final failure
  - [ ] Return ScrapingResult with data, sufficient flag, missing_fields, attempts count

- [ ] **Task 2: Implement Sufficiency Evaluation Logic** (AC: 8)
  - [ ] Implement `evaluate_sufficiency()` with strong prompt instructions
  - [ ] LLM analyzes scraped data against required_fields list
  - [ ] Returns JSON: `{"sufficient": bool, "missing_fields": list}`
  - [ ] Retry JSON parsing up to 3 times if malformed
  - [ ] Handle 4 response formats: markdown codeblock, raw JSON, text with JSON, plain text
  - [ ] Conservative fallback: assume insufficient on parse failure
  - [ ] **NOTE:** thinking parameter NOT supported in SDK v0.1.1 - use strong system_prompt

- [ ] **Task 3: Implement Retry Logic with Tenacity** (AC: 1)
  - [ ] Integrate `scrape_with_sufficiency()` with tenacity decorator (optional enhancement)
  - [ ] OR: Implement retry logic within `scrape_with_sufficiency()` loop (simpler)
  - [ ] Use exponential backoff: max 3 retries, 1s initial delay, 10s max delay
  - [ ] Retry on network errors and timeouts
  - [ ] Log retry attempts with correlation IDs

- [ ] **Task 4: Implement Timeouts** (AC: 5)
  - [ ] Implement `fetch_with_timeout()` wrapper using asyncio.wait_for
  - [ ] 30 second timeout per page (configurable)
  - [ ] Wrap `scrape_with_sufficiency()` calls
  - [ ] Handle timeout gracefully with TimeoutError exception
  - [ ] Log timeout occurrences with URL and duration

- [ ] **Task 5: Add Rate Limiting** (AC: 3)
  - [ ] Implement DomainRateLimiter class using aiolimiter AsyncLimiter
  - [ ] Limit requests per domain: 1 req/second default
  - [ ] Configurable rate limits in system_params.json
  - [ ] Per-domain tracking (different domains have independent limits)
  - [ ] Async context manager pattern for rate limit acquisition

- [ ] **Task 6: Respect robots.txt** (AC: 4)
  - [ ] Implement `check_robots_txt()` using urllib.robotparser
  - [ ] Check robots.txt before scraping
  - [ ] Skip if disallowed, add `robots_txt_blocked` flag
  - [ ] Gracefully handle missing robots.txt (assume allowed)
  - [ ] Use UserAgent: "LabFinder/1.0"

- [ ] **Task 7: Implement Comprehensive Error Logging** (AC: 6)
  - [ ] Log all failure types with structured logger
  - [ ] Include failure context (URL, error type, retry count, attempt number)
  - [ ] Add correlation IDs to all log statements
  - [ ] Log timeout occurrences with duration
  - [ ] Log robots.txt blocking events
  - [ ] Log sufficiency evaluation results (sufficient/insufficient, missing fields)
  - [ ] Log Puppeteer MCP escalations

- [ ] **Task 8: Implement Graceful Degradation Pattern** (AC: 7)
  - [ ] Continue processing on web scraping failures
  - [ ] Set appropriate data quality flags for failed operations
  - [ ] Return partial Lab records with flags instead of failing
  - [ ] Ensure pipeline continues with degraded data
  - [ ] Add flags: `insufficient_webfetch`, `puppeteer_mcp_used`, `sufficiency_evaluation_failed`

- [ ] **Task 9: Create Web Scraping Utility Module** (AC: all)
  - [ ] Create `src/utils/web_scraping.py` with all functions from Tasks 1-8
  - [ ] Export public interface: `scrape_with_sufficiency()`, `evaluate_sufficiency()`, `DomainRateLimiter`, `check_robots_txt()`
  - [ ] Define ScrapingResult TypedDict return type
  - [ ] Add comprehensive docstrings with usage examples
  - [ ] Ensure all functions use correlation_id parameter for logging

- [ ] **Task 10: Integrate Error Handling into Lab Research Agent** (AC: all)
  - [ ] Import error handling utilities into `src/agents/lab_research.py`
  - [ ] Add `DomainRateLimiter` instance to `LabResearchAgent.__init__()`
  - [ ] Update `scrape_lab_website()` to use `scrape_with_sufficiency()` instead of direct WebFetch
  - [ ] Add robots.txt check before scraping (call `check_robots_txt()`)
  - [ ] Apply rate limiting via `rate_limiter.acquire()`
  - [ ] Update Lab model with new error-related data quality flags
  - [ ] Test integration with existing Story 4.1 implementation

## Dev Notes

### Relevant Architecture Information

**Component:** Lab Research Agent - Web Scraping Error Handler (Epic 4)

**Responsibility:** Robust web scraping with multi-stage pattern, sufficiency evaluation, and retry logic (Epic 4: NFR5, NFR13)

**Key Interfaces:**
- `scrape_with_sufficiency(url, required_fields, max_attempts, correlation_id) -> ScrapingResult` - Multi-stage scraping orchestrator
- `evaluate_sufficiency(data, required_fields, correlation_id) -> dict` - Data completeness evaluator
- `check_robots_txt(url) -> bool` - Verify scraping allowed
- `DomainRateLimiter.acquire(url)` - Rate limiting per domain

**Dependencies:**
- **Story 4.1:** Provides `src/agents/lab_research.py` and `src/models/lab.py` (COMPLETE - QA approved)
- Claude Agent SDK built-in WebFetch tool (primary)
- **Puppeteer MCP** (@modelcontextprotocol/server-puppeteer) for JS-heavy sites
- tenacity 9.1.2 for exponential backoff (optional)
- urllib.robotparser for robots.txt compliance (Python stdlib)
- structlog 25.4.0 for error logging
- aiolimiter 1.2.1 for rate limiting

**Technology Stack:**
- **Puppeteer MCP:** @modelcontextprotocol/server-puppeteer (configured in `claude/.mcp.json`)
- tenacity 9.1.2 for exponential backoff (verified in requirements.txt)
- urllib.robotparser for robots.txt (Python stdlib)
- structlog 25.4.0 for error logging (verified in requirements.txt)
- aiolimiter 1.2.1 for rate limiting (verified in requirements.txt)
- **Note:** Direct Playwright library usage deprecated - use Puppeteer MCP instead

**Source Tree Location:**
- **Create:** `src/utils/web_scraping.py` (reusable web scraping helpers)
- **Modify:** `src/agents/lab_research.py` (integrate error handling into existing Story 4.1 implementation)

**Checkpoint Strategy:**
- This story operates **within Story 4.1's checkpoint batches** (`phase-4-labs-batch-N.jsonl`)
- No separate checkpoints for Story 4.5 (error handling is integrated into existing batch flow)
- Resumability handled by Story 4.1's checkpoint manager

**Multi-Stage Web Scraping Pattern (from Architecture):**
```
Stage 1: ClaudeSDKClient + WebFetch
  ↓
Stage 2: Sufficiency Evaluation (separate LLM prompt)
  ↓ (if sufficient)
Return data ✓
  ↓ (if insufficient)
Stage 3: ClaudeSDKClient + Puppeteer MCP
  ↓
Stage 4: Re-evaluate Sufficiency
  ↓ (if sufficient)
Return data ✓
  ↓ (if insufficient and attempts < max)
Loop back to Stage 3
  ↓ (if max attempts reached)
Return partial data + missing_fields flag
```

### Implementation Patterns

**1. Multi-Stage Scraping Pattern (Task 1)**

```python
from typing import TypedDict
from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions
from pathlib import Path
from src.utils.logger import get_logger

class ScrapingResult(TypedDict):
    """Result from multi-stage web scraping."""
    data: dict
    sufficient: bool
    missing_fields: list[str]
    attempts: int


async def scrape_with_sufficiency(
    url: str,
    required_fields: list[str],
    max_attempts: int = 3,
    correlation_id: str = ""
) -> ScrapingResult:
    """Multi-stage web scraping with sufficiency evaluation.

    Stages:
    1. WebFetch Attempt → Extract data using built-in tools
    2. Sufficiency Evaluation → LLM determines if data complete
    3. Puppeteer MCP Fallback → Use browser automation if insufficient
    4. Re-evaluate → Check completeness again
    5. Retry or Return → Loop up to max_attempts, then return best effort

    Args:
        url: Target URL to scrape
        required_fields: List of required data fields
        max_attempts: Maximum scraping attempts (default: 3)
        correlation_id: Correlation ID for logging

    Returns:
        ScrapingResult with data, sufficiency flag, missing fields, attempts

    Data Quality Flags:
        - insufficient_webfetch: WebFetch didn't extract all required fields
        - puppeteer_mcp_used: Escalated to Puppeteer MCP fallback
        - sufficiency_evaluation_failed: Could not determine completeness
    """
    logger = get_logger(correlation_id=correlation_id, component="web-scraping")
    attempt = 0
    scraped_data = {}

    while attempt < max_attempts:
        attempt += 1
        logger.info("Starting scraping attempt", url=url, attempt=attempt, max_attempts=max_attempts)

        # Stage 1 or 3: Scrape with appropriate tool
        if attempt == 1:
            # Use WebFetch (fast, simple)
            logger.debug("Stage 1: Using WebFetch", url=url)
            try:
                scraped_data = await _scrape_with_webfetch(url, required_fields, correlation_id)
            except Exception as e:
                logger.warning("WebFetch failed", url=url, error=str(e))
                scraped_data = {}
        else:
            # Use Puppeteer MCP (handles JS-rendered content)
            logger.debug("Stage 3: Using Puppeteer MCP", url=url)
            try:
                scraped_data = await _scrape_with_puppeteer_mcp(url, required_fields, correlation_id)
            except Exception as e:
                logger.error("Puppeteer MCP failed", url=url, error=str(e))
                # Continue with existing scraped_data (may be partial)

        # Stage 2 or 4: Evaluate sufficiency
        logger.debug("Evaluating sufficiency", url=url, attempt=attempt)
        sufficiency_result = await evaluate_sufficiency(
            scraped_data, required_fields, correlation_id
        )

        if sufficiency_result["sufficient"]:
            logger.info("Scraping sufficient", url=url, attempt=attempt)
            return ScrapingResult(
                data=scraped_data,
                sufficient=True,
                missing_fields=[],
                attempts=attempt
            )

        logger.warning(
            "Scraping insufficient, retrying",
            url=url,
            attempt=attempt,
            missing=sufficiency_result["missing_fields"]
        )

    # Max attempts reached - return best effort
    logger.error("Max scraping attempts reached", url=url, attempts=max_attempts)
    return ScrapingResult(
        data=scraped_data,
        sufficient=False,
        missing_fields=sufficiency_result.get("missing_fields", required_fields),
        attempts=max_attempts
    )


async def _scrape_with_webfetch(
    url: str,
    required_fields: list[str],
    correlation_id: str
) -> dict:
    """Scrape URL using WebFetch tool."""
    options = ClaudeAgentOptions(
        cwd=Path(__file__).parent.parent.parent / "claude",
        setting_sources=None,  # Isolated context
        allowed_tools=["WebFetch"],
        max_turns=2,
        system_prompt="You are a web scraping assistant. Extract the requested data fields from the provided URL."
    )

    prompt = f"""Scrape the following URL and extract these data fields:
URL: {url}
Required fields: {', '.join(required_fields)}

Return the extracted data as a JSON object with the field names as keys.
"""

    response_text = ""
    async with ClaudeSDKClient(options=options) as client:
        await client.query(prompt)

        async for message in client.receive_response():
            if hasattr(message, "content") and message.content:
                for block in message.content:
                    if hasattr(block, "text"):
                        response_text += block.text

    # Parse JSON from response
    return _parse_json_from_text(response_text)


async def _scrape_with_puppeteer_mcp(
    url: str,
    required_fields: list[str],
    correlation_id: str
) -> dict:
    """Scrape URL using Puppeteer MCP tools."""
    options = ClaudeAgentOptions(
        cwd=Path(__file__).parent.parent.parent / "claude",
        setting_sources=["project"],  # Loads .mcp.json
        allowed_tools=["mcp__puppeteer__navigate", "mcp__puppeteer__evaluate"],
        max_turns=5,
        system_prompt="You are a web scraping assistant using Puppeteer MCP. Navigate to the URL and extract the requested data."
    )

    prompt = f"""Use Puppeteer MCP to scrape the following URL:
URL: {url}
Required fields: {', '.join(required_fields)}

Steps:
1. Use mcp__puppeteer__navigate to load the page
2. Use mcp__puppeteer__evaluate to execute JavaScript and extract data
3. Return the extracted data as a JSON object

Return the extracted data as a JSON object with the field names as keys.
"""

    response_text = ""
    async with ClaudeSDKClient(options=options) as client:
        await client.query(prompt)

        async for message in client.receive_response():
            if hasattr(message, "content") and message.content:
                for block in message.content:
                    if hasattr(block, "text"):
                        response_text += block.text

    # Parse JSON from response
    return _parse_json_from_text(response_text)


def _parse_json_from_text(text: str) -> dict:
    """Parse JSON from various text formats."""
    import json
    import re

    # Try 4 formats: markdown codeblock, raw JSON, text with JSON, plain text
    patterns = [
        r'```json\s*(.*?)\s*```',  # Markdown codeblock
        r'\{.*\}',                   # Raw JSON object
        r'```\s*(.*?)\s*```',       # Generic codeblock
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1) if '```' in pattern else match.group(0))
            except json.JSONDecodeError:
                continue

    # Fallback: try parsing entire text
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        return {}
```

**2. Sufficiency Evaluation (Task 2)**

```python
import json
from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions
from pathlib import Path


async def evaluate_sufficiency(
    data: dict,
    required_fields: list[str],
    correlation_id: str
) -> dict:
    """Evaluate if scraped data is sufficient.

    NOTE: thinking parameter NOT supported in SDK v0.1.1.
    Uses strong system_prompt instructions instead.

    Args:
        data: Scraped data to evaluate
        required_fields: List of required fields
        correlation_id: Correlation ID for logging

    Returns:
        Dict with 'sufficient' (bool) and 'missing_fields' (list)
    """
    logger = get_logger(correlation_id=correlation_id, component="sufficiency-eval")

    options = ClaudeAgentOptions(
        cwd=Path(__file__).parent.parent.parent / "claude",
        setting_sources=None,  # Isolated context
        allowed_tools=[],
        max_turns=1,
        system_prompt=(
            "You are a data quality evaluator. Analyze scraped data and determine "
            "if it contains all required fields. Think through your analysis step-by-step:\n"
            "1. Check each required field for presence\n"
            "2. Verify field values are non-empty\n"
            "3. Determine overall sufficiency\n\n"
            "Output ONLY a JSON object with 'sufficient' (boolean) and 'missing_fields' (array). "
            "Do not include any explanatory text outside the JSON."
        )
    )

    prompt = f"""Evaluate this scraped data:
{json.dumps(data, indent=2)}

Required fields: {required_fields}

Output format (JSON only):
{{
  "sufficient": true/false,
  "missing_fields": ["field1", "field2"]
}}
"""

    # Retry JSON parsing up to 3 times
    for retry in range(3):
        try:
            response_text = ""
            async with ClaudeSDKClient(options=options) as client:
                await client.query(prompt)

                async for message in client.receive_response():
                    if hasattr(message, "content") and message.content:
                        for block in message.content:
                            if hasattr(block, "text"):
                                response_text += block.text

            result = _parse_json_from_text(response_text)

            # Validate result structure
            if "sufficient" in result and "missing_fields" in result:
                logger.info("Sufficiency evaluation complete", sufficient=result["sufficient"], missing=result["missing_fields"])
                return result

        except json.JSONDecodeError:
            logger.warning(f"JSON parse failed, retry {retry+1}/3", correlation_id=correlation_id)
            continue

    # Failed to parse - conservative fallback: assume insufficient
    logger.error("Sufficiency evaluation failed after 3 retries", correlation_id=correlation_id)
    return {"sufficient": False, "missing_fields": required_fields}
```

**3. Rate Limiting (Task 5)**

```python
from aiolimiter import AsyncLimiter
from urllib.parse import urlparse


class DomainRateLimiter:
    """Per-domain rate limiting to prevent blocking."""

    def __init__(self, default_rate: int = 1):
        """Initialize rate limiter.

        Args:
            default_rate: Default requests per second per domain (default: 1)
        """
        self.limiters: dict[str, AsyncLimiter] = {}
        self.default_rate = default_rate

    async def acquire(self, url: str):
        """Acquire rate limit token for URL's domain.

        Args:
            url: Target URL (domain extracted from URL)
        """
        domain = urlparse(url).netloc

        if domain not in self.limiters:
            # Create limiter for new domain
            self.limiters[domain] = AsyncLimiter(
                max_rate=self.default_rate,
                time_period=1.0  # per second
            )

        await self.limiters[domain].acquire()
```

**4. robots.txt Compliance (Task 6)**

```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin
import httpx


async def check_robots_txt(url: str, user_agent: str = "LabFinder/1.0") -> bool:
    """Check if URL is allowed by robots.txt.

    Args:
        url: Target URL to check
        user_agent: User agent string (default: "LabFinder/1.0")

    Returns:
        True if allowed, False if disallowed
    """
    logger = get_logger(component="robots-txt-checker")

    try:
        parser = RobotFileParser()
        robots_url = urljoin(url, "/robots.txt")

        # Fetch robots.txt
        async with httpx.AsyncClient() as client:
            response = await client.get(robots_url, timeout=5.0)
            robots_content = response.text

        parser.parse(robots_content.splitlines())

        # Check if URL is allowed
        allowed = parser.can_fetch(user_agent, url)

        if not allowed:
            logger.warning("robots.txt disallows scraping", url=url)

        return allowed

    except Exception as e:
        logger.debug("Could not check robots.txt, assuming allowed", url=url, error=str(e))
        # If robots.txt unavailable, assume allowed
        return True
```

**5. Integration with Lab Research Agent (Task 10)**

```python
# In src/agents/lab_research.py

from src.utils.web_scraping import (
    scrape_with_sufficiency,
    check_robots_txt,
    DomainRateLimiter,
)
from src.utils.logger import get_logger


class LabResearchAgent:
    def __init__(self):
        self.rate_limiter = DomainRateLimiter(default_rate=1)  # 1 req/sec
        # ... other initialization

    async def scrape_lab_website(self, lab: Lab, correlation_id: str) -> Lab:
        """Scrape lab website with comprehensive error handling.

        REPLACES/ENHANCES Story 4.1 implementation with multi-stage pattern.
        """
        logger = get_logger(correlation_id=correlation_id, component="lab_research")

        if not lab.lab_url:
            lab.data_quality_flags.append("no_website")
            return lab

        # Check robots.txt before scraping
        if not await check_robots_txt(lab.lab_url):
            lab.data_quality_flags.append("robots_txt_blocked")
            logger.warning("robots.txt blocks scraping", url=lab.lab_url)
            return lab

        try:
            # Apply rate limiting
            await self.rate_limiter.acquire(lab.lab_url)

            # Multi-stage scraping with sufficiency evaluation
            result = await scrape_with_sufficiency(
                url=lab.lab_url,
                required_fields=[
                    "lab_information",
                    "contact",
                    "people",
                    "research_focus",
                    "publications"
                ],
                max_attempts=3,
                correlation_id=correlation_id
            )

            # Store scraped data
            lab.website_data = result["data"]

            # Add data quality flags
            if not result["sufficient"]:
                lab.data_quality_flags.append("insufficient_webfetch")
                for field in result["missing_fields"]:
                    lab.data_quality_flags.append(f"missing_{field}")

            if result["attempts"] > 1:
                lab.data_quality_flags.append("puppeteer_mcp_used")

        except Exception as e:
            # Graceful degradation
            logger.error("Failed to scrape lab website", url=lab.lab_url, error=str(e))
            lab.data_quality_flags.append("scraping_failed")
            # Continue processing with partial data

        return lab
```

**New Data Quality Flags:**

Add the following flags to `src/models/lab.py` constants:

```python
LAB_DATA_QUALITY_FLAGS = [
    # ... existing flags from Story 4.1-4.4
    "no_website",              # From Story 4.1
    "scraping_failed",         # From Story 4.1
    "robots_txt_blocked",      # NEW for Story 4.5
    "insufficient_webfetch",   # NEW for Story 4.5
    "puppeteer_mcp_used",      # NEW for Story 4.5
    "sufficiency_evaluation_failed",  # NEW for Story 4.5
]
```

**Architecture Component Diagram Flow:**

```
Lab Research Agent
  ↓
robots.txt Checker (check_robots_txt)
  ↓ (if allowed)
Rate Limiter (DomainRateLimiter.acquire)
  ↓
Multi-Stage Scraping (scrape_with_sufficiency)
  ↓
Stage 1: WebFetch (ClaudeSDKClient + WebFetch tool)
  ↓
Stage 2: Sufficiency Evaluation (evaluate_sufficiency)
  ↓ (if sufficient → return)
  ↓ (if insufficient → continue)
Stage 3: Puppeteer MCP (ClaudeSDKClient + mcp__puppeteer__navigate/evaluate)
  ↓
Stage 4: Re-evaluate Sufficiency
  ↓ (if sufficient → return)
  ↓ (if insufficient → retry up to 3 attempts)
  ↓ (max attempts → return partial data + flags)
Lab Model Update (with data quality flags)
  ↓
Checkpoint Manager (save with error flags)
```

**Critical Rules (from Coding Standards):**
- Web scraping uses multi-stage pattern (NFR5)
- WebFetch → Sufficiency → Puppeteer MCP → Re-evaluate (max 3 attempts)
- Sufficiency evaluation uses strong prompts (thinking parameter not supported)
- Respect robots.txt for ethical scraping (NFR10)
- Implement per-domain rate limiting (NFR11)
- Never crash on scraping failures (NFR13)
- Never use print() for logging (use structlog)
- MCP servers configured via `claude/.mcp.json`, NOT Python code
- Always use `cwd` parameter pointing to `claude/` directory

**MCP Configuration Reference:**

The Puppeteer MCP server must be configured in `claude/.mcp.json` (created in Story 1.5):

```json
{
  "mcpServers": {
    "puppeteer": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-puppeteer"]
    }
  }
}
```

### Retry Logic and Type Guarantees

The multi-stage pattern includes comprehensive retry logic:

1. **Internal JSON parsing retry**: `_parse_json_from_text()` handles malformed JSON
2. **Sufficiency evaluation retry**: 3 attempts with fallback to insufficient
3. **Stage escalation**: WebFetch → Puppeteer MCP (3 total attempts)
4. **Function-level retry**: `@retry` decorator on calling functions (e.g., `scrape_lab_website()`)

**Type Guarantee Contract:**

`scrape_with_sufficiency()` ALWAYS returns `ScrapingResult` with `data: dict[str, Any]`:
- ✅ Success case: Returns parsed data dict
- ✅ Partial success: Returns dict with some missing fields + flags
- ✅ Failure case: Returns empty dict `{}` + failure flags

**Calling code should NOT add additional parsing fallbacks.** If you see code like:

```python
if isinstance(result["data"], str):
    parsed = json.loads(result["data"])
```

This is redundant—remove it. Trust the type guarantee and let true failures propagate cleanly for checkpoint recovery.

### Testing

**Test File Location:** `tests/unit/test_web_scraping.py`

**Testing Standards:**
- Framework: pytest 8.4.2 with pytest-asyncio
- Mock web requests, MCP tools, and failures
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Test multi-stage pattern progression (WebFetch → Puppeteer MCP)
2. Test sufficiency evaluation with various data completeness scenarios
3. Test retry logic with exponential backoff
4. Test rate limiting enforcement per domain
5. Test robots.txt compliance checking
6. Test timeout handling with asyncio.wait_for
7. Test graceful degradation (partial data with flags)
8. Test JSON parsing from 4 different formats
9. Test Puppeteer MCP escalation (mocked MCP tools)
10. Test data quality flag assignment
11. Integration test with mock HTTP server

**Example Test Patterns:**

```python
import pytest
from src.utils.web_scraping import (
    scrape_with_sufficiency,
    evaluate_sufficiency,
    check_robots_txt,
    DomainRateLimiter,
)


@pytest.mark.asyncio
async def test_multi_stage_webfetch_sufficient(mocker):
    """Test Stage 1 (WebFetch) succeeds with sufficient data."""
    # Arrange
    mock_webfetch = mocker.patch('src.utils.web_scraping._scrape_with_webfetch')
    mock_webfetch.return_value = {
        "lab_information": "AI Research Lab",
        "contact": "contact@example.edu",
        "people": ["Dr. Smith", "Dr. Jones"],
        "research_focus": "Machine Learning",
        "publications": ["Paper 1", "Paper 2"]
    }

    mock_evaluate = mocker.patch('src.utils.web_scraping.evaluate_sufficiency')
    mock_evaluate.return_value = {"sufficient": True, "missing_fields": []}

    # Act
    result = await scrape_with_sufficiency(
        url="https://test.edu/lab",
        required_fields=["lab_information", "contact", "people", "research_focus", "publications"],
        max_attempts=3
    )

    # Assert
    assert result["sufficient"] is True
    assert result["attempts"] == 1
    assert len(result["missing_fields"]) == 0
    mock_webfetch.assert_called_once()
    mocker.patch('src.utils.web_scraping._scrape_with_puppeteer_mcp').assert_not_called()


@pytest.mark.asyncio
async def test_multi_stage_escalates_to_puppeteer(mocker):
    """Test escalation to Puppeteer MCP when WebFetch insufficient."""
    # Arrange
    mock_webfetch = mocker.patch('src.utils.web_scraping._scrape_with_webfetch')
    mock_webfetch.return_value = {"lab_information": "AI Lab"}  # Incomplete

    mock_puppeteer = mocker.patch('src.utils.web_scraping._scrape_with_puppeteer_mcp')
    mock_puppeteer.return_value = {
        "lab_information": "AI Lab",
        "contact": "contact@example.edu",
        "people": ["Dr. Smith"],
        "research_focus": "ML",
        "publications": ["Paper 1"]
    }

    mock_evaluate = mocker.patch('src.utils.web_scraping.evaluate_sufficiency')
    mock_evaluate.side_effect = [
        {"sufficient": False, "missing_fields": ["contact", "people", "research_focus", "publications"]},
        {"sufficient": True, "missing_fields": []}
    ]

    # Act
    result = await scrape_with_sufficiency(
        url="https://test.edu/lab",
        required_fields=["lab_information", "contact", "people", "research_focus", "publications"],
        max_attempts=3
    )

    # Assert
    assert result["sufficient"] is True
    assert result["attempts"] == 2  # WebFetch + Puppeteer
    mock_webfetch.assert_called_once()
    mock_puppeteer.assert_called_once()


@pytest.mark.asyncio
async def test_sufficiency_evaluation_json_formats(mocker):
    """Test sufficiency evaluation handles multiple JSON formats."""
    # Test markdown codeblock format
    mock_client = mocker.patch('src.utils.web_scraping.ClaudeSDKClient')
    mock_client.return_value.__aenter__.return_value.receive_response.return_value = iter([
        type('Message', (), {'content': [type('Block', (), {'text': '```json\n{"sufficient": true, "missing_fields": []}\n```'})]})()
    ])

    result = await evaluate_sufficiency({"field1": "value1"}, ["field1"], "test-corr-id")
    assert result["sufficient"] is True


@pytest.mark.asyncio
async def test_rate_limiter_per_domain(mocker):
    """Test rate limiting enforced per domain."""
    limiter = DomainRateLimiter(default_rate=2)

    # Should create separate limiters for different domains
    await limiter.acquire("https://domain1.edu/page1")
    await limiter.acquire("https://domain2.edu/page1")

    assert len(limiter.limiters) == 2
    assert "domain1.edu" in limiter.limiters
    assert "domain2.edu" in limiter.limiters


@pytest.mark.asyncio
async def test_robots_txt_disallows_scraping(mocker, httpx_mock):
    """Test robots.txt blocking."""
    httpx_mock.add_response(
        url="https://test.edu/robots.txt",
        text="User-agent: *\nDisallow: /private/"
    )

    allowed = await check_robots_txt("https://test.edu/private/lab")
    assert allowed is False


@pytest.mark.asyncio
async def test_graceful_degradation_on_failure(mocker):
    """Test pipeline continues with flags on scraping failure."""
    mock_scrape = mocker.patch('src.utils.web_scraping.scrape_with_sufficiency')
    mock_scrape.side_effect = Exception("Network error")

    lab = Lab(id="lab-1", lab_url="https://test.edu/lab")
    agent = LabResearchAgent()

    result = await agent.scrape_lab_website(lab, correlation_id="test-corr-id")

    # Assert graceful degradation
    assert "scraping_failed" in result.data_quality_flags
    assert result is not None  # Lab object returned, processing continues
```

**Integration Test Example:**

```python
@pytest.mark.integration
@pytest.mark.asyncio
async def test_end_to_end_multi_stage_scraping(httpx_mock):
    """Integration test: WebFetch → Sufficiency → Puppeteer MCP progression."""
    # Mock university lab page
    httpx_mock.add_response(
        url="https://test.edu/lab",
        html="<html><body><h1>AI Lab</h1></body></html>"
    )

    result = await scrape_with_sufficiency(
        url="https://test.edu/lab",
        required_fields=["lab_information", "contact"],
        max_attempts=3,
        correlation_id="integration-test"
    )

    # Verify result structure
    assert "data" in result
    assert "sufficient" in result
    assert "missing_fields" in result
    assert "attempts" in result
    assert result["attempts"] >= 1 and result["attempts"] <= 3
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-09 | 0.5 | **PUPPETEER MCP ARCHITECTURE REWRITE** - Complete rewrite for Sprint Change Proposal compliance: (1) Updated AC #2 to "Puppeteer MCP fallback"; (2) Added AC #8 for sufficiency evaluation; (3) Rewrote Tasks 1-2 for multi-stage pattern (WebFetch → Sufficiency → Puppeteer MCP); (4) Removed all direct Playwright library code; (5) Added Puppeteer MCP tool examples (`mcp__puppeteer__navigate`, `mcp__puppeteer__evaluate`); (6) Updated Dev Notes with `scrape_with_sufficiency()` and `evaluate_sufficiency()` implementations; (7) Added new data quality flags (`insufficient_webfetch`, `puppeteer_mcp_used`, `sufficiency_evaluation_failed`); (8) Updated architecture diagram for multi-stage flow; (9) Added MCP configuration reference; (10) Updated test requirements for Puppeteer MCP. Story now aligns with Phase 1 pre-flight experiments and Patterns 6-7 from implementation-patterns.md. | Sarah (PO) |
| 2025-10-09 | 0.4 | **TASK ORDERING FIX** - Reordered tasks to match dependency flow: Task 1 (Tiered Fallback) → Task 2 (Retry Logic) → Task 3 (Timeouts) → Tasks 4-7 (independent) → Task 8 (Integration). Added NOTE about sequential implementation. Enhanced subtask descriptions to clarify dependencies. Updated validation score: 9.6→10/10. Story APPROVED for implementation. [DEPRECATED - replaced by v0.5] | Sarah (PO) |
| 2025-10-09 | 0.3 | **VALIDATION FIXES** - Applied PO validation corrections. [DEPRECATED - replaced by v0.5] | Sarah (PO) |
| 2025-10-06 | 0.2 | Added missing Tasks 6-8. [DEPRECATED - replaced by v0.5] | Sarah (PO) |
| 2025-10-06 | 0.1 | Initial story creation. [DEPRECATED - replaced by v0.5] | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by agent_

### Debug Log References

_To be populated by agent_

### Completion Notes List

_To be populated by agent_

### File List

_To be populated by agent_

## QA Results

_To be populated by QA agent_
