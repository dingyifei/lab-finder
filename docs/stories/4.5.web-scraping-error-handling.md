# Story 4.5: Web Scraping Error Handling & Retry Logic

## Status

**Draft**

## Story

**As a** user,
**I want** robust error handling for web scraping failures,
**so that** temporary issues don't derail the entire analysis.

## Acceptance Criteria

1. Retry logic implemented for failed requests (NFR13)
2. Built-in tools tried first, then Playwright fallback (NFR5)
3. Rate limiting prevents being blocked (NFR10, NFR11)
4. robots.txt respected for ethical scraping
5. Timeout handling for slow-loading pages
6. All failures logged with details (NFR17)
7. Graceful degradation: partial data better than no data

## Tasks / Subtasks

- [ ] **Task 1: Implement Retry Logic** (AC: 1)
  - [ ] Use tenacity for exponential backoff
  - [ ] Max 3 retries, 1s initial delay
  - [ ] Retry on network errors, timeouts

- [ ] **Task 2: Implement Tiered Fallback** (AC: 2)
  - [ ] Try built-in tools first
  - [ ] On failure, try Playwright
  - [ ] On both failures, flag and continue

- [ ] **Task 3: Add Rate Limiting** (AC: 3)
  - [ ] Limit requests per domain: 1 req/second default
  - [ ] Configurable rate limits in system_params.json
  - [ ] Use asyncio.sleep() between requests

- [ ] **Task 4: Respect robots.txt** (AC: 4)
  - [ ] Check robots.txt before scraping
  - [ ] Use robotparser library
  - [ ] Skip if disallowed, flag accordingly

- [ ] **Task 5: Implement Timeouts** (AC: 5)
  - [ ] 30 second timeout per page
  - [ ] Handle timeout gracefully
  - [ ] Log timeout occurrences

## Dev Notes

### Relevant Architecture Information

**Component:** Lab Research Agent - Web Scraping Error Handler (Epic 4)

**Responsibility:** Robust web scraping with retry logic and tiered fallback (Epic 4: NFR5, NFR13)

**Key Interfaces:**
- `fetch_with_retry(url: str, timeout: int = 30) -> str` - Fetch with retry and fallback
- `check_robots_txt(url: str) -> bool` - Verify scraping allowed

**Dependencies:**
- Claude Agent SDK built-in web tools (primary)
- Playwright (fallback)
- tenacity for retry logic
- robotparser for robots.txt compliance

**Technology Stack:**
- tenacity 8.2.3 for exponential backoff
- Playwright 1.40.0 as fallback
- urllib.robotparser for robots.txt
- structlog for error logging
- aiolimiter 1.1.0 for rate limiting

**Source Tree Location:**
- Modify: `src/agents/lab_research.py` (integrate error handling)
- Create: `src/utils/web_scraper.py` (reusable web scraping helpers)

**Web Scraping Tiered Fallback Strategy (from Architecture):**
```
1. Try Claude Agent SDK built-in web tools (fast, simple)
   ↓ (on failure)
2. Try Playwright (handles JS, authentication)
   ↓ (on failure)
3. Flag with data quality issue, continue with partial data
```

**Retry Logic with Tenacity:**
```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),
    reraise=True
)
async def fetch_with_retry(url: str, timeout: int = 30) -> str:
    """Fetch URL with retry logic and tiered fallback."""
    logger.debug(f"Fetching {url} with built-in tools")

    # Try built-in tools first (NFR5)
    try:
        content = await built_in_fetch(url, timeout=timeout)
        logger.info(f"Built-in tools succeeded for {url}")
        return content
    except Exception as e:
        logger.warning(f"Built-in tools failed for {url}: {e}")

        # Fallback to Playwright
        try:
            logger.debug(f"Falling back to Playwright for {url}")
            content = await playwright_fetch(url, timeout=timeout)
            logger.info(f"Playwright succeeded for {url}")
            return content
        except Exception as e2:
            logger.error(f"Both methods failed for {url}: {e2}")
            raise  # Will be retried by tenacity
```

**Rate Limiting Implementation:**
```python
from aiolimiter import AsyncLimiter
from urllib.parse import urlparse

class DomainRateLimiter:
    """Per-domain rate limiting to prevent blocking."""

    def __init__(self):
        self.limiters = {}  # domain -> AsyncLimiter
        self.default_rate = 1  # 1 request per second default

    async def acquire(self, url: str):
        """Acquire rate limit token for URL's domain."""
        domain = urlparse(url).netloc

        if domain not in self.limiters:
            # Create limiter for new domain
            self.limiters[domain] = AsyncLimiter(
                max_rate=self.default_rate,
                time_period=1.0  # per second
            )

        await self.limiters[domain].acquire()

# Usage
rate_limiter = DomainRateLimiter()

async def fetch_url(url: str):
    await rate_limiter.acquire(url)  # Respect rate limit
    return await fetch_with_retry(url)
```

**robots.txt Compliance:**
```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin

async def check_robots_txt(url: str, user_agent: str = "LabFinder/1.0") -> bool:
    """Check if URL is allowed by robots.txt."""
    try:
        parser = RobotFileParser()
        robots_url = urljoin(url, "/robots.txt")

        # Fetch robots.txt
        robots_content = await fetch_with_timeout(robots_url, timeout=5)
        parser.parse(robots_content.splitlines())

        # Check if URL is allowed
        allowed = parser.can_fetch(user_agent, url)

        if not allowed:
            logger.warning(f"robots.txt disallows scraping {url}")

        return allowed

    except Exception as e:
        logger.debug(f"Could not check robots.txt for {url}: {e}")
        # If robots.txt unavailable, assume allowed
        return True
```

**Timeout Handling:**
```python
import asyncio

async def fetch_with_timeout(url: str, timeout: int = 30) -> str:
    """Fetch URL with timeout."""
    try:
        return await asyncio.wait_for(
            fetch_with_retry(url),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        logger.warning(f"Timeout after {timeout}s fetching {url}")
        raise
```

**Graceful Degradation Pattern:**
```python
async def scrape_lab_website_safe(lab: Lab) -> Lab:
    """Scrape lab website with comprehensive error handling."""
    if not lab.lab_url:
        lab.data_quality_flags.append("no_website")
        return lab

    # Check robots.txt
    if not await check_robots_txt(lab.lab_url):
        lab.data_quality_flags.append("robots_txt_blocked")
        return lab

    try:
        # Rate limit
        await rate_limiter.acquire(lab.lab_url)

        # Fetch with retry and fallback
        content = await fetch_with_timeout(lab.lab_url, timeout=30)

        # Parse content
        lab.website_content = content
        lab.description = extract_description(content)
        lab.research_focus = extract_research_focus(content)

    except Exception as e:
        logger.error(f"Failed to scrape {lab.lab_url}: {e}")
        lab.data_quality_flags.append("scraping_failed")
        # Continue processing with partial data

    return lab
```

**Critical Rules (from Coding Standards):**
- Web scraping must try built-in tools first (NFR5)
- Respect robots.txt for ethical scraping (NFR10)
- Implement per-domain rate limiting (NFR11)
- Never crash on scraping failures (NFR13)
- Never use print() for logging (use structlog)

**Architecture Component Diagram Flow:**
```
Lab Research Agent → robots.txt Checker
  ↓ (if allowed)
Rate Limiter (per domain)
  ↓
Built-in Web Tools (try first)
  ↓ (on failure)
Playwright Fallback
  ↓ (on failure)
Retry Logic (tenacity, 3 attempts)
  ↓ (on final failure)
Graceful Degradation (flag + continue)
  ↓
Lab Model Update (with flags)
  ↓
Checkpoint Manager (save with error flags)
```

### Testing

**Test File Location:** `tests/unit/test_web_scraping_errors.py`

**Testing Standards:**
- Framework: pytest 7.4.4 with pytest-asyncio
- Mock web requests and failures
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Test retry logic with exponential backoff
2. Test tiered fallback (built-in → Playwright → fail)
3. Test rate limiting enforcement per domain
4. Test robots.txt compliance checking
5. Test timeout handling
6. Test graceful degradation (partial data)
7. Test error logging for all failure types
8. Integration test with mock HTTP server

**Example Test Pattern:**
```python
@pytest.mark.asyncio
async def test_fetch_with_retry_fallback(mocker):
    # Arrange
    mock_builtin = mocker.patch('src.utils.web_scraper.built_in_fetch')
    mock_builtin.side_effect = ConnectionError("Built-in failed")

    mock_playwright = mocker.patch('src.utils.web_scraper.playwright_fetch')
    mock_playwright.return_value = "<html>Success via Playwright</html>"

    # Act
    result = await fetch_with_retry("https://test.edu")

    # Assert
    assert "Success via Playwright" in result
    assert mock_builtin.call_count == 3  # Retried 3 times
    assert mock_playwright.call_count == 3  # Fallback tried each time

@pytest.mark.asyncio
async def test_graceful_degradation_on_failure(mocker):
    # Arrange
    mocker.patch('src.utils.web_scraper.fetch_with_timeout',
                 side_effect=Exception("All methods failed"))

    lab = Lab(id="lab-1", lab_url="https://test.edu")

    # Act
    result = await scrape_lab_website_safe(lab)

    # Assert
    assert "scraping_failed" in result.data_quality_flags
    assert result.website_content == ""  # Empty but not None
    # Lab object returned, processing continues
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |
