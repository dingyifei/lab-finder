# Story 3.1: Parallel Professor Discovery

## ⚠️ **DEPRECATED - SPLIT INTO SUB-STORIES**

**This story has been split into three focused sub-stories for better token budget management:**
- **Story 3.1a:** Professor Model + Basic Discovery (Foundation)
- **Story 3.1b:** Parallel Processing + Batch Coordination
- **Story 3.1c:** Deduplication + Rate Limiting + Checkpointing

**Please implement the sub-stories (3.1a → 3.1b → 3.1c) instead of this combined story.**

---

## Status

**Superseded** (Split into Stories 3.1a, 3.1b, 3.1c on 2025-10-07)

## Story

**As a** user,
**I want** professors identified across all relevant departments using parallel async processing,
**so that** discovery is efficient even for large universities.

## Acceptance Criteria

1. Departments loaded from Epic 2 checkpoint output (see `docs/prd/epic-3-professor-discovery.md#fr10`)
2. Professor directory pages discovered and scraped using Claude Code WebFetch/WebSearch tools
3. Professor names, titles, department affiliations extracted
4. Lab affiliations identified where available
5. Research area descriptions extracted from directory listings
6. WebFetch/WebSearch tools used with explicit Playwright fallback when needed (see `docs/prd/non-functional-requirements.md#nfr5`)
7. Results aggregated from parallel async tasks into master professor list
8. Parallel processing uses Python asyncio.gather() with configurable concurrency limits

## Tasks / Subtasks

- [ ] **Task 1: Load Relevant Departments from Epic 2** (AC: 1)
  - [ ] Use checkpoint_manager to load `checkpoints/phase-1-relevant-departments.jsonl`
  - [ ] Deserialize to Department Pydantic models
  - [ ] Verify all departments have required fields (name, url)
  - [ ] Log: "Loaded X relevant departments for professor discovery"

- [ ] **Task 2: Create Professor Pydantic Model** (AC: 3, 4, 5)
  - [ ] Create `src/models/professor.py` module
  - [ ] Define Professor model:
    ```python
    class Professor(BaseModel):
        id: str  # Unique identifier (generated)
        name: str  # Full name
        title: str  # Academic title (Professor, Associate Professor, etc.)
        department_id: str  # Foreign key to Department
        department_name: str  # Department name for display
        school: Optional[str] = None  # School/college
        lab_name: Optional[str] = None  # Lab affiliation if available
        lab_url: Optional[str] = None  # Lab website URL
        research_areas: list[str] = []  # Research area descriptions
        profile_url: str  # Faculty profile URL
        email: Optional[str] = None  # Contact email if available
        data_quality_flags: list[str] = []  # Quality issues (e.g., "missing_email", "multiple_departments", "ambiguous_lab")
    ```
  - [ ] Add type hints and validation
  - [ ] Run mypy to verify type correctness

- [ ] **Task 3: Setup Progress Tracking (Cross-Cutting)** (AC: 8)
  - [ ] Use progress_tracker from `docs/stories/1.7.user-profile-consolidation.md`
  - [ ] Initialize at start: `tracker.start_phase("Phase 2: Professor Discovery", total_items=len(departments))`
  - [ ] Update after each department: `tracker.update(completed=i+1)`
  - [ ] Display batch progress: "Batch M: Processing departments A-B"
  - [ ] Final summary: "Phase 2 complete: X professors discovered"

- [ ] **Task 4: Implement Professor Discovery with Web Scraping** (AC: 2, 3, 4, 5, 6)
  - [ ] Create `src/agents/professor_filter.py` module (Note: combined discovery+filtering agent, filtering in Story 3.2)
  - [ ] Implement `discover_professors_for_department(department: Department, correlation_id: str) -> list[Professor]`
  - [ ] **Subtask 4.1: Core Discovery Logic**
    - [ ] Use Claude Agent SDK `query()` function to scrape professor directory
    - [ ] Construct prompt: "Scrape {dept.url} and extract professor information"
    - [ ] Configure ClaudeAgentOptions with `allowed_tools=["WebFetch", "WebSearch"]`
    - [ ] Parse response messages to extract professor data
  - [ ] **Subtask 4.2: Handle Multiple Directory Page Formats**
    - [ ] In prompt, instruct Claude to try multiple selector patterns:
      - Common selectors: `.faculty-member`, `.professor-card`, `table.faculty`
      - Semantic HTML: `<article>`, `<section class="people">`
      - Link patterns: `<a href="*/faculty/*">`, `<a href="*/people/*">`
    - [ ] If structured directory not found, search for "faculty" or "people" links
    - [ ] Log which pattern succeeded
    - [ ] Flag departments where discovery failed with data_quality_flag
  - [ ] **Subtask 4.3: Extract Research Areas**
    - [ ] In prompt, instruct to look for research area indicators:
      - Explicit "Research Areas:" sections
      - Keywords in bio/description text
      - Subject tags or labels
    - [ ] Parse research areas into list
    - [ ] Handle missing research areas (empty list, not failure)
  - [ ] **Subtask 4.4: Implement Playwright Fallback**
    - [ ] Wrap Claude SDK query() in try-except
    - [ ] On failure or insufficient results, use Playwright:
      ```python
      async with async_playwright() as p:
          browser = await p.chromium.launch()
          page = await browser.new_page()
          await page.goto(department.url)
          content = await page.content()
          await browser.close()
      ```
    - [ ] Parse Playwright HTML content with BeautifulSoup
    - [ ] Flag with data_quality_flag: "scraped_with_playwright_fallback"
  - [ ] **Subtask 4.5: Implement Rate Limiting**
    - [ ] Use aiolimiter.AsyncLimiter for request throttling
    - [ ] Load rate limits from system_params (see `src/models/config.py`)
    - [ ] Apply limiter to both Claude SDK queries and Playwright requests
    - [ ] Example: `async with rate_limiter: await scrape_function()`
  - [ ] Add retry logic with tenacity (3 retries, exponential backoff)

- [ ] **Task 5: Implement Parallel Processing with asyncio.gather()** (AC: 7, 8)
  - [ ] Implement `discover_professors_parallel(departments: list[Department], max_concurrent: int) -> list[Professor]`
  - [ ] Create asyncio.Semaphore for concurrency control:
    ```python
    semaphore = asyncio.Semaphore(max_concurrent)

    async def process_with_semaphore(dept: Department) -> list[Professor]:
        async with semaphore:
            correlation_id = f"prof-disc-{uuid.uuid4()}"
            logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
            return await discover_professors_for_department(dept, correlation_id)
    ```
  - [ ] Execute parallel tasks: `results = await asyncio.gather(*tasks, return_exceptions=True)`
  - [ ] Handle exceptions gracefully: log errors, continue with partial results
  - [ ] Get max_concurrent from `system_params.json::batch_sizes.departments`
  - [ ] Log: "Processing {len(departments)} departments with max_concurrent={max_concurrent}"

- [ ] **Task 6: Aggregate and Deduplicate Professor Results** (AC: 7)
  - [ ] Flatten results from all parallel tasks
  - [ ] Deduplicate by professor name + department combination
  - [ ] Use `llm_helpers.match_names(name1: str, name2: str) -> dict` for fuzzy matching (returns dict with 'decision', 'confidence', 'reasoning')
  - [ ] Threshold: confidence score 90+ considered duplicate
  - [ ] If duplicates found: Merge information (prefer more complete records)
  - [ ] Generate unique ID for each professor: `hashlib.sha256(f"{name}:{department_id}".encode()).hexdigest()[:16]`

- [ ] **Task 7: Save Professor List to Checkpoint** (AC: 7)
  - [ ] Use checkpoint_manager.save_batch()
  - [ ] Save to `checkpoints/phase-2-professors-batch-{N}.jsonl`
  - [ ] Use JSONL format for streaming support
  - [ ] Serialize Professor models with `.model_dump()`
  - [ ] Log: "Discovered X professors across Y departments"

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Discovery & Filter Agent (see `docs/prd/epic-3-professor-discovery.md`)

**Responsibility:** Discover professors across filtered departments; extract profile and research data (see `docs/prd/epic-3-professor-discovery.md#fr10`). Filtering logic implemented in Story 3.2.

**Epic 2 Dependencies - Department Model Context:**
This story consumes the output of Epic 2 (University Discovery). The Department model from `docs/stories/2.4.error-handling-structure.md` includes:
- `id: str` - Unique department identifier
- `name: str` - Department name
- `url: str` - Department website URL
- `school: Optional[str]` - Parent school/college name
- `data_quality_flags: list[str]` - Quality tracking flags from Epic 2 processing

Checkpoint location: `checkpoints/phase-1-relevant-departments.jsonl`

**Key Interfaces:**
- `discover_professors_for_department(department: Department, correlation_id: str) -> list[Professor]` - Scrape single department's professor directory
- `discover_professors_parallel(departments: list[Department], max_concurrent: int) -> list[Professor]` - Parallel processing coordinator

**Dependencies:**
- Department data from Epic 2 checkpoints (see `docs/prd/epic-2-university-discovery.md`)
- Claude Agent SDK (query() function with WebFetch/WebSearch tools)
- Playwright (fallback for JS-heavy pages)
- Checkpoint Manager for saving professor data (see `src/utils/checkpoint_manager.py`)
- Progress Tracker for status updates (see `src/utils/progress_tracker.py`)
- LLM Helpers for name matching (see `src/utils/llm_helpers.py::match_names()`)
- aiolimiter for rate limiting web scraping requests
- SystemParams model for configuration (see `src/models/config.py`)

**Technology Stack:**
- Claude Agent SDK 0.1.1 - `query()` function
- Playwright 1.55.0 as fallback
- Python asyncio for parallel execution
- structlog with correlation IDs (application-managed)
- Pydantic Professor model

**Parallel Execution Pattern:** Application-level asyncio.gather() with Semaphore for concurrency control (NOT Claude Agent SDK feature)

**⚠️ Note on PRD Alignment:**
The PRD (`docs/prd/epic-3-professor-discovery.md`, Story 3.1 AC#1) states "Sub-agents created for each department." This story implements parallel processing using Python's `asyncio.gather()` instead. This deviation reflects an architectural decision (documented in Change Log v0.3) to use accurate Claude Agent SDK patterns rather than non-existent SDK sub-agent features. The parallel execution functionality is preserved; only the implementation mechanism differs from the original PRD language.

**System Configuration:**
The story references `system_params.json::batch_sizes.departments` for concurrency limits. See `config/system_params.example.json` for the template configuration. Example structure:
```json
{
  "batch_sizes": {
    "departments": 5,
    "professors": 20,
    "labs": 10
  }
}
```

**Source Tree Location:**
- Create: `src/agents/professor_filter.py` (combined discovery+filtering agent)
- Create: `src/models/professor.py`
- Update: `src/utils/llm_helpers.py` (add match_names function if not exists)
- Load from: `checkpoints/phase-1-relevant-departments.jsonl`
- Save to: `checkpoints/phase-2-professors-batch-{N}.jsonl`

**File Naming Note:**
The file is named `professor_filter.py` for the combined discovery+filtering agent:
- **Story 3.1:** Implements discovery functions (`discover_professors_for_department`, `discover_professors_parallel`)
- **Story 3.2:** Adds filtering functions to the same file (see `docs/stories/3.2.professor-filtering.md`)
This naming convention reflects the agent's dual responsibility across both stories.

**Professor Model Schema:**
```python
from pydantic import BaseModel
from typing import Optional

class Professor(BaseModel):
    id: str  # Unique identifier (SHA256 hash of name:department_id)
    name: str  # Full name
    title: str  # Academic title
    department_id: str  # Links to Department.id
    department_name: str  # For display
    school: Optional[str] = None  # Parent school
    lab_name: Optional[str] = None  # Lab affiliation
    lab_url: Optional[str] = None  # Lab website
    research_areas: list[str] = []  # Research keywords
    profile_url: str  # Faculty profile URL
    email: Optional[str] = None  # Contact email
    data_quality_flags: list[str] = []  # Quality issues (see Data Quality Flags below)
```

**Data Quality Flags Used in Story 3.1:**
- `scraped_with_playwright_fallback` - WebFetch failed, used Playwright as fallback
- `missing_email` - Professor record has no email address
- `missing_research_areas` - No research areas found in directory listing
- `missing_lab_affiliation` - Lab name/URL not identified
- `ambiguous_lab` - Lab affiliation unclear (multiple possible labs)
- `multiple_departments` - Professor listed in multiple departments (possible duplicate)

---

### Claude Agent SDK Implementation Patterns

**Pattern 1: Using query() with WebFetch**

```python
from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock
from src.utils.logger import get_logger
import json
import hashlib

async def discover_professors_for_department(
    department: Department,
    correlation_id: str
) -> list[Professor]:
    """Discover professors for a single department using Claude Agent SDK."""
    logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
    logger.info("Starting professor discovery", department=department.name, url=department.url)

    # Configure Claude Agent SDK with built-in web scraping tools
    # WebFetch and WebSearch are Claude Code built-in tools (verified in official docs)
    options = ClaudeAgentOptions(
        allowed_tools=["WebFetch", "WebSearch"],  # Built-in Claude Code tools
        max_turns=3,
        system_prompt="You are a web scraping assistant specialized in extracting professor information from university department pages."
    )

    prompt = f"""
    Scrape the professor directory at: {department.url}

    Extract ALL professors with the following information:
    - Full name
    - Title (Professor, Associate Professor, Assistant Professor, etc.)
    - Research areas/interests
    - Lab name and URL (if available)
    - Email address
    - Profile page URL

    Return results as a JSON array of professor objects.
    """

    professors = []
    try:
        async for message in query(prompt=prompt, options=options):
            if isinstance(message, AssistantMessage):
                for block in message.content:
                    if isinstance(block, TextBlock):
                        # Parse JSON response from Claude
                        professors_data = parse_professor_data(block.text)
                        professors.extend(professors_data)

        # Convert to Professor Pydantic models
        professor_models = [
            Professor(
                id=generate_professor_id(p["name"], department.id),
                name=p["name"],
                title=p.get("title", "Unknown"),
                department_id=department.id,
                department_name=department.name,
                school=department.school,
                lab_name=p.get("lab_name"),
                lab_url=p.get("lab_url"),
                research_areas=p.get("research_areas", []),
                profile_url=p.get("profile_url", ""),
                email=p.get("email"),
                data_quality_flags=[]
            )
            for p in professors_data
        ]

        logger.info("Discovery successful", professors_count=len(professor_models))
        return professor_models

    except Exception as e:
        logger.warning("WebFetch failed, falling back to Playwright", error=str(e))
        return await discover_with_playwright_fallback(department, correlation_id)


def parse_professor_data(text: str) -> list[dict]:
    """Parse professor data from Claude's response text."""
    # Try to extract JSON array
    try:
        # Look for JSON array in response
        import re
        json_match = re.search(r'\[.*\]', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
        return []
    except json.JSONDecodeError:
        return []


def generate_professor_id(name: str, department_id: str) -> str:
    """Generate unique professor ID from name and department."""
    content = f"{name}:{department_id}"
    return hashlib.sha256(content.encode()).hexdigest()[:16]
```

**Pattern 2: Playwright Fallback Implementation**

```python
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

async def discover_with_playwright_fallback(
    department: Department,
    correlation_id: str
) -> list[Professor]:
    """Fallback to Playwright when Claude SDK WebFetch fails."""
    logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
    logger.info("Using Playwright fallback", department=department.name)

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # Navigate to department page
            await page.goto(department.url, wait_until="networkidle")

            # Wait for content to load
            await page.wait_for_timeout(2000)

            # Get page content
            content = await page.content()
            await browser.close()

            # Parse with BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')

            # Try multiple selector patterns
            professors_data = []
            for selector in SELECTOR_PATTERNS:
                elements = soup.select(selector)
                if elements:
                    logger.debug(f"Found {len(elements)} professors with selector: {selector}")
                    professors_data = parse_professor_elements(elements, department)
                    break

            # Convert to Professor models with playwright flag
            professor_models = []
            for p_data in professors_data:
                prof = Professor(**p_data)
                prof.data_quality_flags.append("scraped_with_playwright_fallback")
                professor_models.append(prof)

            return professor_models

    except Exception as e:
        logger.error("Playwright fallback also failed", error=str(e), department=department.name)
        # Return empty list, flag department in checkpoint
        return []


# Selector patterns to try
SELECTOR_PATTERNS = [
    '.faculty-member', '.professor-card', '.people-item',
    'article.faculty', 'section.people', 'div.directory-entry',
    'table.faculty tr', 'table.directory tr',
    'a[href*="/faculty/"]', 'a[href*="/people/"]'
]


def parse_professor_elements(elements, department: Department) -> list[dict]:
    """Parse professor data from HTML elements."""
    professors = []
    for elem in elements:
        # Extract name, title, email, etc. from element
        name = elem.select_one('h3, h4, .name')
        title_elem = elem.select_one('.title, .position')

        if name:
            prof_data = {
                "id": "",  # Will be generated
                "name": name.get_text(strip=True),
                "title": title_elem.get_text(strip=True) if title_elem else "Unknown",
                "department_id": department.id,
                "department_name": department.name,
                "school": department.school,
                "research_areas": [],
                "profile_url": "",
                "data_quality_flags": []
            }
            professors.append(prof_data)

    return professors
```

**Pattern 3: Parallel Processing with asyncio.gather()**

```python
import asyncio
from typing import List
import uuid

async def discover_professors_parallel(
    departments: list[Department],
    max_concurrent: int = 5
) -> list[Professor]:
    """
    Discover professors across multiple departments in parallel using asyncio.

    This is APPLICATION-LEVEL parallel execution, NOT a Claude Agent SDK feature.
    Uses asyncio.Semaphore to control concurrency.
    """
    logger = get_logger(correlation_id="prof-discovery-coordinator", phase="professor_discovery")
    logger.info(f"Starting parallel discovery", departments_count=len(departments), max_concurrent=max_concurrent)

    # Create semaphore for concurrency control
    semaphore = asyncio.Semaphore(max_concurrent)

    async def process_with_semaphore(dept: Department, index: int) -> list[Professor]:
        """Process single department with semaphore control."""
        async with semaphore:
            # Generate unique correlation ID for this department's processing
            correlation_id = f"prof-disc-{dept.id}-{uuid.uuid4().hex[:8]}"

            logger.info(f"Processing department {index+1}/{len(departments)}",
                       department=dept.name,
                       correlation_id=correlation_id)

            try:
                professors = await discover_professors_for_department(dept, correlation_id)
                return professors
            except Exception as e:
                logger.error("Department processing failed",
                           department=dept.name,
                           error=str(e),
                           correlation_id=correlation_id)
                return []  # Return empty list, continue with others

    # Create tasks for all departments
    tasks = [process_with_semaphore(dept, i) for i, dept in enumerate(departments)]

    # Execute in parallel with asyncio.gather
    # return_exceptions=True ensures one failure doesn't stop others
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Flatten results
    all_professors = []
    failed_count = 0
    for result in results:
        if isinstance(result, Exception):
            failed_count += 1
            logger.error("Task failed with exception", error=str(result))
            continue
        all_professors.extend(result)

    logger.info("Parallel discovery complete",
               total_professors=len(all_professors),
               failed_departments=failed_count)

    return all_professors
```

**Pattern 4: Deduplication with LLM Helper**

```python
from src.utils.llm_helpers import match_names

async def deduplicate_professors(professors: list[Professor]) -> list[Professor]:
    """
    Deduplicate professors using fuzzy name matching.
    Uses llm_helpers.match_names() for LLM-based name matching (returns dict with decision, confidence, reasoning).
    """
    logger = get_logger(correlation_id="deduplication", phase="professor_discovery")

    unique_professors = []
    seen_combinations = set()

    for prof in professors:
        # Exact match check first (fast)
        key = f"{prof.name.lower()}:{prof.department_id}"
        if key in seen_combinations:
            logger.debug("Exact duplicate found", professor=prof.name)
            continue

        # Fuzzy match check (slower, uses LLM)
        is_duplicate = False
        for existing in unique_professors:
            if existing.department_id == prof.department_id:
                # Use LLM helper for name similarity
                match_result = await match_names(existing.name, prof.name)

                if match_result["confidence"] >= 90:  # 90+ confidence threshold
                    logger.debug("Fuzzy duplicate found",
                               name1=existing.name,
                               name2=prof.name,
                               confidence=match_result["confidence"],
                               reasoning=match_result["reasoning"])

                    # Merge data (prefer more complete record)
                    merged = merge_professor_records(existing, prof)
                    unique_professors.remove(existing)
                    unique_professors.append(merged)
                    is_duplicate = True
                    break

        if not is_duplicate:
            seen_combinations.add(key)
            unique_professors.append(prof)

    logger.info("Deduplication complete",
               original_count=len(professors),
               unique_count=len(unique_professors),
               duplicates_removed=len(professors) - len(unique_professors))

    return unique_professors


def merge_professor_records(existing: Professor, new: Professor) -> Professor:
    """Merge two professor records, preferring more complete data."""
    merged_data = existing.model_dump()
    new_data = new.model_dump()

    # Prefer non-empty values
    for key in new_data:
        if key == "data_quality_flags":
            # Merge flags
            merged_data[key] = list(set(merged_data[key] + new_data[key]))
        elif not merged_data.get(key) and new_data.get(key):
            merged_data[key] = new_data[key]

    return Professor(**merged_data)
```

**Pattern 5: Correlation ID Management (Application-Level)**

```python
import uuid
from src.utils.logger import get_logger

# Correlation IDs are APPLICATION-MANAGED, not provided by Claude Agent SDK
# Generate at task start, pass to all functions, bind to logger

def start_professor_discovery_phase():
    """Entry point for professor discovery phase."""
    # Generate correlation ID for entire phase
    phase_correlation_id = f"phase-2-{uuid.uuid4()}"
    logger = get_logger(correlation_id=phase_correlation_id, phase="professor_discovery")

    logger.info("Starting Professor Discovery Phase")

    # Load departments
    departments = load_departments_from_checkpoint()

    # Run parallel discovery (each department gets sub-correlation-id)
    professors = await discover_professors_parallel(departments, max_concurrent=5)

    # Deduplicate
    unique_professors = await deduplicate_professors(professors)

    # Save checkpoint
    save_professors_to_checkpoint(unique_professors, phase_correlation_id)

    logger.info("Professor Discovery Phase complete", professors_count=len(unique_professors))
```

---

### Web Scraping Fallback Strategy

**Important:** Claude Code WebFetch and WebSearch tools do **NOT automatically fall back to Playwright**. This is an application-level implementation decision.

**Implementation Flow:**
1. **Try Claude SDK WebFetch first** (fast, simple, works for static pages)
2. **Check success:** If Claude returns valid professor data, continue
3. **If WebFetch fails or returns insufficient data:** Explicitly invoke Playwright
4. **Parse Playwright HTML** with BeautifulSoup and selector patterns
5. **Flag in data_quality_flags:** "scraped_with_playwright_fallback"

See "Pattern 2: Playwright Fallback Implementation" above for complete code.

---

### Rate Limiting Implementation

**Important:** Web scraping should respect rate limits to avoid server blocks and maintain ethical scraping practices.

**Implementation:**

```python
from aiolimiter import AsyncLimiter
from src.models.config import SystemParams

# Load system parameters
system_params = SystemParams.model_validate(load_system_params_json())

# Create rate limiter (requests per minute)
# Note: Rate limits apply per-service, not global
# For professor discovery, we're scraping university websites (not archive.org, linkedin, etc.)
# Use a general web scraping rate limit
rate_limit = system_params.timeouts.web_scraping  # Could add web_scraping to rate_limits
web_scraper_limiter = AsyncLimiter(max_rate=10, time_period=60)  # 10 requests per minute


async def discover_professors_with_rate_limit(
    department: Department,
    correlation_id: str
) -> list[Professor]:
    """Discover professors with rate limiting applied."""
    async with web_scraper_limiter:
        return await discover_professors_for_department(department, correlation_id)


# Apply in parallel processing
async def discover_professors_parallel(
    departments: list[Department],
    max_concurrent: int = 5
) -> list[Professor]:
    """Parallel discovery with rate limiting."""
    # Semaphore controls concurrency
    semaphore = asyncio.Semaphore(max_concurrent)

    # Rate limiter controls request rate
    rate_limiter = AsyncLimiter(max_rate=10, time_period=60)

    async def process_with_limits(dept: Department, index: int) -> list[Professor]:
        async with semaphore:  # Limit concurrent tasks
            async with rate_limiter:  # Limit request rate
                correlation_id = f"prof-disc-{dept.id}-{uuid.uuid4().hex[:8]}"
                return await discover_professors_for_department(dept, correlation_id)

    tasks = [process_with_limits(dept, i) for i, dept in enumerate(departments)]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Process results...
    return flatten_results(results)
```

**Rate Limiting Strategy:**
- **Semaphore**: Controls how many concurrent requests are active (e.g., 5 at once)
- **AsyncLimiter**: Controls the rate of requests over time (e.g., 10 per minute)
- **Combined**: Ensures both concurrency limits AND rate limits are respected

---

### LLM Helper Function for Name Matching

**Function Status:** ✅ **ALREADY IMPLEMENTED** in `src/utils/llm_helpers.py::match_names()`

The function is already implemented (Epic 1 Story 1.4) and returns a dict:

```python
# Implemented in src/utils/llm_helpers.py (lines 337-373)

async def match_names(
    name1: str,
    name2: str,
    context: str = "",
    correlation_id: Optional[str] = None
) -> dict[str, Any]:
    """
    Determine if two names refer to the same person.
    Handles name variants (Dr. Jane Smith vs Jane A. Smith vs J. Smith).

    Returns: Dict with 'decision' (Yes/No), 'confidence' (0-100), and 'reasoning' (str)
    """
    prompt = NAME_MATCH_TEMPLATE.format(
        name1=name1, name2=name2, context=context or "No additional context"
    )

    response = await call_llm_with_retry(prompt, correlation_id=correlation_id)

    # Parse response
    lines = response.strip().split("\n")
    result = {"decision": "No", "confidence": 0, "reasoning": ""}

    for line in lines:
        if line.startswith("Decision:"):
            result["decision"] = line.split(":", 1)[1].strip()
        elif line.startswith("Confidence:"):
            try:
                result["confidence"] = int(line.split(":", 1)[1].strip())
            except ValueError:
                result["confidence"] = 0
        elif line.startswith("Reasoning:"):
            result["reasoning"] = line.split(":", 1)[1].strip()

    return result
```

**Usage in Task 6 Deduplication:**
```python
match_result = await match_names(existing.name, prof.name)
if match_result["confidence"] >= 90:  # 90+ confidence threshold
    # Names match with high confidence
    reasoning = match_result["reasoning"]
```

---

### Error Handling Pattern

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    reraise=True
)
async def discover_professors_with_retry(department: Department, correlation_id: str) -> list[Professor]:
    """Wrapper with retry logic."""
    return await discover_professors_for_department(department, correlation_id)
```

**Failure Modes:**
- **WebFetch timeout:** Fallback to Playwright
- **Playwright crash:** Return empty list, flag department in checkpoint
- **Department URL invalid:** Log error, return empty list
- **Parser fails:** Return partial data, flag with data_quality issue

### Testing

**Test File Location:** `tests/integration/test_professor_discovery.py`

**Testing Standards:**
- Framework: pytest 8.4.2
- Async tests: pytest-asyncio
- Coverage requirement: 70% minimum

**Test Requirements:**

1. **Unit test for Professor model validation**
   ```python
   def test_professor_model_validation():
       prof = Professor(
           id="test-001",
           name="Dr. Jane Smith",
           title="Professor",
           department_id="dept-001",
           department_name="Computer Science",
           profile_url="https://cs.edu/faculty/jsmith"
       )
       assert prof.name == "Dr. Jane Smith"
       assert prof.data_quality_flags == []
   ```

2. **Integration test with mock WebFetch response**
   ```python
   @pytest.mark.asyncio
   async def test_discover_professors_with_mock_sdk(mocker):
       # Mock Claude Agent SDK query() function
       mock_query = mocker.patch('claude_agent_sdk.query')
       mock_query.return_value = async_iterator_of_mock_messages()

       dept = Department(id="dept-1", name="CS", url="https://cs.edu/faculty")
       professors = await discover_professors_for_department(dept, "test-corr-id")

       assert len(professors) > 0
       assert professors[0].name == "Dr. Jane Smith"
   ```

3. **Test Playwright fallback**
   ```python
   @pytest.mark.asyncio
   async def test_playwright_fallback_on_webfetch_failure(mocker):
       # Mock SDK query to fail
       mocker.patch('claude_agent_sdk.query', side_effect=Exception("WebFetch failed"))

       # Mock Playwright
       mock_playwright = mocker.patch('playwright.async_api.async_playwright')
       # ... setup mock to return HTML content

       dept = Department(id="dept-1", name="CS", url="https://cs.edu/faculty")
       professors = await discover_professors_for_department(dept, "test-corr-id")

       assert len(professors) > 0
       assert "scraped_with_playwright_fallback" in professors[0].data_quality_flags
   ```

4. **Test parallel execution with asyncio.gather**
   ```python
   @pytest.mark.asyncio
   async def test_parallel_discovery_with_multiple_departments(mocker):
       # Mock discover_professors_for_department to return test data
       async def mock_discover(dept, corr_id):
           return [Professor(id=f"prof-{dept.id}", name=f"Prof from {dept.name}", ...)]

       mocker.patch('src.agents.professor_filter.discover_professors_for_department',
                    side_effect=mock_discover)

       depts = [Department(id=f"d{i}", name=f"Dept{i}", url=f"http://d{i}.edu") for i in range(5)]
       professors = await discover_professors_parallel(depts, max_concurrent=3)

       assert len(professors) == 5
   ```

5. **Test correlation ID propagation**
   ```python
   @pytest.mark.asyncio
   async def test_correlation_id_propagation(mocker, caplog):
       dept = Department(id="dept-1", name="CS", url="https://cs.edu/faculty")
       correlation_id = "test-correlation-123"

       with caplog.at_level(logging.INFO):
           await discover_professors_for_department(dept, correlation_id)

       # Verify correlation_id appears in logs
       assert any(correlation_id in record.message for record in caplog.records)
   ```

6. **Test deduplication with match_names**
   ```python
   @pytest.mark.asyncio
   async def test_deduplication_fuzzy_matching(mocker):
       # Mock llm_helpers.match_names to return dict
       mocker.patch('src.utils.llm_helpers.match_names',
                    return_value={"decision": "Yes", "confidence": 95, "reasoning": "Same person with different title format"})

       professors = [
           Professor(id="p1", name="Dr. Jane Smith", department_id="d1", ...),
           Professor(id="p2", name="Jane A. Smith", department_id="d1", ...)  # Duplicate
       ]

       unique = await deduplicate_professors(professors)
       assert len(unique) == 1
   ```

7. **Test checkpoint saving**
   ```python
   def test_save_professors_checkpoint(tmp_path):
       checkpoint_manager = CheckpointManager(checkpoint_dir=tmp_path)
       professors = [Professor(id="p1", name="Jane", ...)]

       checkpoint_manager.save_batch(phase="phase-2-professors", batch_id=1, data=professors)

       # Verify JSONL file created
       checkpoint_file = tmp_path / "phase-2-professors-batch-1.jsonl"
       assert checkpoint_file.exists()
   ```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |
| 2025-10-07 | 0.2 | Fixed critical issues: Added data_quality_flags to Professor model, specified WebFetch/WebSearch tools explicitly, added correlation_id management, batch config key reference, and deduplication threshold | Sarah (PO) |
| 2025-10-07 | 0.3 | MAJOR REVISION: Removed non-existent AgentDefinition/sub-agent SDK features; replaced with accurate Claude Agent SDK query() patterns; added asyncio.gather() parallel execution; clarified WebFetch/Playwright fallback is application-level; renamed to professor_filter.py; added complete code examples; restructured tasks; added LLM helper spec | Winston (Architect) |
| 2025-10-07 | 0.4 | QA fixes from validation: Fixed template compliance (moved Testing under Dev Notes); added PRD alignment note explaining asyncio.gather() vs sub-agents; added hashlib import to code examples; added system_params.json structure example | Sarah (PO) |
| 2025-10-07 | 0.5 | CRITICAL FIX: Corrected match_names() function usage - updated Task 6, Dev Notes Pattern 4, and Test #6 to use dict return type ({"decision", "confidence", "reasoning"}) instead of float; verified Claude Agent SDK tool names (WebFetch, WebSearch confirmed correct) | Sarah (PO) |
| 2025-10-07 | 0.6 | VERIFICATION & APPROVAL: SDK API patterns verified against official Claude Agent SDK docs (AssistantMessage, TextBlock confirmed correct); added WebFetch/WebSearch tool clarification comments; added file naming scope note; parallel processing conflict resolved; story APPROVED for implementation | Sarah (PO) + Winston (Architect) |
| 2025-10-07 | 0.7 | QA IMPROVEMENTS: Standardized reference format (added doc paths); added Epic 2 Department model context summary; added system_params.json example location reference; improved reference effectiveness per validation feedback | Sarah (PO) |
| 2025-10-07 | 0.8 | CRITICAL FIXES from validation: (1) Fixed match_names() code example to match actual implementation (dict return, not float) - function already exists in src/utils/llm_helpers.py; (2) Added Subtask 4.5 for rate limiting implementation; (3) Added comprehensive rate limiting code examples with aiolimiter; (4) Added data quality flags documentation; (5) Updated dependencies list to include aiolimiter and SystemParams model | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
