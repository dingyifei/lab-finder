# Story 3.2: LLM-Based Professor Research Field Filtering

## Status

**Draft**

## Story

**As a** user,
**I want** professors filtered based on research field alignment with my interests,
**so that** I only analyze labs that are potentially good fits.

## Acceptance Criteria

1. LLM analyzes each professor's research areas against user profile (FR11)
2. Matching logic: Include if ANY major research field overlaps with user interests
3. Interdisciplinary researchers appropriately handled (not over-filtered)
4. Major deviations excluded (e.g., literature professor for bioengineering student)
5. Edge cases handled: emerging fields, multi-disciplinary labs, co-appointments
6. Filtering uses consolidated research profile from Epic 1

## Dependencies

**Must Be Complete Before Starting:**
- **Epic 1** (Foundation & Configuration Infrastructure)
  - Story 1.7: User Profile Consolidation (provides `output/user-profile.md`)
  - Story 1.4: Shared Utilities Implementation (provides `llm_helpers`, `checkpoint_manager`, `progress_tracker`)
- **Story 3.1a**: Professor Model + Basic Discovery (provides Professor model definition)
- **Story 3.1b**: Parallel Processing & Batch Coordination (provides `checkpoints/phase-2-professors-batch-*.jsonl`)

## Tasks / Subtasks

- [ ] **Task 1: Load User Research Profile** (AC: 1, 6)
  - [ ] Load consolidated profile from `output/user-profile.md` (Story 1.7)
  - [ ] Validate required fields are present (research interests, degree program)
  - [ ] If research interests missing: Log error and halt (cannot filter without criteria)
  - [ ] If degree program missing: Use "General" as fallback and log warning
  - [ ] Extract research interests section
  - [ ] Extract degree program and academic background
  - [ ] Pass profile context to LLM filtering function

- [ ] **Task 2: Update Professor Model in `src/models/professor.py`** (AC: 1)
  - [ ] Modify existing `src/models/professor.py` (created in Story 3.1a) to add NEW filter fields:
    ```python
    # NEW fields for Story 3.2 (LLM-based filtering)
    is_relevant: bool = False  # Filter decision
    relevance_confidence: int = 0  # 0-100
    relevance_reasoning: str = ""  # LLM explanation
    ```
  - [ ] Update `PROFESSOR_DATA_QUALITY_FLAGS` constant to add new flags: `"llm_filtering_failed"`, `"low_confidence_filter"`
  - [ ] Set fields based on LLM filtering decision
  - [ ] Preserve all professors (both relevant and filtered) in checkpoint

- [ ] **Task 3: Implement LLM-Based Filtering in `src/agents/professor_filter.py`** (AC: 1, 2, 3, 4, 5)
  - [ ] Add filtering methods to existing `ProfessorFilterAgent` class (created in Story 3.1a)
  - [ ] **Subtask 3.1: Implement Core LLM Filtering Logic**
    - [ ] Add `async def filter_professor_single(professor: Professor, profile: UserProfile) -> dict` helper method
    - [ ] Load confidence threshold from `system_params.json` (`confidence_thresholds.professor_relevance_minimum`) - NOTE: Threshold stored for Story 3.3 use; this story uses LLM decision directly
    - [ ] Extract string fields from Professor and UserProfile models to call existing `llm_helpers.filter_professor_research()`
    - [ ] Convert Professor model fields: `professor.name`, `", ".join(professor.research_areas)`, `professor.title`
    - [ ] Convert UserProfile to formatted string (see UserProfile Loading section in Dev Notes)
    - [ ] Call `llm_helpers.filter_professor_research(professor_name, research_areas, user_profile_str, bio="", correlation_id)`
    - [ ] Map response from `{"confidence": 0-100, "reasoning": "..."}` to `{"decision": "include"|"exclude", "confidence": 0-100, "reasoning": "..."}`
    - [ ] Decision logic: confidence >= 70 → "include", confidence < 70 → "exclude" (uses threshold for decision making)
    - [ ] Note: LLM helper uses existing PROFESSOR_FILTER_TEMPLATE with retry logic (3 attempts, exponential backoff)
    - [ ] Parse JSON response (decision, confidence, reasoning)
    - [ ] If LLM call fails after retries: Log error, default to "include" (inclusive filtering), set confidence to 0, add data quality flag
    - [ ] If JSON parsing fails: Log error, default to "include", set confidence to 0, add data quality flag
    - [ ] Validate response has required fields (decision, confidence, reasoning)
  - [ ] **Subtask 3.2: Handle Interdisciplinary Researchers** (AC: 3, 5)
    - [ ] Ensure professors with multiple research areas get fair evaluation
    - [ ] If ANY research area matches user interests: INCLUDE
    - [ ] Example: CS professor researching computational biology → include for biology student
    - [ ] Edge case: Co-appointed professors (multiple departments) → evaluate all affiliations
    - [ ] Log interdisciplinary inclusions separately
  - [ ] **Subtask 3.3: Handle Edge Cases** (AC: 5)
    - [ ] **Emerging Fields:** Include if conceptually related (e.g., "quantum ML" for ML student)
    - [ ] **Generic Descriptions:** If research areas vague (e.g., "various topics") → include by default
    - [ ] **Missing Research Areas:** If no research areas listed → include (investigate further in next phase)
    - [ ] **Multiple Departments:** Evaluate all department affiliations
    - [ ] Log edge cases with WARNING level for user review

- [ ] **Task 4: Implement Main Batch Orchestration in `ProfessorFilterAgent.filter_professors()`** (AC: 1)
  - [ ] Add `async def filter_professors(professors: list[Professor], profile: UserProfile, config: SystemParams) -> list[Professor]` method to ProfessorFilterAgent
  - [ ] Load professors in batches from `checkpoints/phase-2-professors-batch-*.jsonl` (Story 3.1b output)
  - [ ] Process each batch through LLM filtering (call `filter_professor_single()` for each professor)
  - [ ] Use batch size from `config` (`batch_config.professor_filter_batch_size`)
  - [ ] Save filtered results after each batch (see Task 5)
  - [ ] Enable resumability from last completed batch using `checkpoint_manager.get_resume_point()`

- [ ] **Task 5: Save Filtered Professors to Checkpoint** (AC: 1)
  - [ ] Save ALL professors (relevant + filtered) to updated checkpoint after each batch
  - [ ] Use `checkpoint_manager.save_batch("phase-2-filter", batch_id, professors_batch)`
  - [ ] Save to `checkpoints/phase-2-professors-filtered-batch-{N}.jsonl`
  - [ ] Include filter decision, confidence, reasoning in serialized data (Professor model fields)

- [ ] **Task 6: Integrate Progress Tracking** (AC: 1)
  - [ ] Use `progress_tracker` from Story 1.4
  - [ ] Display: "Phase 2: Professor Filtering [X/Y professors processed]"
  - [ ] Show batch progress using `progress_tracker.update_batch(batch_num, total_batches)`
  - [ ] Display summary: "X of Y professors included (Z% match rate)"

- [ ] **Task 7: Create Comprehensive Test Suite** (AC: all)
  - [ ] Create `tests/unit/test_professor_filter.py` for filtering-specific tests
  - [ ] Implement all 7 test cases from Testing section (see Dev Notes)
  - [ ] Mock LLM responses with pytest-mock
  - [ ] Test edge cases: interdisciplinary, emerging fields, missing data
  - [ ] Ensure 70% coverage minimum (pytest --cov)
  - [ ] All tests must pass before marking story complete

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Filter Agent (Epic 3)

**Responsibility:** Filter professors by research field alignment using LLM (Epic 3: FR11)

**Key Interfaces:**
- `async def filter_professors(professors: list[Professor], profile: UserProfile, config: SystemParams) -> list[Professor]` - Main filtering orchestration function (returns ALL professors with is_relevant field updated)
- `async def filter_professor_single(professor: Professor, profile: UserProfile) -> FilterResult` - LLM-based filtering for single professor (internal helper)

**Dependencies:**
- LLM Helpers for prompt templates and LLM calls (Story 1.4)
- Professor data from Story 3.1b checkpoints
- User profile from Story 1.7
- Checkpoint Manager for saving results (Story 1.4)
- Progress Tracker for status updates (Story 1.4)

**UserProfile Model Reference (from Story 1.7):**
- **Defined in:** `src/models/user_profile.py` (Story 1.7)
- **Key fields needed for this story:**
  - `research_interests: str` - User's research interests (required)
  - `degree_program: str` - Current degree program (optional, defaults to "General")
  - `background: str` - Educational background summary (optional)
- **Loading Pattern:**
  ```python
  from src.models.user_profile import UserProfile
  from src.utils.checkpoint_manager import CheckpointManager

  def load_user_profile() -> UserProfile:
      """Load consolidated user profile from Story 1.7 output."""
      checkpoint_manager = CheckpointManager()
      profile_data = checkpoint_manager.load_batches("phase-0-user-profile")[0]
      profile = UserProfile(**profile_data)

      # Validation
      if not profile.research_interests:
          logger.error("Research interests missing from user profile")
          raise ValueError("Cannot filter professors without research interests")

      if not profile.degree_program:
          logger.warning("Degree program missing, using 'General' as fallback")
          profile.degree_program = "General"

      return profile
  ```
- **Conversion to String for LLM Helper:**
  ```python
  def format_profile_for_llm(profile: UserProfile) -> str:
      """Format UserProfile as string for llm_helpers.filter_professor_research()."""
      return f"""Research Interests: {profile.research_interests}
  Degree Program: {profile.degree_program}
  Background: {profile.background or 'Not specified'}"""
  ```

**Shared Utility Function Signatures:**
```python
# llm_helpers.py (Story 1.4)
async def filter_professor_research(
    professor_name: str,
    research_areas: str,
    user_profile: str,
    bio: str = "",
    correlation_id: Optional[str] = None,
) -> dict[str, Any]
# Returns: {"confidence": 0-100, "reasoning": "..."}

async def call_llm_with_retry(prompt: str, max_retries: int = 3) -> str

# checkpoint_manager.py (Story 1.4)
def save_batch(phase: str, batch_id: int, data: list[BaseModel]) -> None
def load_batches(phase: str) -> list[list[BaseModel]]
def get_resume_point(phase: str) -> int

# progress_tracker.py (Story 1.4)
def start_phase(name: str, total_items: int) -> None
def update(completed: int) -> None
def update_batch(batch_num: int, total_batches: int, batch_desc: str = "") -> None
def complete_phase() -> None
# Note: update_batch() signature verified against src/utils/progress_tracker.py (Story 1.4 implementation)
# batch_desc parameter defaults to empty string

# logger.py (Story 1.4)
def get_logger(correlation_id: str, phase: str, component: str) -> BoundLogger
```

**Technology Stack:**
- Claude LLM via llm_helpers
- structlog with filtering context
- Pydantic Professor model with filter fields

**Source Tree Location:**
- **Modify:** `src/agents/professor_filter.py` (add filtering methods to existing agent from Story 3.1a)
- **Modify:** `src/models/professor.py` (add filter fields)
- **Create:** `tests/unit/test_professor_filter.py` (filtering-specific tests)
- **Load from:** `checkpoints/phase-2-professors-batch-*.jsonl` (Story 3.1 output)
- **Save to:** `checkpoints/phase-2-professors-filtered-batch-{N}.jsonl`

**Note:** Story 3.1a created `professor_filter.py` with discovery functions. This story adds filtering methods to the same file.

**Story 3.1a/3.1b Integration:**
- **File Relationship:** Story 3.1a created `professor_filter.py` with `ProfessorFilterAgent` class containing discovery methods (`discover_professors_for_department()`, `load_relevant_departments()`). Story 3.1b adds parallel processing and batch coordination. Story 3.2 adds filtering methods (`filter_professors()`, `filter_professor_single()`) to the same `ProfessorFilterAgent` class.
- **Checkpoint Flow:**
  - Story 3.1b produces: `checkpoints/phase-2-professors-batch-*.jsonl` (all discovered professors from parallel processing)
  - Story 3.2 loads from above, filters, produces: `checkpoints/phase-2-professors-filtered-batch-*.jsonl` (all professors with `is_relevant` field updated)
- **Execution Model:** Discovery (3.1a/3.1b) and filtering (3.2) are separate operations. Filtering operates on completed discovery output, not called within discovery loop.
- **Model Fields:** Professor model created in 3.1a with base fields. This story adds filter-specific fields (`is_relevant`, `relevance_confidence`, `relevance_reasoning`).

**Professor Model Extensions (NEW fields for Story 3.2):**

Modify existing `src/models/professor.py` to add the following NEW fields:

```python
class Professor(BaseModel):
    # ... existing fields from Story 3.1 ...
    id: str
    name: str
    title: str
    department_id: str
    department_name: str
    school: Optional[str] = None
    lab_name: Optional[str] = None
    lab_url: Optional[str] = None
    research_areas: list[str] = []
    profile_url: str
    email: Optional[str] = None
    data_quality_flags: list[str] = []

    # NEW fields for Story 3.2 (LLM-based filtering) - ADD THESE:
    is_relevant: bool = False  # Filter decision
    relevance_confidence: int = 0  # 0-100
    relevance_reasoning: str = ""  # LLM explanation
```

**Additional Data Quality Flags for Story 3.2:**

Add to `PROFESSOR_DATA_QUALITY_FLAGS` constant in `src/models/professor.py`:
```python
PROFESSOR_DATA_QUALITY_FLAGS = {
    # ... existing flags from Story 3.1a ...
    "scraped_with_playwright_fallback",
    "missing_email",
    "missing_research_areas",
    "missing_lab_affiliation",
    "ambiguous_lab",
    # NEW flags for Story 3.2:
    "llm_filtering_failed",  # LLM call failed, used inclusive default (confidence=0)
    "low_confidence_filter",  # Confidence below threshold (for Story 3.3 use)
}
```

**LLM Filtering Approach:**
- Uses existing `llm_helpers.filter_professor_research()` function (Story 1.4)
- Returns confidence score (0-100), not direct decision
- Story 3.2 maps confidence to decision: >= 70 = "include", < 70 = "exclude"
- Template (`PROFESSOR_FILTER_TEMPLATE` in llm_helpers.py:112-140) uses XML-tagged format
- Includes retry logic (3 attempts, exponential backoff 2-10s)

**Filtering Philosophy:**
- **Inclusive by default:** ANY research overlap → include
- **Interdisciplinary friendly:** Multiple research areas evaluated holistically
- **Emerging fields:** Include conceptually related novel areas
- **Exclude only obvious mismatches:** E.g., literature for engineering, music for biology

**Error Fallback Strategy:**
- **LLM call fails** → include by default (conservative: don't lose potential matches)
- **JSON parse fails** → include by default
- **Confidence set to 0** on fallback
- **Data quality flag added:** `"llm_filtering_failed"` (see Data Quality Flags section below)
- **Rationale:** Better to include borderline cases for human review than auto-exclude potential matches

**Error Handling & Configuration:**

**System Configuration (`system_params.json`):**
```json
{
  "batch_config": {
    "professor_filter_batch_size": 15  // Batch size for professor filtering
  },
  "confidence_thresholds": {
    "professor_relevance_minimum": 70.0  // NOTE: Loaded in this story but used in Story 3.3 for flagging low-confidence matches
  }
}
```

**LLM Error Handling Pattern:**
```python
# Retry logic with exponential backoff (from tenacity library)
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def call_llm_for_filtering(professor, profile):
    try:
        response = await llm_helpers.filter_professor(professor, profile)
        result = json.loads(response)
        validate_filter_response(result)  # Ensure required fields present
        return result
    except json.JSONDecodeError as e:
        logger.error("JSON parsing failed", error=str(e), professor=professor.name)
        # Fallback: inclusive filtering with data quality flag
        return {
            "decision": "include",
            "confidence": 0,
            "reasoning": "LLM response parsing failed - included by default"
        }
    except Exception as e:
        logger.error("LLM call failed", error=str(e), professor=professor.name)
        raise  # Let retry decorator handle

# After all retries exhausted, use fallback
try:
    result = await call_llm_for_filtering(professor, profile)
except Exception:
    # Inclusive fallback
    result = {"decision": "include", "confidence": 0, "reasoning": "LLM unavailable"}
    professor.data_quality_flags.append("llm_filtering_failed")
```

**User Profile Validation:**
```python
def load_user_profile() -> UserProfile:
    profile = load_from_file("output/user-profile.md")

    # Critical field validation
    if not profile.research_interests:
        logger.error("Research interests missing from user profile")
        raise ValueError("Cannot filter professors without research interests")

    # Optional field fallback
    if not profile.degree_program:
        logger.warning("Degree program missing, using 'General' as fallback")
        profile.degree_program = "General"

    return profile
```

**Batch Processing Pattern:**
```python
# Load professors in batches
resume_batch = checkpoint_manager.get_resume_point("phase-2-filter")
for batch_id in range(resume_batch, total_batches):
    professors_batch = load_professor_batch(batch_id)

    # Filter each professor in batch
    for professor in professors_batch:
        filter_result = llm_filter(professor, user_profile)
        professor.is_relevant = filter_result.decision == "include"
        professor.relevance_confidence = filter_result.confidence
        professor.relevance_reasoning = filter_result.reasoning

    # Save batch with filter results
    checkpoint_manager.save_batch("phase-2-filter", batch_id, professors_batch)
```

**Critical Rules (from Coding Standards):**
- All LLM calls must use llm_helpers module
- Never use print() for logging (use structlog)
- Always type hint function signatures
- Use checkpoint_manager.save_batch() - never write JSONL directly

**Architecture Component Diagram Flow:**
```
CLI Coordinator → Professor Filter Agent
Professor Filter Agent → Checkpoint Manager (load professors)
Professor Filter Agent → LLM Helpers (filter each professor)
Professor Filter Agent → Checkpoint Manager (save filtered results)
Professor Filter Agent → Progress Tracker (display progress)
```

### Testing

**Test File Location:** `tests/unit/test_professor_filter.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Mock LLM responses with pytest-mock
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Unit test for filter_professors with mock LLM responses
2. Test inclusive filtering (ANY overlap includes professor)
3. Test exclusion of completely unrelated professors
4. Test interdisciplinary researcher handling
5. Test edge cases (emerging fields, missing research areas)
6. Test batch processing and checkpoint saving
7. Test confidence score assignment

**Example Test Pattern:**
```python
def test_filter_includes_interdisciplinary_professor(mocker):
    # Arrange
    mock_llm = mocker.patch('src.utils.llm_helpers.call_llm_with_retry')
    mock_llm.return_value = json.dumps({
        "decision": "include",
        "confidence": 85,
        "reasoning": "Computational biology overlaps with ML interests"
    })

    professor = Professor(
        id="prof-1",
        name="Dr. Jane Smith",
        research_areas=["Computational Biology", "Bioinformatics"],
        profile_url="https://prof.edu"
    )
    profile = UserProfile(research_interests="Machine Learning")

    agent = ProfessorFilterAgent()

    # Act
    filtered = agent.filter_professors([professor], profile)

    # Assert
    assert len(filtered) == 1
    assert filtered[0].is_relevant == True
    assert filtered[0].relevance_confidence == 85
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |
| 2025-10-07 | 0.2 | Added Dependencies section, enhanced error handling in Task 2, clarified NEW fields in Professor model, added user profile validation to Task 1, standardized config keys, added comprehensive error handling patterns in Dev Notes | Sarah (PO) |
| 2025-10-07 | 0.3 | CRITICAL FIX: Reordered Task 5 (Update Professor Model) to Task 2 (before LLM filtering implementation); Added shared utility function signatures to Dev Notes; Clarified confidence threshold usage (stored for Story 3.3) | Sarah (PO) |
| 2025-10-07 | 0.4 | DOCUMENTATION FIX: Corrected utility references from Story 1.7→1.4; Added default parameter to update_batch() signature; Added verification note confirming signature matches implementation | Sarah (PO) |
| 2025-10-07 | 0.5 | PO VALIDATION FIXES: (1) Fixed Dependencies section Story 1.4/1.7 inversion; (2) Added main function signatures to Key Interfaces; (3) Clarified file modification vs creation in Source Tree Location; (4) Restructured Tasks 4-5 as subtasks of Task 3 for better clarity; (5) Added explicit code location context to all tasks; (6) Added Story 3.1a Integration section explaining relationship with discovery functions; (7) Added test file creation note | Sarah (PO) |
| 2025-10-07 | 0.6 | **PO VALIDATION CORRECTIONS** - Post-validation fixes: (1) CRITICAL: Fixed config key mismatches (`batch_config.professor_filter_batch_size`, `confidence_thresholds.professor_relevance_minimum`); (2) CRITICAL: Updated Dependencies to reference 3.1a/3.1b specifically; (3) Added UserProfile model reference section with loading pattern; (4) Clarified LLM helper usage - use existing `filter_professor_research()` with field conversion; (5) Removed redundant prompt template (uses existing PROFESSOR_FILTER_TEMPLATE); (6) Added Task 7: Create Test Suite; (7) Added Error Fallback Strategy section; (8) Added Data Quality Flags constants section | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
