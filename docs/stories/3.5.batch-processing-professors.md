# Story 3.5: Batch Processing for Professor Analysis

## Status

**Draft**

## Story

**As a** user,
**I want** professor filtering executed in configurable batches,
**so that** the system manages parallel async operations and API calls efficiently.

## Acceptance Criteria

1. Professor list divided into batches based on config (NFR4)
2. Each batch processed before moving to next
3. Checkpoint saved after each batch completion (NFR12)
4. Progress indicators show: "Processing batch X of Y (Z professors)"
5. If execution interrupted, resumable from last completed batch
6. Batch size configurable (default: 10-20 professors per batch)

## Tasks / Subtasks

- [ ] **Task 1: Load Batch Configuration** (AC: 1, 6)
  - [ ] Load from `config/system_params.json`
  - [ ] Use `batch_config.professor_discovery_batch_size` for discovery
  - [ ] Use `batch_config.professor_filtering_batch_size` for filtering (default: 15)
  - [ ] Validate batch sizes > 0 and < 100
  - [ ] Log loaded batch configuration

- [ ] **Task 2: Divide Professors into Batches** (AC: 1)
  - [ ] Use utility from Story 2.5: `divide_into_batches(professors, batch_size)`
  - [ ] Calculate total batches: `ceil(total_professors / batch_size)`
  - [ ] Return list of professor batches
  - [ ] Log: "Divided X professors into Y batches"

- [ ] **Task 3: Process Batches Sequentially** (AC: 2)
  - [ ] For each batch:
    - Process all professors in batch through filtering
    - Wait for batch completion
    - Save checkpoint
    - Move to next batch
  - [ ] Within batch: Parallel LLM calls for professors (async)
  - [ ] Between batches: Sequential execution
  - [ ] Log batch start/completion

- [ ] **Task 4: Implement Batch-Level Checkpointing** (AC: 3)
  - [ ] Use checkpoint_manager from Story 1.7
  - [ ] After each batch filtered, save:
    - `checkpoints/phase-2-professors-filtered-batch-{N}.jsonl`
  - [ ] Include all professor data with filter results
  - [ ] Use `checkpoint_manager.save_batch("phase-2-filter", batch_id, batch_data)`

- [ ] **Task 5: Implement Resume from Checkpoint** (AC: 5)
  - [ ] On startup, check for existing filter checkpoints
  - [ ] Use `checkpoint_manager.get_resume_point("phase-2-filter")`
  - [ ] If resume_point > 0: Load completed batches, skip to resume point
  - [ ] Log: "Resuming professor filtering from batch X"
  - [ ] Process only remaining batches

- [ ] **Task 6: Integrate Two-Level Progress Tracking** (AC: 4)
  - [ ] Use progress_tracker from Story 1.7
  - [ ] Level 1: Overall batch progress
    - "Phase 2: Professor Filtering [batch X/Y]"
  - [ ] Level 2: Within-batch progress
    - "Batch X: Filtering professor 3 of 15"
  - [ ] Update progress after each professor filtered
  - [ ] Display ETA based on batch completion rate

- [ ] **Task 7: Optimize LLM Calls within Batch** (AC: 1)
  - [ ] Use async/await for parallel LLM calls within batch
  - [ ] Launch all LLM filter calls for batch simultaneously
  - [ ] Gather results when all complete
  - [ ] Respect rate limits (use semaphore if needed)
  - [ ] Log: "Processing batch X with Y parallel LLM calls"

- [ ] **Task 8: Handle Batch Processing Failures** (AC: 5)
  - [ ] If batch processing fails: Log error, save partial results
  - [ ] Mark failed professors with `data_quality_flags: ["filter_failed"]`
  - [ ] Continue to next batch (don't fail entire pipeline)
  - [ ] At end, report: "X professors failed filtering (see logs)"
  - [ ] Failed professors excluded but logged for manual review

## Dev Notes

### Relevant Architecture Information

**Component:** CLI Coordinator + Professor Filter Agent (Epic 3)

**Responsibility:** Batch-level coordination for professor filtering with resumability (NFR4, NFR12)

**Key Interfaces:**
- `process_professor_batch(batch: list[Professor], batch_id: int) -> list[Professor]`
- `resume_professor_filtering(start_batch: int) -> None`

**Dependencies:**
- Checkpoint Manager for batch checkpointing (Story 1.7)
- Progress Tracker for batch progress display (Story 1.7)
- Professor Filter Agent from Story 3.2
- System parameters config for batch size

**Technology Stack:**
- Python asyncio for parallel LLM calls within batch
- Checkpoint Manager JSONL batching
- math.ceil for batch calculation

**Source Tree Location:**
- Modify: `src/coordinator.py` (add batch processing logic for Phase 2)
- Modify: `src/agents/professor_filter.py` (support batch processing)
- Load from: `checkpoints/phase-2-professors-batch-*.jsonl` (from discovery)
- Save to: `checkpoints/phase-2-professors-filtered-batch-*.jsonl`

**Batch Processing Flow:**
```
1. Load all discovered professors from Phase 2 discovery checkpoints
2. Check for resume point (existing filter checkpoints)
3. Divide professors into batches (batch_size from config)
4. For each batch (starting from resume point):
   a. Launch parallel LLM filter calls for all professors in batch
   b. Gather all results
   c. Update professor models with filter results
   d. Save batch checkpoint
   e. Update progress tracker
5. Mark phase-2-filter complete
6. Generate filtered-professors.md report
```

**Async LLM Calls within Batch:**
```python
async def filter_professor_batch(batch: list[Professor], profile: UserProfile) -> list[Professor]:
    """Filter all professors in batch with parallel LLM calls."""
    tasks = [filter_single_professor_async(p, profile) for p in batch]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Handle results and exceptions
    for professor, result in zip(batch, results):
        if isinstance(result, Exception):
            professor.data_quality_flags.append("filter_failed")
            logger.error(f"Filter failed for {professor.name}: {result}")
        else:
            professor.is_relevant = result.decision == "include"
            professor.relevance_confidence = result.confidence
            professor.relevance_reasoning = result.reasoning

    return batch
```

**Resumability Pattern:**
```python
# Get resume point
resume_batch = checkpoint_manager.get_resume_point("phase-2-filter")

if resume_batch > 0:
    logger.info(f"Resuming from batch {resume_batch}")
    # Load already completed batches for aggregation
    completed_professors = checkpoint_manager.load_batches("phase-2-filter")
else:
    logger.info("Starting fresh professor filtering")
    completed_professors = []

# Process remaining batches
for batch_id in range(resume_batch, total_batches):
    batch = professor_batches[batch_id]
    filtered_batch = await filter_professor_batch(batch, user_profile)
    checkpoint_manager.save_batch("phase-2-filter", batch_id, filtered_batch)
    progress_tracker.update(batch_id + 1)
```

**Batch Size Recommendations:**
- **Small universities (<50 professors):** batch_size = 10
- **Medium universities (50-200 professors):** batch_size = 20
- **Large universities (>200 professors):** batch_size = 30
- **Rate-limited scenarios:** batch_size = 5-10

**Critical Rules (from Coding Standards):**
- Use checkpoint_manager.save_batch() - never write JSONL directly
- Always type hint function signatures
- Never use print() for logging (use structlog)
- Batch sizes must be configurable, never hardcoded

**Architecture Component Diagram Flow:**
```
CLI Coordinator → Load batch config
CLI Coordinator → Divide professors into batches
CLI Coordinator → Get resume point (checkpoint_manager)
For each batch:
  CLI Coordinator → Professor Filter Agent (filter batch)
  Professor Filter Agent → Async LLM calls (parallel within batch)
  CLI Coordinator → Checkpoint Manager (save batch)
  CLI Coordinator → Progress Tracker (update progress)
CLI Coordinator → Report Generator (filtered-professors.md)
```

### Testing

**Test File Location:** `tests/integration/test_professor_batch_processing.py`

**Testing Standards:**
- Framework: pytest 7.4.4 with pytest-asyncio
- Integration tests with mock LLM responses
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Test batch division with different batch sizes
2. Test async LLM calls within batch
3. Test batch checkpoint creation
4. Test resume from checkpoint (partial completion)
5. Test progress tracking for batches
6. Test batch processing failure handling
7. Integration test with full batch processing flow

**Example Test Pattern:**
```python
@pytest.mark.asyncio
async def test_process_professor_batch_async(tmp_path, mocker):
    # Arrange
    mock_llm = mocker.patch('src.utils.llm_helpers.call_llm_with_retry')
    mock_llm.return_value = '{"decision": "include", "confidence": 90, "reasoning": "Match"}'

    professors = [
        Professor(id=str(i), name=f"Prof{i}", profile_url=f"https://p{i}.edu")
        for i in range(15)
    ]
    profile = UserProfile(research_interests="AI")
    agent = ProfessorFilterAgent()

    # Act
    start_time = time.time()
    filtered = await agent.filter_professor_batch(professors, profile)
    duration = time.time() - start_time

    # Assert
    assert len(filtered) == 15
    assert all(p.is_relevant for p in filtered)
    # Async should be faster than sequential (15 * LLM_time)
    assert duration < 5  # Should complete in parallel, not 15+ seconds
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
