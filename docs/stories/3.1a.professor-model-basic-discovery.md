# Story 3.1a: Professor Model + Basic Discovery (Foundation)

## Status

**Draft**

## Story

**As a** user,
**I want** professors identified from relevant departments with basic web scraping,
**so that** I have a foundation list of professors to filter.

## Acceptance Criteria

1. Departments loaded from Epic 2 checkpoint output
2. Professor directory pages discovered and scraped using Claude Code WebFetch/WebSearch tools
3. Professor names, titles, department affiliations extracted
4. Lab affiliations identified where available
5. Research area descriptions extracted from directory listings
6. WebFetch/WebSearch tools used with explicit Playwright fallback when needed
7. Professor Pydantic model created with all required fields

## Dependencies

**Must Be Complete Before Starting:**
- **Epic 2** (University Discovery) - provides `checkpoints/phase-1-relevant-departments.jsonl`
- **Story 1.4**: Shared Utilities Implementation (provides `checkpoint_manager`, `logger`)

**Blocks:**
- **Story 3.1b** - Requires Professor model and basic discovery functions

## Tasks / Subtasks

- [ ] **Task 1: Load Relevant Departments from Epic 2** (AC: 1)
  - [ ] Use checkpoint_manager to load `checkpoints/phase-1-relevant-departments.jsonl`
  - [ ] Deserialize to Department Pydantic models
  - [ ] Verify all departments have required fields (name, url)
  - [ ] Log: "Loaded X relevant departments for professor discovery"

- [ ] **Task 2: Create Professor Pydantic Model** (AC: 3, 4, 5, 7)
  - [ ] Create `src/models/professor.py` module
  - [ ] Define Professor model:
    ```python
    class Professor(BaseModel):
        id: str  # Unique identifier (generated)
        name: str  # Full name
        title: str  # Academic title (Professor, Associate Professor, etc.)
        department_id: str  # Foreign key to Department
        department_name: str  # Department name for display
        school: Optional[str] = None  # School/college
        lab_name: Optional[str] = None  # Lab affiliation if available
        lab_url: Optional[str] = None  # Lab website URL
        research_areas: list[str] = []  # Research area descriptions
        profile_url: str  # Faculty profile URL
        email: Optional[str] = None  # Contact email if available
        data_quality_flags: list[str] = []  # Quality issues
    ```
  - [ ] Add type hints and validation
  - [ ] Run mypy to verify type correctness

- [ ] **Task 3: Implement Professor Discovery with Web Scraping** (AC: 2, 3, 4, 5, 6)
  - [ ] Create `src/agents/professor_filter.py` module
  - [ ] **Note:** File named `professor_filter.py` for combined discovery+filtering agent architecture
    - Story 3.1a implements discovery functions
    - Story 3.2 adds filtering functions to same file
    - Rationale: Discovery and filtering are tightly coupled operations in same agent context
  - [ ] Implement `discover_professors_for_department(department: Department, correlation_id: str) -> list[Professor]`
  - [ ] **Subtask 3.1: Core Discovery Logic**
    - [ ] Use Claude Agent SDK `query()` function to scrape professor directory
    - [ ] Construct prompt: "Scrape {dept.url} and extract professor information"
    - [ ] Configure ClaudeAgentOptions with `allowed_tools=["WebFetch", "WebSearch"]`
    - [ ] Parse response messages to extract professor data
  - [ ] **Subtask 3.2: Handle Multiple Directory Page Formats**
    - [ ] In prompt, instruct Claude to try multiple selector patterns:
      - Common selectors: `.faculty-member`, `.professor-card`, `table.faculty`
      - Semantic HTML: `<article>`, `<section class="people">`
      - Link patterns: `<a href="*/faculty/*">`, `<a href="*/people/*">`
    - [ ] If structured directory not found, search for "faculty" or "people" links
    - [ ] Log which pattern succeeded
    - [ ] Flag departments where discovery failed with data_quality_flag
  - [ ] **Subtask 3.3: Extract Research Areas**
    - [ ] In prompt, instruct to look for research area indicators:
      - Explicit "Research Areas:" sections
      - Keywords in bio/description text
      - Subject tags or labels
    - [ ] Parse research areas into list
    - [ ] Handle missing research areas (empty list, not failure)

- [ ] **Task 4: Implement Playwright Fallback** (AC: 6)
  - [ ] Wrap Claude SDK query() in try-except
  - [ ] On failure or insufficient results, use Playwright:
    ```python
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(department.url)
        content = await page.content()
        await browser.close()
    ```
  - [ ] Parse Playwright HTML content with BeautifulSoup
  - [ ] Flag with data_quality_flag: "scraped_with_playwright_fallback"
  - [ ] Add retry logic with tenacity (3 retries, exponential backoff)

- [ ] **Task 5: Generate Professor IDs** (AC: 3)
  - [ ] Implement ID generation: `hashlib.sha256(f"{name}:{department_id}".encode()).hexdigest()[:16]`
  - [ ] Ensure unique IDs for each professor

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Discovery & Filter Agent (see `docs/prd/epic-3-professor-discovery.md`)

**Responsibility:** Discover professors across filtered departments; extract profile and research data. Filtering logic implemented in Story 3.2.

**Epic 2 Dependencies - Department Model Context:**
This story consumes the output of Epic 2 (University Discovery). The Department model from `docs/stories/2.4.error-handling-structure.md` includes:
- `id: str` - Unique department identifier
- `name: str` - Department name
- `url: str` - Department website URL
- `school: Optional[str]` - Parent school/college name
- `data_quality_flags: list[str]` - Quality tracking flags from Epic 2 processing

Checkpoint location: `checkpoints/phase-1-relevant-departments.jsonl`

**Key Interfaces:**
- `discover_professors_for_department(department: Department, correlation_id: str) -> list[Professor]` - Scrape single department's professor directory

**Dependencies:**
- Department data from Epic 2 checkpoints (see `docs/prd/epic-2-university-discovery.md`)
- Claude Agent SDK (query() function with WebFetch/WebSearch tools)
- Playwright (fallback for JS-heavy pages)
- Checkpoint Manager for loading department data (see `src/utils/checkpoint_manager.py`)
- Structured Logger for correlation tracking (see `src/utils/logger.py`)

**Technology Stack:**
- Claude Agent SDK 0.1.1 - `query()` function
- Playwright 1.55.0 as fallback
- structlog with correlation IDs (application-managed)
- Pydantic Professor model

**Source Tree Location:**
- Create: `src/agents/professor_filter.py` (combined discovery+filtering agent)
- Create: `src/models/professor.py`
- Load from: `checkpoints/phase-1-relevant-departments.jsonl`

**Professor Model Schema:**
```python
from pydantic import BaseModel
from typing import Optional

class Professor(BaseModel):
    id: str  # Unique identifier (SHA256 hash of name:department_id)
    name: str  # Full name
    title: str  # Academic title
    department_id: str  # Links to Department.id
    department_name: str  # For display
    school: Optional[str] = None  # Parent school
    lab_name: Optional[str] = None  # Lab affiliation
    lab_url: Optional[str] = None  # Lab website
    research_areas: list[str] = []  # Research keywords
    profile_url: str  # Faculty profile URL
    email: Optional[str] = None  # Contact email
    data_quality_flags: list[str] = []  # Quality issues
```

**Data Quality Flags Used in Story 3.1a:**
- `scraped_with_playwright_fallback` - WebFetch failed, used Playwright as fallback
- `missing_email` - Professor record has no email address
- `missing_research_areas` - No research areas found in directory listing
- `missing_lab_affiliation` - Lab name/URL not identified
- `ambiguous_lab` - Lab affiliation unclear (multiple possible labs, e.g., professor lists 2+ lab affiliations)

**Error Handling Examples:**
```python
# Example 1: Handle None/invalid department URL
if not department.url or not department.url.startswith('http'):
    logger.error("Invalid department URL", url=department.url)
    # Flag department and skip
    return []

# Example 2: Graceful degradation when all professors fail
try:
    professors = await discover_professors_for_department(dept, correlation_id)
    if not professors:
        logger.warning("No professors discovered", department=dept.name)
        # Continue with empty list - don't block pipeline
except Exception as e:
    logger.error("Department processing failed", error=str(e))
    # Return empty list, process continues with other departments
    return []
```

### Claude Agent SDK Implementation Pattern

**Pattern: Using query() with WebFetch**

```python
from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock
from src.utils.logger import get_logger
import json
import hashlib

async def discover_professors_for_department(
    department: Department,
    correlation_id: str
) -> list[Professor]:
    """Discover professors for a single department using Claude Agent SDK."""
    logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
    logger.info("Starting professor discovery", department=department.name, url=department.url)

    # Configure Claude Agent SDK with built-in web scraping tools
    options = ClaudeAgentOptions(
        allowed_tools=["WebFetch", "WebSearch"],
        max_turns=3,
        system_prompt="You are a web scraping assistant specialized in extracting professor information from university department pages."
    )

    prompt = f"""
    Scrape the professor directory at: {department.url}

    Extract ALL professors with the following information:
    - Full name
    - Title (Professor, Associate Professor, Assistant Professor, etc.)
    - Research areas/interests
    - Lab name and URL (if available)
    - Email address
    - Profile page URL

    Return results as a JSON array of professor objects.
    """

    professors = []
    try:
        async for message in query(prompt=prompt, options=options):
            if isinstance(message, AssistantMessage):
                for block in message.content:
                    if isinstance(block, TextBlock):
                        # Parse JSON response from Claude
                        professors_data = parse_professor_data(block.text)
                        professors.extend(professors_data)

        # Convert to Professor Pydantic models
        professor_models = [
            Professor(
                id=generate_professor_id(p["name"], department.id),
                name=p["name"],
                title=p.get("title", "Unknown"),
                department_id=department.id,
                department_name=department.name,
                school=department.school,
                lab_name=p.get("lab_name"),
                lab_url=p.get("lab_url"),
                research_areas=p.get("research_areas", []),
                profile_url=p.get("profile_url", ""),
                email=p.get("email"),
                data_quality_flags=[]
            )
            for p in professors_data
        ]

        logger.info("Discovery successful", professors_count=len(professor_models))
        return professor_models

    except Exception as e:
        logger.warning("WebFetch failed, falling back to Playwright", error=str(e))
        return await discover_with_playwright_fallback(department, correlation_id)


def parse_professor_data(text: str) -> list[dict]:
    """Parse professor data from Claude's response text."""
    try:
        import re
        json_match = re.search(r'\[.*\]', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
        return []
    except json.JSONDecodeError:
        return []


def generate_professor_id(name: str, department_id: str) -> str:
    """Generate unique professor ID from name and department."""
    content = f"{name}:{department_id}"
    return hashlib.sha256(content.encode()).hexdigest()[:16]
```

### Playwright Fallback Implementation

```python
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

async def discover_with_playwright_fallback(
    department: Department,
    correlation_id: str
) -> list[Professor]:
    """Fallback to Playwright when Claude SDK WebFetch fails."""
    logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
    logger.info("Using Playwright fallback", department=department.name)

    # Handle invalid/missing department URL
    if not department.url or not department.url.startswith('http'):
        logger.error("Invalid department URL", url=department.url, department=department.name)
        return []

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # Set timeout to prevent hanging
            await page.goto(department.url, wait_until="networkidle", timeout=30000)
            await page.wait_for_timeout(2000)
            content = await page.content()
            await browser.close()

            # Parse with BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')

            # Try multiple selector patterns
            professors_data = []
            for selector in SELECTOR_PATTERNS:
                elements = soup.select(selector)
                if elements:
                    logger.debug(f"Found {len(elements)} professors with selector: {selector}")
                    professors_data = parse_professor_elements(elements, department)
                    break

            # No professors found - flag but don't fail
            if not professors_data:
                logger.warning("No professors found with any selector",
                             department=department.name,
                             url=department.url)
                return []

            # Convert to Professor models with playwright flag
            professor_models = []
            for p_data in professors_data:
                prof = Professor(**p_data)
                prof.data_quality_flags.append("scraped_with_playwright_fallback")
                professor_models.append(prof)

            return professor_models

    except Exception as e:
        logger.error("Playwright fallback also failed", error=str(e), department=department.name)
        return []


# Selector patterns to try
SELECTOR_PATTERNS = [
    '.faculty-member', '.professor-card', '.people-item',
    'article.faculty', 'section.people', 'div.directory-entry',
    'table.faculty tr', 'table.directory tr',
    'a[href*="/faculty/"]', 'a[href*="/people/"]'
]
```

### Testing

**Test File Location:** `tests/integration/test_professor_discovery.py`

**Testing Standards:**
- Framework: pytest 8.4.2
- Async tests: pytest-asyncio
- Coverage requirement: 70% minimum

**Test Requirements:**

1. **Unit test for Professor model validation**
   ```python
   def test_professor_model_validation():
       prof = Professor(
           id="test-001",
           name="Dr. Jane Smith",
           title="Professor",
           department_id="dept-001",
           department_name="Computer Science",
           profile_url="https://cs.edu/faculty/jsmith"
       )
       assert prof.name == "Dr. Jane Smith"
       assert prof.data_quality_flags == []
   ```

2. **Integration test with mock WebFetch response**
   ```python
   @pytest.mark.asyncio
   async def test_discover_professors_with_mock_sdk(mocker):
       mock_query = mocker.patch('claude_agent_sdk.query')
       mock_query.return_value = async_iterator_of_mock_messages()

       dept = Department(id="dept-1", name="CS", url="https://cs.edu/faculty")
       professors = await discover_professors_for_department(dept, "test-corr-id")

       assert len(professors) > 0
       assert professors[0].name == "Dr. Jane Smith"
   ```

3. **Test Playwright fallback**
   ```python
   @pytest.mark.asyncio
   async def test_playwright_fallback_on_webfetch_failure(mocker):
       mocker.patch('claude_agent_sdk.query', side_effect=Exception("WebFetch failed"))
       # ... setup mock Playwright

       dept = Department(id="dept-1", name="CS", url="https://cs.edu/faculty")
       professors = await discover_professors_for_department(dept, "test-corr-id")

       assert len(professors) > 0
       assert "scraped_with_playwright_fallback" in professors[0].data_quality_flags
   ```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | 0.1 | Split from Story 3.1 - Foundation: Professor model and basic discovery | Bob (SM) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

**Files Created/Modified in This Story:**
- **CREATE:** `src/models/professor.py` - Professor Pydantic model
- **CREATE:** `src/agents/professor_filter.py` - Discovery functions (Story 3.1a baseline)

**Cross-Story File Tracking for `src/agents/professor_filter.py`:**
- **Story 3.1a (this story):** Creates file with discovery functions
- **Story 3.1b:** Adds parallel processing functions
- **Story 3.1c:** Adds deduplication + rate limiting functions
- **Story 3.2 (future):** Will add filtering functions

_Dev agent: Update with actual files modified during implementation_

## QA Results

_To be populated by QA agent_
