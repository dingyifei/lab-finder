# Story 3.1a: Professor Model + Basic Discovery (Foundation)

## Status

**Done**

## Story

**As a** user,
**I want** professors identified from relevant departments with basic web scraping,
**so that** I have a foundation list of professors to filter.

## Acceptance Criteria

1. Departments loaded from Epic 2 checkpoint output
2. Professor directory pages discovered and scraped using Claude Code WebFetch/WebSearch tools
3. Professor names, titles, department affiliations extracted
4. Lab affiliations identified where available
5. Research area descriptions extracted from directory listings
6. WebFetch/WebSearch tools used with explicit Playwright fallback when needed
7. Professor Pydantic model created with all required fields

## Dependencies

**Must Be Complete Before Starting:**
- **Epic 2** (University Discovery) - provides `checkpoints/phase-1-relevant-departments.jsonl`
- **Story 1.4**: Shared Utilities Implementation (provides `checkpoint_manager`, `logger`)

**Blocks:**
- **Story 3.1b** - Requires Professor model and basic discovery functions

## Tasks / Subtasks

- [x] **Task 1: Load Relevant Departments from Epic 2** (AC: 1)
  - [x] Use checkpoint_manager to load `checkpoints/phase-1-relevant-departments.jsonl`
  - [x] Deserialize to Department Pydantic models
  - [x] Verify all departments have required fields (name, url)
  - [x] Log: "Loaded X relevant departments for professor discovery"

- [x] **Task 2: Create Professor Pydantic Model** (AC: 3, 4, 5, 7)
  - [x] Create `src/models/professor.py` module
  - [x] Define Professor model:
    ```python
    class Professor(BaseModel):
        id: str  # Unique identifier (generated)
        name: str  # Full name
        title: str  # Academic title (Professor, Associate Professor, etc.)
        department_id: str  # Foreign key to Department
        department_name: str  # Department name for display
        school: Optional[str] = None  # School/college
        lab_name: Optional[str] = None  # Lab affiliation if available
        lab_url: Optional[str] = None  # Lab website URL
        research_areas: list[str] = []  # Research area descriptions
        profile_url: str  # Faculty profile URL
        email: Optional[str] = None  # Contact email if available
        data_quality_flags: list[str] = []  # Quality issues
    ```
  - [x] Add type hints and validation
  - [x] Run mypy to verify type correctness

- [x] **Task 3: Implement Professor Discovery with Web Scraping** (AC: 2, 3, 4, 5, 6)
  - [x] Create `src/agents/professor_filter.py` module
  - [x] **Note:** File named `professor_filter.py` for combined discovery+filtering agent architecture
    - Story 3.1a implements discovery functions
    - Story 3.2 adds filtering functions to same file
    - Rationale: Discovery and filtering are tightly coupled operations in same agent context
  - [x] Implement `discover_professors_for_department(department: Department, correlation_id: str) -> list[Professor]`
  - [x] **Subtask 3.1: Core Discovery Logic**
    - [x] Use Claude Agent SDK `ClaudeSDKClient` class to scrape professor directory
    - [x] Construct prompt: "Scrape {dept.url} and extract professor information"
    - [x] Configure ClaudeAgentOptions with `allowed_tools=["WebFetch", "WebSearch"]`
    - [x] Parse response messages to extract professor data
  - [x] **Subtask 3.2: Handle Multiple Directory Page Formats**
    - [x] In prompt, instruct Claude to try multiple selector patterns:
      - Common selectors: `.faculty-member`, `.professor-card`, `table.faculty`
      - Semantic HTML: `<article>`, `<section class="people">`
      - Link patterns: `<a href="*/faculty/*">`, `<a href="*/people/*">`
    - [x] If structured directory not found, search for "faculty" or "people" links
    - [x] Log which pattern succeeded
    - [x] Flag departments where discovery failed with data_quality_flag
  - [x] **Subtask 3.3: Extract Research Areas**
    - [x] In prompt, instruct to look for research area indicators:
      - Explicit "Research Areas:" sections
      - Keywords in bio/description text
      - Subject tags or labels
    - [x] Parse research areas into list
    - [x] Handle missing research areas (empty list, not failure)
  - [x] **Subtask 3.4: Implement Playwright Fallback** (AC: 6)
    - [x] Wrap Claude SDK ClaudeSDKClient in try-except
    - [x] On failure or insufficient results, use Playwright:
      ```python
      async with async_playwright() as p:
          browser = await p.chromium.launch()
          page = await browser.new_page()
          await page.goto(department.url)
          content = await page.content()
          await browser.close()
      ```
    - [x] Parse Playwright HTML content with BeautifulSoup
    - [x] Flag with data_quality_flag: "scraped_with_playwright_fallback"
    - [x] Add retry logic with tenacity (3 retries, exponential backoff)
  - [x] **Subtask 3.5: Generate Professor IDs** (AC: 3)
    - [x] Implement ID generation: `hashlib.sha256(f"{name}:{department_id}".encode()).hexdigest()[:16]`
    - [x] Ensure unique IDs for each professor

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Discovery & Filter Agent (see `docs/prd/epic-3-professor-discovery.md`)

**Responsibility:** Discover professors across filtered departments; extract profile and research data. Filtering logic implemented in Story 3.2.

**Epic 2 Dependencies - Department Model Context:**
This story consumes the output of Epic 2 (University Discovery). The Department model from `docs/stories/2.4.error-handling-structure.md` includes:
- `id: str` - Unique department identifier
- `name: str` - Department name
- `url: str` - Department website URL
- `school: Optional[str]` - Parent school/college name
- `data_quality_flags: list[str]` - Quality tracking flags from Epic 2 processing

Checkpoint location: `checkpoints/phase-1-relevant-departments.jsonl`

**Sample JSONL Checkpoint Format (Epic 2 Output):**
```jsonl
{"id": "cs-001", "name": "Computer Science", "url": "https://engineering.example.edu/cs", "school": "School of Engineering", "data_quality_flags": [], "is_relevant": true, "relevance_reasoning": "Strong alignment with AI/ML research interests"}
{"id": "bio-eng-002", "name": "Bioengineering", "url": "https://engineering.example.edu/bioe", "school": "School of Engineering", "data_quality_flags": ["missing_url"], "is_relevant": true, "relevance_reasoning": "Potential overlap in computational biology"}
```

**Key Interfaces:**
- `discover_professors_for_department(department: Department, correlation_id: str) -> list[Professor]` - Scrape single department's professor directory

**Dependencies:**
- Department data from Epic 2 checkpoints (see `docs/prd/epic-2-university-discovery.md`)
- Claude Agent SDK (query() function with WebFetch/WebSearch tools)
- Playwright (fallback for JS-heavy pages)
- Checkpoint Manager for loading department data (see `src/utils/checkpoint_manager.py`)
- Structured Logger for correlation tracking (see `src/utils/logger.py`)

**Technology Stack:**
- Claude Agent SDK 0.1.1 - `query()` function
- Playwright 1.55.0 as fallback
- structlog with correlation IDs (application-managed)
- Pydantic Professor model

**Source Tree Location:**
- Create: `src/agents/professor_filter.py` (combined discovery+filtering agent)
- Create: `src/models/professor.py`
- Load from: `checkpoints/phase-1-relevant-departments.jsonl`

**Professor Model Schema:**
```python
from pydantic import BaseModel
from typing import Optional

class Professor(BaseModel):
    id: str  # Unique identifier (SHA256 hash of name:department_id)
    name: str  # Full name
    title: str  # Academic title
    department_id: str  # Links to Department.id
    department_name: str  # For display
    school: Optional[str] = None  # Parent school
    lab_name: Optional[str] = None  # Lab affiliation
    lab_url: Optional[str] = None  # Lab website
    research_areas: list[str] = []  # Research keywords
    profile_url: str  # Faculty profile URL
    email: Optional[str] = None  # Contact email
    data_quality_flags: list[str] = []  # Quality issues
```

**Data Quality Flags Used in Story 3.1a:**
- `scraped_with_playwright_fallback` - WebFetch failed, used Playwright as fallback
- `missing_email` - Professor record has no email address
- `missing_research_areas` - No research areas found in directory listing
- `missing_lab_affiliation` - Lab name/URL not identified
- `ambiguous_lab` - Lab affiliation unclear (multiple possible labs, e.g., professor lists 2+ lab affiliations)

**Error Handling Examples:**
```python
from src.utils.logger import get_logger

# Example 1: Handle None/invalid department URL
if not department.url or not department.url.startswith('http'):
    logger.error("Invalid department URL", url=department.url)
    # Flag department and skip
    return []

# Example 2: Graceful degradation when all professors fail
try:
    professors = await discover_professors_for_department(dept, correlation_id)
    if not professors:
        logger.warning("No professors discovered", department=dept.name)
        # Continue with empty list - don't block pipeline
except Exception as e:
    logger.error("Department processing failed", error=str(e))
    # Return empty list, process continues with other departments
    return []

# Example 3: Playwright fallback (see full implementation in Playwright Fallback section below)
# Imports: from playwright.async_api import async_playwright
#          from bs4 import BeautifulSoup
```

### Claude Agent SDK Implementation Pattern

**Pattern: Using ClaudeSDKClient with WebFetch (UPDATED 2025-10-07)**

**IMPORTANT:** Always use `ClaudeSDKClient` class, NOT standalone `query()` function. The standalone function injects unwanted codebase context.

```python
from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions, AssistantMessage, TextBlock
from src.utils.logger import get_logger
import json
import hashlib

async def discover_professors_for_department(
    department: Department,
    correlation_id: str
) -> list[Professor]:
    """Discover professors for a single department using Claude Agent SDK."""
    logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
    logger.info("Starting professor discovery", department=department.name, url=department.url)

    # Configure Claude Agent SDK with built-in web scraping tools
    options = ClaudeAgentOptions(
        allowed_tools=["WebFetch", "WebSearch"],
        max_turns=3,
        system_prompt="You are a web scraping assistant specialized in extracting professor information from university department pages.",
        setting_sources=None  # CRITICAL: Prevents codebase context injection
    )

    prompt = f"""
    Scrape the professor directory at: {department.url}

    Extract ALL professors with the following information:
    - Full name
    - Title (Professor, Associate Professor, Assistant Professor, etc.)
    - Research areas/interests
    - Lab name and URL (if available)
    - Email address
    - Profile page URL

    Return results as a JSON array of professor objects.
    """

    professors_data = []
    try:
        # Use ClaudeSDKClient class (stateless mode)
        async with ClaudeSDKClient(options=options) as client:
            await client.query(prompt)
            async for message in client.receive_response():
                if isinstance(message, AssistantMessage):
                    for block in message.content:
                        if isinstance(block, TextBlock):
                            # Parse JSON response from Claude
                            parsed_data = parse_professor_data(block.text)
                            professors_data.extend(parsed_data)

        # Convert to Professor Pydantic models
        professor_models = [
            Professor(
                id=generate_professor_id(p["name"], department.id),
                name=p["name"],
                title=p.get("title", "Unknown"),
                department_id=department.id,
                department_name=department.name,
                school=department.school,
                lab_name=p.get("lab_name"),
                lab_url=p.get("lab_url"),
                research_areas=p.get("research_areas", []),
                profile_url=p.get("profile_url", ""),
                email=p.get("email"),
                data_quality_flags=[]
            )
            for p in professors_data
        ]

        logger.info("Discovery successful", professors_count=len(professor_models))
        return professor_models

    except Exception as e:
        logger.warning("WebFetch failed, falling back to Playwright", error=str(e))
        return await discover_with_playwright_fallback(department, correlation_id)


def parse_professor_data(text: str) -> list[dict]:
    """Parse professor data from Claude's response text."""
    try:
        import re
        json_match = re.search(r'\[.*\]', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
        return []
    except json.JSONDecodeError:
        return []


def generate_professor_id(name: str, department_id: str) -> str:
    """Generate unique professor ID from name and department."""
    content = f"{name}:{department_id}"
    return hashlib.sha256(content.encode()).hexdigest()[:16]
```

### Playwright Fallback Implementation

```python
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

async def discover_with_playwright_fallback(
    department: Department,
    correlation_id: str
) -> list[Professor]:
    """Fallback to Playwright when Claude SDK WebFetch fails."""
    logger = get_logger(correlation_id=correlation_id, phase="professor_discovery")
    logger.info("Using Playwright fallback", department=department.name)

    # Handle invalid/missing department URL
    if not department.url or not department.url.startswith('http'):
        logger.error("Invalid department URL", url=department.url, department=department.name)
        return []

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # Set timeout to prevent hanging
            await page.goto(department.url, wait_until="networkidle", timeout=30000)
            await page.wait_for_timeout(2000)
            content = await page.content()
            await browser.close()

            # Parse with BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')

            # Try multiple selector patterns
            professors_data = []
            for selector in SELECTOR_PATTERNS:
                elements = soup.select(selector)
                if elements:
                    logger.debug(f"Found {len(elements)} professors with selector: {selector}")
                    professors_data = parse_professor_elements(elements, department)
                    break

            # No professors found - flag but don't fail
            if not professors_data:
                logger.warning("No professors found with any selector",
                             department=department.name,
                             url=department.url)
                return []

            # Convert to Professor models with playwright flag
            professor_models = []
            for p_data in professors_data:
                prof = Professor(**p_data)
                prof.data_quality_flags.append("scraped_with_playwright_fallback")
                professor_models.append(prof)

            return professor_models

    except Exception as e:
        logger.error("Playwright fallback also failed", error=str(e), department=department.name)
        return []


# Selector patterns to try
SELECTOR_PATTERNS = [
    '.faculty-member', '.professor-card', '.people-item',
    'article.faculty', 'section.people', 'div.directory-entry',
    'table.faculty tr', 'table.directory tr',
    'a[href*="/faculty/"]', 'a[href*="/people/"]'
]
```

### Testing

**Test File Location:** `tests/integration/test_professor_discovery.py`

**Testing Standards:**
- Framework: pytest 8.4.2
- Async tests: pytest-asyncio
- Coverage requirement: 70% minimum

**Test Requirements:**

1. **Unit test for Professor model validation**
   ```python
   def test_professor_model_validation():
       prof = Professor(
           id="test-001",
           name="Dr. Jane Smith",
           title="Professor",
           department_id="dept-001",
           department_name="Computer Science",
           profile_url="https://cs.edu/faculty/jsmith"
       )
       assert prof.name == "Dr. Jane Smith"
       assert prof.data_quality_flags == []
   ```

2. **Integration test with mock ClaudeSDKClient**
   ```python
   @pytest.mark.asyncio
   async def test_discover_professors_with_mock_sdk(mocker):
       # Mock ClaudeSDKClient class (not query function)
       mock_client = mocker.AsyncMock()
       mock_client.query = mocker.AsyncMock()
       mock_client.receive_response = mocker.AsyncMock(
           return_value=async_iterator_of_mock_messages()
       )
       mock_client.__aenter__ = mocker.AsyncMock(return_value=mock_client)
       mock_client.__aexit__ = mocker.AsyncMock(return_value=None)

       mocker.patch('claude_agent_sdk.ClaudeSDKClient', return_value=mock_client)

       dept = Department(id="dept-1", name="CS", url="https://cs.edu/faculty")
       professors = await discover_professors_for_department(dept, "test-corr-id")

       assert len(professors) > 0
       assert professors[0].name == "Dr. Jane Smith"
   ```

3. **Test Playwright fallback**
   ```python
   @pytest.mark.asyncio
   async def test_playwright_fallback_on_webfetch_failure(mocker):
       # Mock ClaudeSDKClient to raise exception
       mock_client = mocker.AsyncMock()
       mock_client.__aenter__ = mocker.AsyncMock(side_effect=Exception("WebFetch failed"))
       mocker.patch('claude_agent_sdk.ClaudeSDKClient', return_value=mock_client)

       # Setup mock Playwright
       # ... mock Playwright implementation

       dept = Department(id="dept-1", name="CS", url="https://cs.edu/faculty")
       professors = await discover_professors_for_department(dept, "test-corr-id")

       assert len(professors) > 0
       assert "scraped_with_playwright_fallback" in professors[0].data_quality_flags
   ```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | 0.1 | Split from Story 3.1 - Foundation: Professor model and basic discovery | Bob (SM) |
| 2025-10-07 | 0.2 | **Critical Fix:** Updated Claude Agent SDK pattern from query() to ClaudeSDKClient class per CLAUDE.md standards. Added setting_sources=None. Updated test mocking examples. Added Playwright import reference. Added sample JSONL checkpoint schema. | Sarah (PO) |
| 2025-10-07 | 0.3 | **Task Restructure:** Consolidated Tasks 4-5 as Subtasks 3.4-3.5 for better cohesion. Improved task organization without changing implementation requirements. | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

claude-sonnet-4-5-20250929 (Sonnet 4.5)

### Debug Log References

No debug log entries required - implementation completed without blockers.

### Completion Notes List

**Implementation Summary:**
- Successfully created Professor Pydantic model with all required fields and data quality tracking
- Implemented professor discovery using ClaudeSDKClient (not standalone query()) with WebFetch/WebSearch tools
- Added comprehensive Playwright fallback with retry logic (tenacity: 3 retries, exponential backoff)
- Implemented department loading from Epic 2 checkpoint with validation
- All 17 tests pass (7 unit tests + 10 integration tests)
- Type checking (mypy) and linting (ruff) pass without errors

**Key Implementation Details:**
- Used ClaudeSDKClient class with `setting_sources=None` to prevent codebase context injection per CLAUDE.md
- Implemented multiple selector pattern support in both WebFetch prompt and Playwright fallback
- Added data quality flags: `scraped_with_playwright_fallback`, `missing_email`, `missing_research_areas`, `missing_lab_affiliation`, `ambiguous_lab`
- Professor ID generation using SHA256 hash (first 16 chars) of `name:department_id`
- Graceful degradation: Invalid URLs and failed scraping return empty list (don't block pipeline)

**Testing Coverage:**
- Professor model validation, optional fields, quality flags
- ID generation determinism
- JSON parsing from Claude responses
- ClaudeSDKClient mocking with proper isinstance checks
- Playwright fallback triggering on WebFetch failure
- Department loading with relevance filtering and validation

**Technical Debt/Follow-up:**
- None identified

### File List

**Files Created/Modified in This Story:**
- **CREATE:** `src/models/professor.py` - Professor Pydantic model with data quality tracking
- **CREATE:** `src/agents/professor_filter.py` - Discovery functions (Story 3.1a baseline)
- **CREATE:** `tests/unit/test_professor_model.py` - Unit tests for Professor model (7 tests)
- **CREATE:** `tests/integration/test_professor_discovery.py` - Integration tests for discovery (10 tests)

**Cross-Story File Tracking for `src/agents/professor_filter.py`:**
- **Story 3.1a (this story):** Creates file with discovery functions: `discover_professors_for_department()`, `discover_with_playwright_fallback()`, `load_relevant_departments()`, helper functions
- **Story 3.1b:** Adds parallel processing functions
- **Story 3.1c:** Adds deduplication + rate limiting functions
- **Story 3.2 (future):** Will add filtering functions

## QA Results

### Review Date: 2025-10-07

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall: Excellent ✓**

The implementation demonstrates high-quality software engineering practices with comprehensive test coverage, proper architectural patterns, and excellent adherence to project standards. The developer correctly implemented the ClaudeSDKClient pattern with `setting_sources=None` to prevent codebase context injection, used proper async/await patterns throughout, and implemented graceful degradation with retry logic.

**Strengths:**
- ✓ Correct use of ClaudeSDKClient class (not standalone query()) per CLAUDE.md
- ✓ Comprehensive error handling with graceful degradation (returns empty list vs. failing)
- ✓ Proper async/await patterns throughout all I/O operations
- ✓ Excellent Pydantic model design with validation and helper methods
- ✓ Data quality tracking with well-defined flags
- ✓ Structured logging with correlation IDs (no print() statements)
- ✓ Complete type hints on all functions (mypy passes)
- ✓ Comprehensive docstrings following Google style
- ✓ Retry logic properly implemented with tenacity (3 attempts, exponential backoff)
- ✓ 17 comprehensive tests (7 unit + 10 integration) covering all edge cases

### Requirements Traceability (AC → Tests)

**AC1: Departments loaded from Epic 2 checkpoint**
- ✓ `test_load_relevant_departments` - Verifies loading from checkpoint with is_relevant filtering
- ✓ `test_load_relevant_departments_filters_invalid` - Validates filtering of invalid departments

**AC2: Professor directory pages discovered/scraped using WebFetch/WebSearch**
- ✓ `test_discover_professors_with_mock_sdk` - Verifies WebFetch scraping via ClaudeSDKClient
- ✓ Implementation: Properly configured ClaudeAgentOptions with allowed_tools=["WebFetch", "WebSearch"]

**AC3: Professor names, titles, department affiliations extracted**
- ✓ `test_professor_model_validation` - Validates all required model fields
- ✓ `test_discover_professors_with_mock_sdk` - Verifies extraction from JSON response
- ✓ `test_generate_professor_id` - Validates deterministic ID generation

**AC4: Lab affiliations identified where available**
- ✓ `test_discover_professors_with_mock_sdk` - Verifies lab_name and lab_url extraction
- ✓ `test_professor_model_with_optional_fields` - Validates optional lab fields

**AC5: Research area descriptions extracted**
- ✓ `test_discover_professors_with_mock_sdk` - Verifies research_areas list extraction
- ✓ Implementation: Extracts from JSON response and BeautifulSoup parsing

**AC6: WebFetch/WebSearch with Playwright fallback**
- ✓ `test_discover_professors_sdk_failure_triggers_playwright` - Verifies fallback activation
- ✓ `test_playwright_fallback_invalid_url` - Validates fallback error handling
- ✓ `test_discover_professors_invalid_url` - Validates URL validation
- ✓ Implementation: Retry decorator with 3 attempts, exponential backoff (2-10s)

**AC7: Professor Pydantic model created**
- ✓ All 7 unit tests validate model behavior
- ✓ `test_professor_model_validation` - Required fields
- ✓ `test_professor_model_with_optional_fields` - Optional fields
- ✓ `test_professor_model_missing_required_fields` - Validation errors
- ✓ `test_add_quality_flag` - Quality flag management
- ✓ `test_add_invalid_quality_flag` - Flag validation
- ✓ `test_has_quality_issues` - Quality issue detection
- ✓ `test_professor_data_quality_flags_constant` - Flag constants

**Coverage Result: 7/7 ACs fully tested** ✓

### Refactoring Performed

No refactoring performed. Code quality is excellent as-is.

### Compliance Check

- **Coding Standards:** ✓ PASS
  - No print() statements (uses structured logger)
  - All LLM calls via ClaudeSDKClient (not standalone query())
  - Type hints on all functions
  - Pydantic models with validation
  - Data quality flags implemented
  - Async/await for I/O operations
  - Correlation IDs in logging
  - No hardcoded batch sizes
  - ruff: All checks passed ✓
  - mypy: Success - no issues ✓

- **Project Structure:** ✓ PASS
  - Files in correct locations per source-tree.md
  - src/models/professor.py - Model layer
  - src/agents/professor_filter.py - Agent layer
  - tests/unit/ and tests/integration/ - Test organization
  - Proper naming conventions (snake_case)

- **Testing Strategy:** ✓ PASS
  - 17 tests covering all ACs
  - Unit tests for model validation
  - Integration tests for discovery logic
  - Edge cases covered (invalid URLs, missing data, failures)
  - Proper mocking with MagicMock(spec=Class)
  - All tests passing ✓

- **All ACs Met:** ✓ PASS
  - All 7 acceptance criteria fully implemented and tested

### Improvements Checklist

**Completed Items:**
- [x] All ACs implemented with test coverage
- [x] Proper ClaudeSDKClient usage (not standalone query())
- [x] Comprehensive error handling
- [x] Data quality flag tracking
- [x] Type hints throughout
- [x] Structured logging
- [x] Graceful degradation

**Future Enhancements (Non-Blocking):**
- [ ] Consider extracting hardcoded research keywords from `parse_professor_elements:127-132` to config or use LLM-based extraction
- [ ] Add direct unit test for `parse_professor_elements` helper function (currently tested indirectly)
- [ ] Consider extracting URL validation logic (used in 2 places) to shared utility function

### Security Review

**Status: PASS** ✓

- No hardcoded credentials (uses credential_manager pattern)
- No SQL injection risks (using Pydantic models)
- Safe HTML parsing with BeautifulSoup
- Proper error handling prevents information leakage
- Input validation on department URLs
- No unsafe eval() or exec() usage

### Performance Considerations

**Status: PASS** ✓

- Async/await for all I/O operations (WebFetch, Playwright, checkpoint loading)
- Retry logic with exponential backoff (2s-10s) prevents hammering
- Timeout set on Playwright (30s) prevents hanging
- Efficient checkpoint loading with deduplication by ID
- No N+1 query patterns detected

**Estimated Performance:**
- Single department discovery: 2-5s (WebFetch) or 5-10s (Playwright fallback)
- Retry overhead: Max 2s + 4.6s + 10s = ~17s worst case (3 failures)

### Non-Functional Requirements Assessment

**Reliability:** ✓ PASS
- Comprehensive error handling with try-except blocks
- Graceful degradation (returns [] on failure, doesn't crash)
- Retry logic with tenacity for transient failures
- Input validation (URL format checking)
- Data quality tracking for downstream awareness

**Maintainability:** ✓ PASS
- Clear separation of concerns (model/agent/tests)
- Well-documented code with comprehensive docstrings
- Consistent naming conventions
- Type hints enable IDE assistance
- Good test coverage (all functions tested)

### Files Modified During Review

No files modified during review. Implementation is production-ready as delivered.

### Gate Status

**Gate: PASS** → docs/qa/gates/3.1a-professor-model-basic-discovery.yml

**Quality Score: 100/100**
- 0 failures
- 0 concerns
- All ACs covered
- All tests passing
- All validations passing

### Recommended Status

**✓ Ready for Done**

Story 3.1a is production-ready. All acceptance criteria met with comprehensive test coverage. Code quality is excellent with proper architectural patterns, error handling, and adherence to project standards. The three future enhancement suggestions are nice-to-haves that can be addressed in future refactoring sprints if needed.

**Next Steps:**
1. Mark story status as "Done"
2. Begin Story 3.1b (Parallel Processing & Batch Coordination)
3. Consider future enhancement items during technical debt review
