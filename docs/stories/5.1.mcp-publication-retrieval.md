# Story 5.1: MCP Integration for Publication Retrieval

## Status

**Draft**

## Story

**As a** user,
**I want** paper-search-mcp integrated to retrieve publications for each professor,
**so that** I can analyze recent research output.

## Acceptance Criteria

1. paper-search-mcp configured and connected (NFR6)
2. Publications retrieved for each PI from last 3 years (FR15)
3. Search queries constructed from professor name + university affiliation
4. Publication metadata collected: title, authors, journal, year, abstract
5. Results validated (correct professor, not name collision)
6. Missing publications handled gracefully (some professors may have none)
7. Progress tracking shows publication retrieval status

## Tasks / Subtasks

- [ ] **Task 1: Create Publication Pydantic Model** (AC: 4)
  - [ ] Create `src/models/publication.py`
  - [ ] Define Publication model:
    ```python
    class Publication(BaseModel):
        id: str  # DOI or generated ID
        title: str
        authors: list[str]
        journal: str
        year: int
        abstract: str = ""
        doi: Optional[str] = None
        citations: int = 0
        professor_id: str  # Links to Professor
        is_pi_publication: bool = True  # vs lab member pub
    ```

- [ ] **Task 2: Configure paper-search-mcp in AgentDefinition** (AC: 1)
  - [ ] Use `get_mcp_server_config()` from Story 1.5 to configure Claude Agent SDK
  - [ ] Add `mcp__papers__search_papers` to allowed_tools in AgentDefinition
  - [ ] MCP server spawned automatically by SDK via STDIO - no manual connection needed

- [ ] **Task 3: Query Publications for Each Professor** (AC: 2, 3)
  - [ ] For each professor from Epic 3:
    - Construct query: "author:{name} affiliation:{university}"
    - Set year_range: last 3 years
    - Call paper-search-mcp
    - Parse results into Publication models

- [ ] **Task 4: Validate Publication Authorship** (AC: 5)
  - [ ] Check if professor name in author list
  - [ ] Handle name variations (J. Smith vs Jane Smith)
  - [ ] Use LLM for fuzzy name matching if needed
  - [ ] Filter out false positives

- [ ] **Task 5: Handle Missing Publications** (AC: 6)
  - [ ] If no pubs found: Flag "no_recent_publications"
  - [ ] Don't fail processing
  - [ ] Log: "No publications for {professor_name}"

- [ ] **Task 6: Save Publications to Checkpoint** (AC: 2)
  - [ ] Save to `checkpoints/phase-5a-pi-publications-batch-{N}.jsonl`
  - [ ] Use checkpoint_manager

- [ ] **Task 7: Integrate Progress Tracking** (AC: 7)
  - [ ] Display: "Phase 5A: PI Publication Retrieval [X/Y professors]"

## Dependencies

**Depends On:**
- Epic 3 complete (filtered professor list from `docs/stories/3.*.md`)
- Story 1.5 complete (MCP server setup from `docs/stories/1.5.mcp-server-setup.md` - provides `src/utils/mcp_client.py`)

**Execution Model:**
- FIRST step in Epic 5A pipeline within `src/agents/publication_retrieval.py`
- Followed immediately by Story 5.2 (abstract analysis) and Story 5.3 (journal weighting) in same agent
- All three stories execute in a single pass per professor batch
- Reference: `docs/architecture.md` (lines 827-856, Publication Retrieval Agent)

**Does NOT Depend On:**
- Epic 4 (Lab Websites) - **This story can execute in parallel with Epic 4**

**Blocks:**
- Story 5.2 (must retrieve publications before analysis)
- Story 5.3 (must retrieve publications before weighting)
- Epic 5B (provides PI publications for lab member inference)

**Parallel Execution:**
This story is part of Epic 5A, which executes **concurrently with Epic 4** to reduce overall pipeline timeline by 15-20%. Stories 5.1-5.3 retrieve and analyze PI publications while Epic 4 scrapes lab websites.

## Testing

### Test Approach

**Unit Tests:**
- Publication Pydantic model validation (field types, required fields, defaults)
- MCP query construction (author name formatting, affiliation handling)
- Publication ID generation (DOI vs generated ID)
- Name collision detection logic
- Missing publication handling

**Integration Tests:**
- paper-search-mcp MCP server connectivity
- End-to-end publication retrieval for test professor
- Checkpoint save/load with Publication objects
- Progress tracking display

**Mocking Strategy:**
- Mock `mcp__papers__search_papers` responses using pytest-mock
- Use fixture data from `tests/fixtures/sample_publications.json`
- Mock checkpoint manager for unit tests
- Don't hit live paper-search-mcp in unit tests

**Test Framework:**
- pytest 7.4.4 (per `docs/stories/1.6.testing-infrastructure.md`)
- pytest-asyncio 0.23.3 for async agent methods
- pytest-mock 3.12.0 for MCP mocking

### Key Test Scenarios

**1. Successful Publication Retrieval**
- Input: Professor "Jane Smith" at "Stanford University"
- Expected: 10+ publications from last 3 years
- Validates: MCP query construction, Publication model creation, metadata extraction

**2. Professor with Zero Publications**
- Input: Professor with no recent papers
- Expected: Empty result list, no errors, logged as "No publications for {name}"
- Validates: AC 6 - Missing publications handled gracefully

**3. Name Collision Detection**
- Input: Professor "J. Smith" (common name)
- Expected: Fuzzy matching filters out wrong "J. Smith" authors
- Validates: AC 5 - Results validated, name collision handling

**4. MCP Server Timeout/Failure**
- Input: paper-search-mcp unavailable or slow
- Expected: Graceful degradation, logged error, retry logic triggered
- Validates: Error handling, doesn't crash pipeline

**5. Publication Metadata Completeness**
- Input: Publications with missing abstracts or DOIs
- Expected: Optional fields handled, Publication objects created with defaults
- Validates: AC 4 - Publication metadata collected with proper handling of missing data

**6. Checkpoint Save/Load**
- Input: 20 publications for batch
- Expected: JSONL file created at correct path, reloadable as Publication objects
- Validates: AC 6 - Checkpoint manager integration

**7. Progress Tracking Display**
- Input: Processing 5 professors
- Expected: "Phase 5A: PI Publication Retrieval [3/5 professors]" displayed
- Validates: AC 7 - Progress tracking

### Success Criteria

- ✅ 100% of valid MCP queries return structured Publication objects
- ✅ Missing publications logged but don't crash pipeline
- ✅ Name validation filters correctly identify professor's papers (>95% precision)
- ✅ All required Publication fields populated for successful retrievals
- ✅ Optional fields (abstract, DOI) default to None/empty string when missing
- ✅ Checkpoint files are valid JSONL and reloadable
- ✅ Progress indicators update correctly during batch processing
- ✅ Test coverage >80% for publication_retrieval.py module

### Special Testing Considerations

**MCP Mocking:**
```python
# Example: Mock paper-search-mcp response
@pytest.fixture
def mock_mcp_response():
    return {
        "publications": [
            {
                "title": "Deep Learning for Robotics",
                "authors": ["Jane Smith", "John Doe"],
                "journal": "Nature Robotics",
                "year": 2024,
                "abstract": "We present a novel approach...",
                "doi": "10.1038/s41586-024-00001-0"
            }
        ]
    }

@pytest.mark.asyncio
async def test_fetch_publications(mocker, mock_mcp_response):
    mocker.patch('src.utils.mcp_client.query_paper_search',
                 return_value=mock_mcp_response)

    agent = PublicationRetrievalAgent()
    pubs = await agent.fetch_publications("Jane Smith", "Stanford", 3)

    assert len(pubs) == 1
    assert pubs[0].title == "Deep Learning for Robotics"
    assert pubs[0].year == 2024
```

**Name Collision Testing:**
- Create test fixtures with multiple "J. Smith" authors from different universities
- Validate fuzzy matching logic correctly identifies target professor
- Test edge cases: middle initials, name variations (Jane vs J.)

**Integration Test Data:**
- Use real but anonymized test data for integration tests
- Keep sample publication count small (5-10) to avoid slow tests
- Store in `tests/fixtures/sample_publications.json`

**Performance Testing:**
- Ensure batch of 20 professors completes within reasonable time (< 5 minutes)
- MCP rate limiting doesn't cause excessive delays
- Checkpoint writes don't block processing

## Dev Notes

### Source Tree Location
- Create: `src/agents/publication_retrieval.py`
- Create: `src/models/publication.py`
- Use: `src/utils/mcp_client.py` (from Story 1.5)

**MCP Usage Pattern (Claude Agent SDK):**
```python
from src.utils.mcp_client import get_mcp_server_config
from claude_agent_sdk import ClaudeAgentOptions, AgentDefinition

# Configure SDK with MCP server
mcp_config = get_mcp_server_config()
options = ClaudeAgentOptions(
    mcp_servers=mcp_config,
    allowed_tools=["mcp__papers__search_papers", "Read", "Write"]
)

# Define agent that uses MCP tool
publication_agent = AgentDefinition(
    description="Retrieves publications via paper-search-mcp",
    prompt=f"""Query paper-search-mcp for publications:
    Author: {professor.name}
    Affiliation: {university_name}
    Years: 3

    Use tool: mcp__papers__search_papers
    """,
    tools=["mcp__papers__search_papers", "Read", "Write"],
    model="sonnet"
)

# SDK handles MCP server spawning and communication
```

**Note:** No manual `query_paper_search()` function needed - SDK handles MCP communication.

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |
| 2025-10-06 | 0.2 | Updated with SDK-based MCP usage pattern (no manual wrapper functions needed) | Bob (SM) |
