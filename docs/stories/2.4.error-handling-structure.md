# Story 2.4: Error Handling for Missing Structure Data

## Status

**Draft**

## Story

**As a** user,
**I want** the system to handle cases where department structure is unclear or incomplete,
**so that** analysis continues with graceful degradation instead of failing.

## Acceptance Criteria

1. System identifies when structure discovery is incomplete or ambiguous (NFR13)
2. User notified of structure gaps with specific details
3. Partial structure data saved and used where available
4. Fallback: Process all discoverable departments even if hierarchy unclear
5. Data quality issues flagged in output (FR32)
6. Detailed logging of what couldn't be discovered (NFR17)

## Tasks / Subtasks

- [ ] **Task 1: Detect Incomplete Structure Discovery** (AC: 1)
  - [ ] Check if university core website returned valid HTML
  - [ ] Validate discovered structure has at least 1 department
  - [ ] Identify ambiguous hierarchies (missing school/division names)
  - [ ] Detect missing URLs for departments
  - [ ] Flag departments with incomplete metadata
  - [ ] Log all detection findings with WARNING level

- [ ] **Task 2: Implement Graceful Degradation for Missing Data** (AC: 3, 4)
  - [ ] If school name missing: Use "Unknown School" placeholder
  - [ ] If division name missing: Set to None (acceptable for flat structures)
  - [ ] If department URL missing: Flag but continue processing
  - [ ] If hierarchy unclear: Assume flat structure (all hierarchy_level=0)
  - [ ] Never fail entire discovery due to partial data
  - [ ] Continue with best available information

- [ ] **Task 3: Add Data Quality Flags to Department Model** (AC: 5)
  - [ ] Add `data_quality_flags: list[str]` field to Department model
  - [ ] Flag types:
    - `"missing_url"` - Department URL not found
    - `"missing_school"` - School name unknown
    - `"ambiguous_hierarchy"` - Hierarchy level unclear
    - `"partial_metadata"` - Some fields missing
    - `"inference_based"` - Information inferred not scraped
  - [ ] Include flags in checkpoint and JSON output
  - [ ] Display flags in user-facing reports

- [ ] **Task 4: Create Structure Gap Report for User** (AC: 2)
  - [ ] Create `output/structure-gaps.md` report
  - [ ] List all departments with data quality issues
  - [ ] Provide specific details for each gap:
    - What data is missing
    - Why it couldn't be discovered
    - What fallback was used
  - [ ] Include recommendations: "Consider manually verifying these departments"
  - [ ] Summary statistics: "X of Y departments have incomplete data"

- [ ] **Task 5: Implement Fallback for Completely Failed Discovery** (AC: 4)
  - [ ] If web scraping completely fails: Check for manual fallback config
  - [ ] Allow user to provide manual department list in config
  - [ ] Manual format: `departments_fallback.json` with basic structure
  - [ ] Load manual list if automated discovery fails
  - [ ] Flag all manual entries with `"manual_entry"` data quality flag
  - [ ] Log: "Automated discovery failed, using manual department list"

- [ ] **Task 6: Comprehensive Error Logging** (AC: 6)
  - [ ] Use structured logger from Story 1.7
  - [ ] Log all discovery failures with ERROR level
  - [ ] Log partial data issues with WARNING level
  - [ ] Include context: university URL, specific failure reason, attempted fallbacks
  - [ ] Example log entry:
    ```json
    {
      "level": "WARNING",
      "component": "university_discovery",
      "issue": "missing_department_url",
      "department": "Computer Science",
      "school": "Engineering",
      "attempted_selectors": ["a.dept-link", ".department-url"],
      "fallback_used": "marked_for_manual_verification"
    }
    ```

- [ ] **Task 7: Add Retry Logic for Transient Failures** (AC: 1)
  - [ ] Distinguish transient failures (network timeout) from permanent (page not found)
  - [ ] Use tenacity retry with exponential backoff for transient failures
  - [ ] Max 3 retries per page
  - [ ] Log each retry attempt
  - [ ] After max retries, apply graceful degradation
  - [ ] Never crash pipeline on retryable failures

- [ ] **Task 8: Validate Discovered Structure Before Proceeding** (AC: 1, 3)
  - [ ] Implement `validate_department_structure(departments: list[Department]) -> ValidationResult`
  - [ ] Check minimum requirements:
    - At least 1 department discovered
    - At least 50% of departments have URLs
    - At least 50% of departments have school names
  - [ ] If validation fails: Surface error to user with details
  - [ ] If validation passes with warnings: Proceed but notify user
  - [ ] Save validation results to `output/structure-validation.json`

## Dev Notes

### Relevant Architecture Information

**Component:** University Structure Discovery Agent - Error Handling Module (Epic 2)

**Responsibility:** Graceful degradation for incomplete structure data (Epic 2, NFR13, FR32)

**Key Interfaces:**
- `validate_department_structure(departments: list[Department]) -> ValidationResult`
- `apply_fallback_strategy(departments: list[Department]) -> list[Department]`

**Dependencies:**
- Structured Logger for error and warning logging (Story 1.7)
- Checkpoint Manager for saving partial results (Story 1.7)
- Department Pydantic model with data_quality_flags (Story 2.1, updated in 2.4)

**Technology Stack:**
- tenacity for retry logic with exponential backoff
- structlog for structured error logging
- Pydantic for data validation

**Source Tree Location:**
- Modify: `src/agents/university_discovery.py` (add error handling logic)
- Modify: `src/models/department.py` (add data_quality_flags field)
- Create: `output/structure-gaps.md` (gap report)
- Create: `output/structure-validation.json` (validation results)
- Optional: `config/departments_fallback.json` (manual fallback)

**Updated Department Model:**
```python
class Department(BaseModel):
    id: str  # Unique identifier (generated)
    name: str  # Department name
    school: Optional[str] = None  # Parent school/college
    division: Optional[str] = None  # Parent division
    url: str  # Department homepage URL
    hierarchy_level: int  # Depth in organizational tree
    is_relevant: bool = False  # Result of relevance filtering (Story 2.3)
    relevance_reasoning: str = ""  # LLM explanation (Story 2.3)
    data_quality_flags: list[str] = []  # Data quality issues (Story 2.4)
```

**Data Quality Flags:**
- `missing_url` - Department URL not found
- `missing_school` - School name unknown (using placeholder)
- `ambiguous_hierarchy` - Hierarchy level unclear
- `partial_metadata` - Some fields missing
- `inference_based` - Information inferred not scraped
- `manual_entry` - From manual fallback config
- `scraping_failed` - Web scraping failed, using fallback

**Error Handling Pattern (from Architecture):**
- **Retry Policy:** Exponential backoff with jitter (tenacity library)
  - Max retries: 3
  - Initial delay: 1 second
  - Backoff multiplier: 2x
  - Max delay: 10 seconds
- **Timeout Configuration:** Web scraping: 30 seconds per page
- **Error Translation:**
  - HTTP 404 (Not Found) → Flag as missing data, continue
  - Network timeouts → Retry with exponential backoff
  - HTML parsing failures → Apply fallback selectors, then flag

**Fallback Strategy Decision Tree:**
```
1. Try primary web scraping (built-in tools)
   ↓ (failure)
2. Try Playwright fallback
   ↓ (failure)
3. Apply graceful degradation:
   - Use partial data if available
   - Apply placeholders for missing fields
   - Flag with data_quality_flags
   ↓ (complete failure)
4. Check for manual fallback config
   ↓ (not found)
5. Skip department with ERROR log, continue with others
```

**Critical Rules (from Coding Standards):**
- Never crash pipeline on missing data - always degrade gracefully
- Always log failures with sufficient context for debugging
- Use data quality flags to surface issues to user
- Never use print() for logging (use structlog)

**Architecture Component Diagram Flow:**
```
University Discovery Agent → Web Scraping (with retries)
  ↓ (on failure)
University Discovery Agent → Fallback Strategy
  ↓
University Discovery Agent → Data Quality Flags (mark issues)
  ↓
University Discovery Agent → Logger (log all failures)
  ↓
University Discovery Agent → File System (save gap report)
```

### Testing

**Test File Location:** `tests/unit/test_error_handling_structure.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Mock web scraping failures
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Test detection of incomplete structure (missing fields)
2. Test graceful degradation (partial data handling)
3. Test data quality flag application
4. Test structure gap report generation
5. Test manual fallback config loading
6. Test retry logic for transient failures
7. Test validation of discovered structure
8. Test error logging for all failure scenarios

**Example Test Pattern:**
```python
def test_graceful_degradation_missing_url(mocker):
    # Arrange
    mock_scrape = mocker.patch('src.agents.university_discovery.scrape_page')
    mock_scrape.return_value = {"name": "CS", "school": "Engineering", "url": None}

    agent = UniversityDiscoveryAgent()

    # Act
    departments = agent.discover_structure("https://test.edu")

    # Assert
    assert len(departments) == 1
    assert departments[0].name == "CS"
    assert departments[0].url == ""  # Fallback empty string
    assert "missing_url" in departments[0].data_quality_flags
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
