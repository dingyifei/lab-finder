# Story 3.1c: Deduplication + Rate Limiting + Checkpointing

## Status

**Approved**

## Story

**As a** user,
**I want** discovered professors deduplicated with rate-limited scraping and saved to checkpoints,
**so that** I have a clean, unique professor list ready for filtering.

## Acceptance Criteria

1. Professors deduplicated by name + department combination
2. Fuzzy name matching handles variations (J. Smith vs Jane Smith) using LLM
3. Duplicate threshold: confidence score 90+ considered duplicate
4. Rate limiting prevents being blocked by university servers
5. Results saved to checkpoint after processing complete
6. Checkpoint uses JSONL format for streaming support

## Dependencies

**Must Be Complete Before Starting:**
- **Story 3.1b** - Provides parallel professor discovery results
- **Story 1.4**: Shared Utilities Implementation (provides `llm_helpers`, `checkpoint_manager`)

**Blocks:**
- **Story 3.2** - Professor Filtering (uses deduplicated professor list)

## Tasks / Subtasks

- [ ] **Task 0: Verify Dependencies** (Prerequisites)
  - [ ] Confirm `src/agents/professor_filter.py` exists (from Story 3.1a)
  - [ ] Confirm `discover_professors_for_department()` function exists (Story 3.1a)
  - [ ] Confirm `discover_professors_parallel()` function exists (Story 3.1b)
  - [ ] If missing: HALT and notify user that Stories 3.1a/3.1b must be completed first

- [ ] **Task 1: Implement Rate Limiting** (AC: 4)
  - [ ] Use aiolimiter.AsyncLimiter for request throttling
  - [ ] Load rate limits from system_params (see `src/models/config.py`)
  - [ ] Apply limiter to both Claude SDK queries and Playwright requests
  - [ ] Example: `async with rate_limiter: await scrape_function()`
  - [ ] Create per-domain rate limiter: 1 request/second default

- [ ] **Task 2: Integrate Rate Limiting into Discovery** (AC: 4)
  - [ ] **Integration Point:** Modify `discover_professors_parallel()` function from Story 3.1b
  - [ ] Add rate limiting BEFORE calling `discover_professors_for_department()`
  - [ ] Update the `process_with_semaphore()` inner function to include rate limiter acquisition
  - [ ] Example: `await rate_limiter.acquire(dept.url)` before discovery call
  - [ ] Ensure rate limiting works correctly with asyncio.Semaphore from 3.1b
  - [ ] Log rate limiting pauses: "Rate limit applied for domain {domain}, waiting..."
  - [ ] **Note:** This modifies code written in Story 3.1b, not creating new functions

- [ ] **Task 3: Aggregate and Deduplicate Professor Results** (AC: 1, 2, 3)
  - [ ] Flatten results from parallel processing (from Story 3.1b)
  - [ ] Deduplicate by professor name + department combination
  - [ ] Use `llm_helpers.match_names(name1: str, name2: str) -> dict` for fuzzy matching
  - [ ] **CRITICAL:** Check BOTH `decision == "yes"` AND `confidence >= 90` to confirm duplicate
  - [ ] Threshold: confidence score 90+ considered duplicate (only when decision is "yes")
  - [ ] If duplicates found: Merge information (prefer more complete records)

- [ ] **Task 4: Save Professor List to Checkpoint** (AC: 5, 6)
  - [ ] **Batch ID Strategy:** Use `batch_id=1` for single consolidated professor list (recommended for <500 professors)
    - All professors saved in one batch after complete deduplication
    - Alternative: If implementing batch-based deduplication, use sequential batch IDs (1, 2, 3...)
  - [ ] Use checkpoint_manager.save_batch()
  - [ ] Save to `checkpoints/phase-2-professors-batch-{N}.jsonl`
  - [ ] Use JSONL format for streaming support
  - [ ] Serialize Professor models with `.model_dump()`
  - [ ] Log: "Discovered X professors across Y departments, saved to checkpoint"

## Dev Notes

### Relevant Architecture Information

**Component:** Professor Discovery & Filter Agent (Epic 3)

**Responsibility:** Deduplicate professors using LLM-based name matching; apply rate limiting; save to checkpoints.

**Key Interfaces:**
- `deduplicate_professors(professors: list[Professor]) -> list[Professor]` - LLM-based deduplication
- `rate_limited_discovery(department: Department) -> list[Professor]` - Discovery with rate limiting

**Dependencies:**
- Parallel discovery results from Story 3.1b
- LLM Helpers for name matching (see `src/utils/llm_helpers.py::match_names()`)
- Checkpoint Manager for saving professor data (see `src/utils/checkpoint_manager.py`)
- aiolimiter for rate limiting web scraping requests
- SystemParams model for configuration (see `src/models/config.py`)

**Technology Stack:**
- aiolimiter 1.2.1 for async rate limiting
- LLM-based name matching via llm_helpers
- Checkpoint Manager JSONL format
- structlog for deduplication logging

**Source Tree Location:**
- Modify: `src/agents/professor_filter.py` (add deduplication, rate limiting to existing parallel functions)
- Save to: `checkpoints/phase-2-professors-batch-1.jsonl` (single batch recommended)
- Use: `src/utils/llm_helpers.py` (match_names function - ALREADY IMPLEMENTED ✅)
- Use: `src/utils/checkpoint_manager.py`
- Use: `aiolimiter` (AsyncLimiter from dependencies)

**File Modification Sequence (Cross-Story Tracking):**
1. **Story 3.1a:** Creates `src/agents/professor_filter.py` with discovery functions
2. **Story 3.1b:** Adds parallel processing functions to same file
3. **Story 3.1c:** Adds deduplication + rate limiting to parallel functions in same file
4. **Story 3.2:** Will add filtering functions to same file (professor research field filtering)

### Rate Limiting Implementation

**Pattern: aiolimiter with per-domain throttling**

```python
from aiolimiter import AsyncLimiter
from urllib.parse import urlparse

class DomainRateLimiter:
    """Per-domain rate limiting to prevent blocking."""

    def __init__(self):
        self.limiters = {}  # domain -> AsyncLimiter
        self.default_rate = 1  # 1 request per second default

    async def acquire(self, url: str):
        """Acquire rate limit token for URL's domain."""
        domain = urlparse(url).netloc

        if domain not in self.limiters:
            # Create limiter for new domain
            self.limiters[domain] = AsyncLimiter(
                max_rate=self.default_rate,
                time_period=1.0  # per second
            )

        await self.limiters[domain].acquire()

# Usage
rate_limiter = DomainRateLimiter()

async def rate_limited_discovery(department: Department, correlation_id: str):
    await rate_limiter.acquire(department.url)
    return await discover_professors_for_department(department, correlation_id)
```

**Integration with Parallel Processing (Story 3.1b Modification):**

This story **MODIFIES** the `discover_professors_parallel()` function implemented in Story 3.1b by adding rate limiting to the `process_with_semaphore()` inner function:

```python
# MODIFIED function from Story 3.1b - add rate limiting
async def discover_professors_parallel(
    departments: list[Department],
    max_concurrent: int = 5
) -> list[Professor]:
    """Parallel discovery with rate limiting integration."""
    # ... setup code from 3.1b ...

    # CREATE: Initialize rate limiter (NEW in 3.1c)
    rate_limiter = DomainRateLimiter()

    async def process_with_semaphore(dept: Department, index: int) -> list[Professor]:
        """Process single department with BOTH semaphore and rate limiting."""
        nonlocal completed_count

        async with semaphore:
            # ADD: Rate limit before processing (NEW in 3.1c)
            await rate_limiter.acquire(dept.url)

            # Existing code from 3.1b continues here...
            correlation_id = f"prof-disc-{dept.id}-{uuid.uuid4().hex[:8]}"
            # ... rest of function unchanged ...
```

**Key Point:** Story 3.1c adds rate limiting to existing parallel processing, not creating new parallel execution patterns.

### Deduplication with LLM Name Matching

**Pattern: Using llm_helpers.match_names()**

**Function Status:** ✅ **ALREADY IMPLEMENTED** in `src/utils/llm_helpers.py::match_names()`

The function returns a dict:
```python
{
    "decision": "yes" | "no",  # lowercase
    "confidence": 0-100,
    "reasoning": "explanation"
}
```

**Deduplication Implementation:**

```python
from src.utils.llm_helpers import match_names

async def deduplicate_professors(professors: list[Professor]) -> list[Professor]:
    """
    Deduplicate professors using fuzzy name matching.
    Uses llm_helpers.match_names() for LLM-based name matching.

    Performance Note: O(n²) complexity with LLM calls for fuzzy matching.
    For large professor lists:
    - < 50 professors: Use naive O(n²) approach (acceptable performance)
    - 50-200 professors: Add exact-match pre-filtering (reduces LLM calls by ~70%)
    - 200+ professors: Consider batching fuzzy matches OR caching within departments
    - 500+ professors: Use batch-based deduplication (as mentioned in Task 4)
    """
    logger = get_logger(correlation_id="deduplication", phase="professor_discovery")

    unique_professors = []
    seen_combinations = set()

    for prof in professors:
        # Exact match check first (fast)
        key = f"{prof.name.lower()}:{prof.department_id}"
        if key in seen_combinations:
            logger.debug("Exact duplicate found", professor=prof.name)
            continue

        # Fuzzy match check (slower, uses LLM)
        is_duplicate = False
        for existing in unique_professors:
            if existing.department_id == prof.department_id:
                # Use LLM helper for name similarity
                match_result = await match_names(existing.name, prof.name)

                # Check BOTH decision AND confidence to confirm duplicate
                if match_result["decision"] == "yes" and match_result["confidence"] >= 90:
                    logger.debug("Fuzzy duplicate found",
                               name1=existing.name,
                               name2=prof.name,
                               confidence=match_result["confidence"],
                               reasoning=match_result["reasoning"])

                    # Merge data (prefer more complete record)
                    merged = merge_professor_records(existing, prof)
                    unique_professors.remove(existing)
                    unique_professors.append(merged)
                    is_duplicate = True
                    break

        if not is_duplicate:
            seen_combinations.add(key)
            unique_professors.append(prof)

    logger.info("Deduplication complete",
               original_count=len(professors),
               unique_count=len(unique_professors),
               duplicates_removed=len(professors) - len(unique_professors))

    return unique_professors


def merge_professor_records(existing: Professor, new: Professor) -> Professor:
    """Merge two professor records, preferring more complete data."""
    merged_data = existing.model_dump()
    new_data = new.model_dump()

    # Prefer non-empty values
    for key in new_data:
        if key == "data_quality_flags":
            # Merge flags
            merged_data[key] = list(set(merged_data[key] + new_data[key]))
        elif not merged_data.get(key) and new_data.get(key):
            merged_data[key] = new_data[key]

    return Professor(**merged_data)
```

### Checkpoint Saving

**Pattern: Using checkpoint_manager**

```python
from src.utils.checkpoint_manager import CheckpointManager

checkpoint_manager = CheckpointManager(checkpoint_dir="checkpoints")

# Save professors to checkpoint
def save_professors_to_checkpoint(professors: list[Professor], batch_id: int = 1):
    """Save professor list to JSONL checkpoint."""
    checkpoint_manager.save_batch(
        phase="phase-2-professors",
        batch_id=batch_id,
        data=professors  # List of Pydantic models
    )

    logger.info("Saved professors to checkpoint",
               count=len(professors),
               batch_id=batch_id,
               file=f"checkpoints/phase-2-professors-batch-{batch_id}.jsonl")
```

### Testing

**Test File Location:** `tests/integration/test_professor_discovery.py`

**Testing Standards:**
- Framework: pytest 8.4.2
- Async tests: pytest-asyncio
- Coverage requirement: 70% minimum

**Test Requirements:**

1. **Test deduplication with match_names**
   ```python
   @pytest.mark.asyncio
   async def test_deduplication_fuzzy_matching(mocker):
       # Mock llm_helpers.match_names to return dict with correct lowercase "yes"
       mocker.patch('src.utils.llm_helpers.match_names',
                    return_value={"decision": "yes", "confidence": 95, "reasoning": "Same person"})

       professors = [
           Professor(id="p1", name="Dr. Jane Smith", department_id="d1", profile_url="test"),
           Professor(id="p2", name="Jane A. Smith", department_id="d1", profile_url="test")
       ]

       unique = await deduplicate_professors(professors)

       # Should merge to 1 professor (decision="yes" AND confidence >= 90)
       assert len(unique) == 1
       assert unique[0].name in ["Dr. Jane Smith", "Jane A. Smith"]  # Merged record
   ```

2. **Test rate limiting enforcement**
   ```python
   @pytest.mark.asyncio
   async def test_rate_limiting_delays_requests(mocker):
       import time

       rate_limiter = DomainRateLimiter()
       rate_limiter.default_rate = 2  # 2 per second

       start = time.time()

       # Make 5 requests to same domain
       for i in range(5):
           await rate_limiter.acquire("https://example.edu/dept")

       duration = time.time() - start

       # Should take at least 2 seconds for 5 requests at 2/sec
       assert duration >= 2.0
   ```

3. **Test checkpoint saving**
   ```python
   def test_save_professors_checkpoint(tmp_path):
       checkpoint_manager = CheckpointManager(checkpoint_dir=tmp_path)
       professors = [Professor(id="p1", name="Jane", title="Prof",
                              department_id="d1", department_name="CS",
                              profile_url="test")]

       checkpoint_manager.save_batch(phase="phase-2-professors", batch_id=1, data=professors)

       # Verify JSONL file created
       checkpoint_file = tmp_path / "phase-2-professors-batch-1.jsonl"
       assert checkpoint_file.exists()
   ```

4. **Test exact vs fuzzy deduplication**
   ```python
   @pytest.mark.asyncio
   async def test_exact_duplicate_skips_llm(mocker):
       # Mock match_names - should NOT be called for exact duplicates
       mock_match = mocker.patch('src.utils.llm_helpers.match_names')

       professors = [
           Professor(id="p1", name="Jane Smith", department_id="d1", profile_url="test"),
           Professor(id="p2", name="Jane Smith", department_id="d1", profile_url="test")  # Exact duplicate
       ]

       unique = await deduplicate_professors(professors)

       # Should detect exact duplicate without calling LLM
       mock_match.assert_not_called()
       assert len(unique) == 1
   ```

5. **Test merge prefers complete records**
   ```python
   def test_merge_professor_records():
       existing = Professor(
           id="p1", name="Jane", title="Prof", department_id="d1",
           department_name="CS", profile_url="test",
           email=None, research_areas=[]
       )
       new = Professor(
           id="p2", name="Jane", title="Prof", department_id="d1",
           department_name="CS", profile_url="test",
           email="jane@edu", research_areas=["AI", "ML"]
       )

       merged = merge_professor_records(existing, new)

       # Should take email and research_areas from new
       assert merged.email == "jane@edu"
       assert "AI" in merged.research_areas
   ```

6. **Test high-confidence "no" match is not treated as duplicate (Critical Edge Case)**
   ```python
   @pytest.mark.asyncio
   async def test_high_confidence_no_match_not_treated_as_duplicate(mocker):
       """Verify high-confidence 'no' decisions don't cause incorrect merges."""

       # Mock: High confidence that these are DIFFERENT people
       mocker.patch('src.utils.llm_helpers.match_names',
                    return_value={"decision": "no", "confidence": 95,
                                 "reasoning": "Different people with similar names"})

       professors = [
           Professor(id="p1", name="Jane Smith", department_id="d1", profile_url="test1"),
           Professor(id="p2", name="Jane A. Smith", department_id="d1", profile_url="test2")
       ]

       unique = await deduplicate_professors(professors)

       # Should NOT merge despite high confidence (because decision is "no")
       assert len(unique) == 2
   ```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | 0.1 | Split from Story 3.1 - Deduplication, rate limiting, and checkpointing | Bob (SM) |
| 2025-10-07 | 0.2 | **PO Validation Fixes:** Fixed critical deduplication logic error (added decision field check), fixed case sensitivity (Yes/No→yes/no), added Task 0 (dependency verification), improved Task 3/4 clarity, updated Test #1, added Test #6 (high-confidence "no" edge case), added performance thresholds | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

**Files Modified in This Story:**
- **MODIFY:** `src/agents/professor_filter.py` - Add deduplication, rate limiting, and checkpointing
- **CREATE:** Checkpoint file `checkpoints/phase-2-professors-batch-1.jsonl`

**Cross-Story File Tracking for `src/agents/professor_filter.py`:**
- **Story 3.1a:** Created file with discovery functions ✅
- **Story 3.1b:** Added parallel processing functions ✅
- **Story 3.1c (this story):** Adds deduplication + rate limiting + checkpointing
- **Story 3.2 (future):** Will add filtering functions

**Expected Function Additions:**
- `DomainRateLimiter` class - Per-domain rate limiting
- `deduplicate_professors()` - LLM-based name deduplication
- `merge_professor_records()` - Merge duplicate professor data
- Modify `discover_professors_parallel()` - Add rate limiting integration

_Dev agent: Update with actual files modified during implementation_

## QA Results

_To be populated by QA agent_
