# Story 2.5: Batch Configuration for Department Processing

## Status

**Draft**

## Story

**As a** user,
**I want** department processing configured to run in reasonable batches,
**so that** the system doesn't overwhelm resources with parallel operations.

## Acceptance Criteria

1. Configurable batch size for parallel department processing (NFR4)
2. Default batch size set to reasonable value (e.g., 5 departments at a time)
3. Batch size configurable in system parameters JSON
4. Progress tracking shows current batch and remaining batches
5. Checkpointing between batches for resumability (NFR12)

## Tasks / Subtasks

- [ ] **Task 1: Add Batch Configuration to System Parameters** (AC: 1, 2, 3)
  - [ ] Update `config/system-parameters.json` schema to include batch settings
  - [ ] Add fields:
    ```json
    {
      "batch_config": {
        "department_discovery_batch_size": 5,
        "professor_discovery_batch_size": 10,
        "publication_retrieval_batch_size": 20,
        "linkedin_matching_batch_size": 15
      }
    }
    ```
  - [ ] Set defaults for Epic 2 (department_discovery_batch_size: 5)
  - [ ] Validate batch size > 0 and < 100 (reasonable range)
  - [ ] Load batch config in CLI Coordinator initialization

- [ ] **Task 2: Implement Batch Division for Department Processing** (AC: 1, 4)
  - [ ] Create utility function `divide_into_batches(items: list, batch_size: int) -> list[list]`
  - [ ] Divide department list into batches based on config
  - [ ] Calculate total batches: `total_batches = ceil(total_departments / batch_size)`
  - [ ] Return list of batches for sequential processing
  - [ ] Handle edge case: departments < batch_size (single batch)

- [ ] **Task 3: Spawn Sub-Agents per Batch** (AC: 1, 4)
  - [ ] For each batch of departments:
    - Spawn sub-agents for parallel department processing
    - Wait for all sub-agents in batch to complete
    - Aggregate results from batch
    - Save checkpoint before moving to next batch
  - [ ] Limit parallelism to batch size (not all departments at once)
  - [ ] Use Claude Agent SDK agent spawning from architecture
  - [ ] Log: "Processing batch X of Y (departments A-B)"

- [ ] **Task 4: Implement Batch-Level Checkpointing** (AC: 5)
  - [ ] Use checkpoint_manager from Story 1.7
  - [ ] Save checkpoint after each batch completion:
    - `checkpoints/phase-1-departments-batch-1.jsonl`
    - `checkpoints/phase-1-departments-batch-2.jsonl`
    - etc.
  - [ ] Use `checkpoint_manager.save_batch(phase="phase-1", batch_id=X, data=batch_results)`
  - [ ] Enable resumability: Skip already-completed batches on restart
  - [ ] Use `checkpoint_manager.get_resume_point(phase="phase-1")` to identify restart batch

- [ ] **Task 5: Implement Resume from Checkpoint** (AC: 5)
  - [ ] On startup, check for existing checkpoints
  - [ ] Use `checkpoint_manager.get_resume_point("phase-1")` to get first incomplete batch
  - [ ] If resume_point > 0: Load completed batches and skip them
  - [ ] Start processing from resume_point batch
  - [ ] Log: "Resuming from batch X (batches 1-Y already complete)"
  - [ ] If all batches complete: Skip to next epic

- [ ] **Task 6: Integrate Progress Tracking for Batches** (AC: 4)
  - [ ] Use progress_tracker from Story 1.7
  - [ ] Display two-level progress:
    - Overall: "Phase 1: University Discovery [batch X/Y]"
    - Batch: "Batch X: Processing departments 1-5 of 25"
  - [ ] Update progress after each batch completion
  - [ ] Show ETA based on batch completion rate
  - [ ] Final summary: "Phase 1 complete: X departments discovered in Y batches"

- [ ] **Task 7: Add Batch Configuration Documentation** (AC: 3)
  - [ ] Update project README with batch configuration section
  - [ ] Document batch size parameters and defaults
  - [ ] Provide guidance: "Increase batch size for more parallelism, decrease to reduce resource usage"
  - [ ] Include example configurations for different scenarios:
    - Small university: batch_size = 3
    - Large university: batch_size = 10
    - Resource-constrained: batch_size = 2
  - [ ] Warn about rate limiting if batch size too large

- [ ] **Task 8: Test Batch Processing with Different Sizes** (AC: 1, 2)
  - [ ] Test with batch_size = 1 (sequential processing)
  - [ ] Test with batch_size = 5 (default)
  - [ ] Test with batch_size = 20 (high parallelism)
  - [ ] Verify checkpoint creation for each batch
  - [ ] Verify resume works correctly
  - [ ] Monitor resource usage (memory, CPU) at different batch sizes

## Dev Notes

### Relevant Architecture Information

**Component:** CLI Coordinator + University Discovery Agent (Epic 2)

**Responsibility:** Batch-level coordination and checkpointing for department processing (NFR4, NFR12)

**Key Interfaces:**
- `divide_into_batches(items: list, batch_size: int) -> list[list]`
- `process_department_batch(batch: list[Department], batch_id: int) -> list[Department]`

**Dependencies:**
- Checkpoint Manager for batch-level checkpointing (Story 1.7)
- Progress Tracker for batch progress display (Story 1.7)
- System parameters config for batch size settings
- Claude Agent SDK for sub-agent spawning

**Technology Stack:**
- Python math.ceil for batch calculation
- Claude Agent SDK agent spawning
- Checkpoint Manager JSONL batching

**Source Tree Location:**
- Modify: `src/coordinator.py` (add batch processing logic)
- Modify: `src/agents/university_discovery.py` (integrate with batching)
- Modify: `config/system-parameters.json` (add batch_config section)
- Create: `checkpoints/phase-1-departments-batch-*.jsonl` (batch checkpoints)

**Batch Processing Pattern (from Architecture):**

**NFR4 Context:** "Execute professor filtering (FR11) and subsequent lab research/analysis steps (FR14-FR24) in configurable batch sizes to limit concurrent parallel agents and Playwright instances"

**Batch Configuration Structure:**
```json
{
  "batch_config": {
    "department_discovery_batch_size": 5,
    "professor_discovery_batch_size": 10,
    "lab_research_batch_size": 8,
    "publication_retrieval_batch_size": 20,
    "linkedin_matching_batch_size": 15
  },
  "resource_limits": {
    "max_parallel_agents": 20,
    "max_playwright_instances": 5
  }
}
```

**Batch Processing Flow:**
```
1. Load departments from checkpoint/discovery
2. Divide into batches (batch_size from config)
3. For each batch:
   a. Spawn sub-agents for departments in batch (parallel within batch)
   b. Wait for all sub-agents to complete
   c. Aggregate batch results
   d. Save checkpoint: phase-1-departments-batch-{N}.jsonl
   e. Update progress tracker
4. Mark phase complete when all batches done
```

**Resumability Pattern:**
```python
# Pseudocode
resume_batch = checkpoint_manager.get_resume_point("phase-1")
if resume_batch > 0:
    # Load already completed batches
    completed = checkpoint_manager.load_batches("phase-1")
    logger.info(f"Resuming from batch {resume_batch}")

# Process remaining batches
for batch_id in range(resume_batch, total_batches):
    batch_data = batches[batch_id]
    results = process_batch(batch_data)
    checkpoint_manager.save_batch("phase-1", batch_id, results)
```

**Critical Rules (from Coding Standards):**
- Use checkpoint_manager.save_batch() - never write JSONL directly
- Always type hint function signatures
- Never use print() for logging (use structlog)
- Batch sizes must be configurable, never hardcoded

**Architecture Component Diagram Flow:**
```
CLI Coordinator → Configuration Validator (load batch_config)
  ↓
CLI Coordinator → Batch Divider (divide departments into batches)
  ↓
For each batch:
  CLI Coordinator → University Discovery Agent (spawn sub-agents for batch)
  University Discovery Agent → Sub-Agents (parallel processing within batch)
  CLI Coordinator → Checkpoint Manager (save batch results)
  CLI Coordinator → Progress Tracker (update progress)
```

### Testing

**Test File Location:** `tests/integration/test_batch_processing.py`

**Testing Standards:**
- Framework: pytest 7.4.4
- Integration tests with temporary checkpoint directories
- Coverage requirement: 70% minimum

**Test Requirements:**
1. Unit test for divide_into_batches function
2. Test batch processing with different batch sizes
3. Test checkpoint creation for each batch
4. Test resume from checkpoint (partial completion)
5. Test progress tracking for batch processing
6. Integration test with mock department data

**Example Test Pattern:**
```python
def test_batch_processing_with_resume(tmp_path):
    # Arrange
    departments = [Department(id=str(i), name=f"Dept{i}",
                              url=f"https://dept{i}.edu", hierarchy_level=0)
                   for i in range(15)]
    batch_size = 5
    coordinator = CLICoordinator(checkpoint_dir=tmp_path, batch_size=batch_size)

    # Simulate partial completion (first 2 batches done)
    checkpoint_manager.save_batch("phase-1", 0, departments[0:5])
    checkpoint_manager.save_batch("phase-1", 1, departments[5:10])

    # Act
    resume_batch = checkpoint_manager.get_resume_point("phase-1")
    coordinator.process_departments_in_batches(departments, start_batch=resume_batch)

    # Assert
    assert resume_batch == 2  # Should resume from batch 2
    batch_2_file = tmp_path / "checkpoints" / "phase-1-departments-batch-2.jsonl"
    assert batch_2_file.exists()
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-06 | 0.1 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
