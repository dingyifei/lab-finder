# Risk Profile: Story 3.1 - Parallel Professor Discovery

**Date:** 2025-10-07
**Reviewer:** Quinn (Test Architect)
**Story:** 3.1 Parallel Professor Discovery

---

## Executive Summary

- **Total Risks Identified:** 11
- **Critical Risks:** 0
- **High Risks:** 4
- **Medium Risks:** 3
- **Low Risks:** 4
- **Risk Score:** 37/100 ⚠️ (High risk story - significant mitigation required)

**Key Concerns:**
This story involves multiple high-risk areas: web scraping variability across university sites, concurrent Playwright browser management, LLM-based deduplication performance, and business value dependency on discovery coverage. The combination of external dependencies (university websites), parallel async processing, and ML-based operations creates substantial technical and operational risk.

---

## Critical Risks Requiring Immediate Attention

**None identified.** However, 4 high-risk items require careful mitigation before production deployment.

---

## High-Risk Items (Score: 6)

### 1. TECH-003: HTML Parser Brittleness Across University Sites

**Score: 6 (High Risk)**
**Probability:** High (3) - Every university has different HTML structure, markup conventions, and page layouts
**Impact:** Medium (2) - Missing professors, incomplete data, low discovery accuracy

**Description:**
The story specifies trying multiple selector patterns (`.faculty-member`, `.professor-card`, `table.faculty`, etc.), but university websites vary wildly. Even with Playwright fallback, parsing logic may fail to extract professor data from non-standard HTML structures.

**Affected Components:**
- `src/agents/professor_filter.py::discover_professors_for_department()`
- `src/agents/professor_filter.py::discover_with_playwright_fallback()`
- HTML parsing logic with BeautifulSoup

**Mitigation:**
- **Preventive:**
  - Expand selector pattern list to 15-20 common patterns based on research of major university site builders
  - Add fallback to searching for "faculty" or "people" link text when structured selectors fail
  - Implement confidence scoring for scraped data (e.g., "found 0 professors" = low confidence, flag for manual review)
  - Add configurable per-university selector overrides in `university_config.json`
- **Detective:**
  - Log which selector pattern succeeded for each department
  - Track discovery success rate metric (professors found / departments processed)
  - Alert if success rate < 60%
- **Corrective:**
  - Provide manual fallback configuration: `departments_fallback.json` with custom selectors per department
  - Build iterative improvement: collect failed department URLs, add to test suite, improve selectors

**Testing Requirements:**
- **Unit tests:** Test parser with 10+ real university HTML samples (different CMS systems: WordPress, Drupal, custom)
- **Integration tests:** Scrape 5 real university departments end-to-end, assert minimum data quality
- **Negative tests:** Test parser with invalid HTML, missing selectors, empty pages
- **Edge cases:** Single-page departments, multi-page directories with pagination, JavaScript-rendered content

**Residual Risk:** Medium - Some universities with highly custom HTML will still fail
**Owner:** Dev
**Timeline:** Before any production use

---

### 2. TECH-001: Playwright Fallback Resource Exhaustion

**Score: 6 (High Risk)**
**Probability:** Medium (2) - Will occur when WebFetch fails for multiple departments simultaneously
**Impact:** High (3) - System crash, memory exhaustion, hung processes

**Description:**
When Claude SDK WebFetch fails, the code falls back to Playwright browser automation. If many departments trigger Playwright fallback concurrently (configured `max_concurrent=5` in `system_params.json`), each launches a full Chromium browser instance (~100-200MB RAM each). With 5 concurrent browsers, system could consume 1GB+ RAM just for Playwright.

**Affected Components:**
- `src/agents/professor_filter.py::discover_with_playwright_fallback()`
- `asyncio.gather()` with Semaphore concurrency control
- System memory and process management

**Mitigation:**
- **Preventive:**
  - **Add separate Playwright-specific semaphore** limiting concurrent browser instances to 2-3 (lower than general concurrency)
  - Implement browser instance pooling: Launch one persistent browser, create new pages within it
  - Add memory monitoring: Check available RAM before launching Playwright, skip with flag if < 500MB free
  - Configure browser launch with `args=['--disable-dev-shm-usage', '--no-sandbox']` to reduce memory footprint
  - Set explicit timeout for Playwright operations (30s max)
- **Detective:**
  - Log memory usage before/after Playwright operations
  - Track Playwright failure rate (crashes, timeouts)
  - Monitor system memory metrics during phase execution
- **Corrective:**
  - Graceful degradation: If Playwright crashes, flag department with `playwright_failed`, continue with other departments
  - Implement Playwright operation timeout with resource cleanup
  - Add retry with exponential backoff, max 2 retries per department

**Testing Requirements:**
- **Load tests:** Simulate 20 concurrent Playwright requests, verify semaphore limits enforced
- **Resource tests:** Monitor memory consumption during concurrent Playwright execution
- **Failure tests:** Kill browser process mid-operation, verify cleanup and error handling
- **Recovery tests:** Verify system recovers from Playwright crashes without hanging

**Residual Risk:** Low - With proper semaphore limits and memory monitoring, risk contained
**Owner:** Dev
**Timeline:** Before any production use

---

### 3. PERF-001: LLM-Based Deduplication Performance Bottleneck

**Score: 6 (High Risk)**
**Probability:** High (3) - Large universities (e.g., UC Berkeley) could have 150+ professors in relevant departments
**Impact:** Medium (2) - Severe performance degradation, user wait times of 10-30 minutes

**Description:**
Story specifies using `llm_helpers.match_names()` for fuzzy name matching during deduplication. This calls Claude API for each potential duplicate pair. With O(n²) comparison complexity for 150 professors = 11,250 LLM calls worst case. Even with parallel execution, this could take 15-30 minutes.

**Affected Components:**
- `src/agents/professor_filter.py::deduplicate_professors()`
- `src/utils/llm_helpers.py::match_names()`
- Claude API rate limits and latency

**Mitigation:**
- **Preventive:**
  - **Implement fast pre-filter before LLM:** Use Levenshtein distance (difflib.SequenceMatcher) to eliminate obvious non-matches (similarity < 0.5)
  - **Only call LLM for 0.5-0.9 similarity range** (ambiguous cases)
  - **Batch similar names:** Group by department + first letter, only compare within groups (reduces search space)
  - **Cache LLM match results:** Store (name1, name2) → confidence in local dict, skip repeated comparisons
  - **Set max comparison limit:** If > 50 potential matches, use only string similarity, skip LLM
  - Parallelize LLM calls with `asyncio.gather()` (respecting Claude API rate limits)
- **Detective:**
  - Log deduplication performance metrics: total comparisons, LLM calls made, time taken
  - Track cache hit rate
  - Alert if deduplication takes > 5 minutes
- **Corrective:**
  - Add progress indicator during deduplication phase
  - Make LLM deduplication optional (config flag `use_llm_deduplication: false` for speed)

**Testing Requirements:**
- **Performance tests:** Benchmark deduplication with 100, 150, 200 professor lists
- **Unit tests:** Verify pre-filter reduces LLM calls by 80%+
- **Integration tests:** Test cache effectiveness across multiple batches
- **Accuracy tests:** Compare LLM vs string-only deduplication accuracy on known duplicate dataset

**Residual Risk:** Medium - Large universities will still be slow, but under 5 minutes acceptable
**Owner:** Dev
**Timeline:** Before any production use (performance testing critical)

---

### 4. BUS-001: Low Discovery Coverage Reducing User Value

**Score: 6 (High Risk)**
**Probability:** Medium (2) - Web scraping failure rate typically 20-40% across diverse sites
**Impact:** High (3) - User receives incomplete professor list, misses relevant labs/opportunities

**Description:**
If web scraping fails for 40% of departments due to HTML variability, anti-scraping measures, or site downtime, user ends up with only 60% of professors discovered. This directly undermines the core value proposition of the tool.

**Affected Components:**
- Overall system value and user satisfaction
- Scraping logic in `professor_filter.py`
- Data quality flags and reporting

**Mitigation:**
- **Preventive:**
  - Set minimum discovery coverage target: **70% of departments must yield at least 1 professor**
  - Implement coverage validation gate: Halt pipeline and prompt user if < 70% coverage
  - Provide clear feedback: "Discovered X professors from Y/Z departments (A% coverage)"
  - Generate discovery gap report: List failed departments with URLs for manual processing
  - Add manual data import option: Allow user to supplement with CSV of professors
- **Detective:**
  - Track discovery metrics per department: success, partial success, failure
  - Aggregate coverage percentage across all departments
  - Flag universities with < 50% discovery coverage
- **Corrective:**
  - Iterative improvement: User provides feedback on failed departments, dev adds custom selectors
  - Semi-automated fallback: If coverage < 70%, prompt user to manually add professor URLs

**Testing Requirements:**
- **Acceptance tests:** Verify coverage gate triggers at 70% threshold
- **Integration tests:** Test gap report generation with mixed success/failure scenarios
- **E2E tests:** Run against 3 real universities, measure actual discovery coverage

**Residual Risk:** Medium - Even with mitigations, some universities will have low coverage
**Owner:** Dev + Product
**Timeline:** Before any production use

---

## Medium-Risk Items (Score: 4)

### 5. PERF-002: Network Timeout Cascades with Slow University Sites

**Score: 4 (Medium Risk)**
**Probability:** Medium (2) - Some university websites are slow (3-10s load times)
**Impact:** Medium (2) - Phase execution delays, partial failures

**Description:**
University websites vary widely in response time. Sites with 5-10s load times combined with retry logic (3 attempts) could result in 30s+ per department. With 20 departments, total time could exceed 10 minutes.

**Mitigation:**
- Set aggressive timeout: `web_scraping` timeout = 30s (from `system_params.json`)
- Implement timeout for individual WebFetch and Playwright operations
- Use `asyncio.wait_for()` to enforce timeouts
- Track timeout occurrences, log slow departments
- Allow user to configure timeouts per university

**Testing Requirements:**
- Mock slow network responses (5s, 10s delays), verify timeouts enforced
- Test retry logic respects cumulative timeout limits

**Residual Risk:** Low
**Owner:** Dev
**Timeline:** Implementation phase

---

### 6. DATA-001: Professor Deduplication Errors (False Positives/Negatives)

**Score: 4 (Medium Risk)**
**Probability:** Medium (2) - 90% confidence threshold is arbitrary, may not be optimal
**Impact:** Medium (2) - Duplicate professors in results, or legitimate duplicates not merged

**Description:**
LLM-based name matching with 90% confidence threshold could produce false positives (different people marked as duplicates) or false negatives (same person not matched). Examples:
- False positive: "Michael Smith" (Computer Science) and "Michael Smith" (Mathematics) merged incorrectly
- False negative: "Dr. Jane A. Smith" and "J. Smith" not matched despite being same person

**Mitigation:**
- Add department context to deduplication: Only compare professors within same department
- Include additional signals: Email domain, profile URL, research areas for matching confidence
- Make threshold configurable in `system_params.json`: `deduplication_threshold: 0.90`
- Log all deduplication decisions with reasoning for manual review
- Generate deduplication report: "Merged 15 duplicates with 90%+ confidence"

**Testing Requirements:**
- Create test dataset with known duplicates and distinct people
- Measure precision (false positive rate) and recall (false negative rate)
- Test threshold sensitivity: Try 80%, 90%, 95% thresholds

**Residual Risk:** Low - With department-scoped matching and logging, acceptable
**Owner:** Dev
**Timeline:** Implementation phase

---

### 7. SEC-002: XSS from Unsanitized Scraped HTML Content

**Score: 4 (Medium Risk)**
**Probability:** Medium (2) - HTML content stored in checkpoints and potentially displayed
**Impact:** Medium (2) - XSS vulnerability if checkpoint data rendered in web UI

**Description:**
Professor data scraped from university websites (e.g., bio text, research areas) may contain HTML/JavaScript. If stored unsanitized in checkpoints and later displayed in a web UI, could enable XSS attacks.

**Mitigation:**
- **Strip HTML from scraped text:** Use `BeautifulSoup.get_text()` to extract plain text only
- **Validate URLs:** Ensure lab_url and profile_url are valid HTTP(S) URLs
- **Sanitize before storage:** Apply HTML entity encoding to any text fields
- **Defense in depth:** If building web UI later, use output encoding (Jinja2 autoescape)

**Testing Requirements:**
- Unit test: Scrape HTML with `<script>alert('XSS')</script>`, verify stripped from output
- Security test: Inject XSS payload in professor name/bio, verify not executable in checkpoint

**Residual Risk:** Low - If properly sanitized during scraping, risk minimal
**Owner:** Dev
**Timeline:** Implementation phase

---

## Low-Risk Items (Score: 2-3)

### 8. DATA-002: Checkpoint Corruption on Write Interruption

**Score: 3 (Low Risk)**
**Probability:** Low (1) - Requires process kill during write operation
**Impact:** High (3) - Lost progress, must re-run entire phase

**Mitigation:**
- Use atomic write pattern: Write to `.tmp` file, rename on success
- Implement checkpoint validation: Load and verify JSONL structure after write
- Add phase progress indicator: Show "Saved checkpoint batch 3/8"

**Residual Risk:** Very Low
**Owner:** Dev
**Timeline:** Implementation phase

---

### 9. SEC-001: SSRF via Unvalidated Department URLs

**Score: 3 (Low Risk)**
**Probability:** Low (1) - Department URLs validated in Epic 2, unlikely to be malicious
**Impact:** High (3) - Could access internal network resources (e.g., `http://192.168.1.1/admin`)

**Mitigation:**
- Validate URLs: Ensure scheme is `http://` or `https://`, reject `file://`, `ftp://`
- Blocklist private IP ranges: Reject URLs pointing to 10.x, 172.16.x, 192.168.x, 127.x
- Use URL parsing: `urllib.parse.urlparse()` to extract and validate components

**Testing Requirements:**
- Security test: Attempt to scrape `http://127.0.0.1:8080`, verify rejected
- Test blocklist: Try private IP ranges, ensure blocked

**Residual Risk:** Very Low
**Owner:** Dev
**Timeline:** Implementation phase

---

### 10. TECH-002: Async Race Conditions in Semaphore Handling

**Score: 2 (Low Risk)**
**Probability:** Low (1) - Python asyncio is well-tested and mature
**Impact:** Medium (2) - Could cause incorrect concurrency limits, resource contention

**Mitigation:**
- Use standard Python asyncio.Semaphore (battle-tested library)
- Add concurrency limit logging: "Processing department 5/20 (3 active tasks)"
- Test with varying concurrency limits: 1, 3, 5, 10

**Testing Requirements:**
- Concurrency test: Verify no more than `max_concurrent` tasks run simultaneously
- Use asyncio debugging tools to detect race conditions

**Residual Risk:** Very Low
**Owner:** Dev
**Timeline:** Implementation phase

---

### 11. OPS-001: Lost Correlation IDs Making Debugging Difficult

**Score: 2 (Low Risk)**
**Probability:** Low (1) - Implementation patterns in Dev Notes look solid
**Impact:** Medium (2) - Difficult to trace failures across parallel tasks

**Mitigation:**
- Follow correlation ID pattern from Dev Notes: Generate unique ID per department
- Bind correlation_id to logger context: `get_logger(correlation_id=...)`
- Propagate correlation_id through all function calls
- Include correlation_id in checkpoint metadata

**Testing Requirements:**
- Log inspection test: Verify correlation_id appears in all log messages for a task
- Trace test: Follow single department's processing across all log entries

**Residual Risk:** Very Low
**Owner:** Dev
**Timeline:** Implementation phase

---

## Risk Distribution

### By Category

| Category | Critical | High | Medium | Low | Total |
|----------|----------|------|--------|-----|-------|
| Technical (TECH) | 0 | 2 | 0 | 1 | 3 |
| Performance (PERF) | 0 | 1 | 1 | 0 | 2 |
| Data (DATA) | 0 | 0 | 2 | 1 | 3 |
| Security (SEC) | 0 | 0 | 1 | 1 | 2 |
| Business (BUS) | 0 | 1 | 0 | 0 | 1 |
| Operational (OPS) | 0 | 0 | 0 | 1 | 1 |
| **Total** | **0** | **4** | **4** | **4** | **12** |

### By Component

| Component | Risk Count | Highest Score |
|-----------|------------|---------------|
| Web scraping logic (`discover_professors_for_department`) | 4 | 6 (TECH-003) |
| Playwright fallback (`discover_with_playwright_fallback`) | 2 | 6 (TECH-001) |
| Deduplication (`deduplicate_professors`) | 2 | 6 (PERF-001) |
| Parallel execution (`discover_professors_parallel`) | 2 | 2 (TECH-002) |
| Checkpoint management | 1 | 3 (DATA-002) |

---

## Detailed Risk Register

| Risk ID | Category | Title | Probability | Impact | Score | Priority |
|---------|----------|-------|-------------|--------|-------|----------|
| TECH-003 | Technical | HTML parser brittleness | High (3) | Medium (2) | 6 | High |
| TECH-001 | Technical | Playwright resource exhaustion | Medium (2) | High (3) | 6 | High |
| PERF-001 | Performance | LLM deduplication bottleneck | High (3) | Medium (2) | 6 | High |
| BUS-001 | Business | Low discovery coverage | Medium (2) | High (3) | 6 | High |
| PERF-002 | Performance | Network timeout cascades | Medium (2) | Medium (2) | 4 | Medium |
| DATA-001 | Data | Deduplication errors | Medium (2) | Medium (2) | 4 | Medium |
| SEC-002 | Security | XSS from scraped content | Medium (2) | Medium (2) | 4 | Medium |
| DATA-002 | Data | Checkpoint corruption | Low (1) | High (3) | 3 | Low |
| SEC-001 | Security | SSRF via URLs | Low (1) | High (3) | 3 | Low |
| TECH-002 | Technical | Async race conditions | Low (1) | Medium (2) | 2 | Low |
| OPS-001 | Operational | Lost correlation IDs | Low (1) | Medium (2) | 2 | Low |

---

## Risk-Based Testing Strategy

### Priority 1: High-Risk Tests (Must Complete Before Deployment)

**TECH-003: HTML Parser Brittleness**
- **Test 10 real university HTML samples** from different CMS systems
- **Negative tests:** Invalid HTML, empty pages, missing selectors
- **Edge cases:** Pagination, JavaScript-rendered content, non-English text
- **Success criteria:** 70%+ success rate on diverse sample set

**TECH-001: Playwright Resource Management**
- **Load test:** 20 concurrent Playwright requests, monitor memory
- **Failure test:** Kill browser mid-operation, verify cleanup
- **Resource test:** Verify semaphore limits Playwright instances to 2-3
- **Success criteria:** Memory usage < 1GB, no hung processes

**PERF-001: Deduplication Performance**
- **Benchmark test:** 100, 150, 200 professor deduplication times
- **Optimization test:** Verify pre-filter reduces LLM calls by 80%+
- **Success criteria:** 150 professors deduplicates in < 5 minutes

**BUS-001: Discovery Coverage**
- **E2E test:** Scrape 3 real universities, measure coverage percentage
- **Gate test:** Verify pipeline halts if coverage < 70%
- **Success criteria:** Coverage gate triggers correctly, gap report generated

### Priority 2: Medium-Risk Tests

**PERF-002: Timeout Handling**
- Mock 5s, 10s network delays, verify timeouts enforced
- Test retry logic respects cumulative timeout limits

**DATA-001: Deduplication Accuracy**
- Test known duplicate dataset, measure precision/recall
- Test threshold sensitivity (80%, 90%, 95%)

**SEC-002: XSS Prevention**
- Inject XSS payloads in professor data, verify sanitization
- Test `<script>` tags, HTML entities, URL schemes

### Priority 3: Low-Risk Tests

**Standard functional tests:**
- Professor model validation
- Checkpoint save/load operations
- Correlation ID propagation
- URL validation and blocklist enforcement

---

## Risk Acceptance Criteria

### Must Fix Before Production

1. **TECH-003:** Achieve 70%+ scraping success rate on test university set
2. **TECH-001:** Implement Playwright-specific concurrency limits and memory monitoring
3. **PERF-001:** Optimize deduplication to < 5 minutes for 150 professors
4. **BUS-001:** Implement coverage gate and gap reporting

### Can Deploy with Monitoring

- **PERF-002:** Network timeouts with aggressive timeout settings
- **DATA-001:** Deduplication errors with manual review logging
- **SEC-002:** XSS risks with sanitization and output encoding

### Accepted Risks

- **DATA-002:** Checkpoint corruption (very low probability with atomic writes)
- **SEC-001:** SSRF (low probability, URLs pre-validated in Epic 2)
- **TECH-002:** Async race conditions (standard library, well-tested)
- **OPS-001:** Correlation ID loss (implementation pattern is solid)

---

## Monitoring Requirements

### Performance Metrics

- **Discovery phase execution time** (target: < 10 minutes for 20 departments)
- **Deduplication time** (target: < 5 minutes for 150 professors)
- **Memory usage during Playwright operations** (alert if > 1GB)

### Success Rate Metrics

- **Discovery coverage percentage** (target: > 70%)
- **Scraping success rate by method:** WebFetch vs Playwright
- **Deduplication accuracy:** False positive/negative rates

### Error Rate Metrics

- **WebFetch failure rate** (track reasons: timeout, connection error, parse failure)
- **Playwright crash rate**
- **Checkpoint write failures**

### Alerts

- **Alert if discovery coverage < 70%**
- **Alert if deduplication takes > 5 minutes**
- **Alert if Playwright memory > 1GB**
- **Alert if scraping failure rate > 40%**

---

## Risk Review Triggers

Re-run risk assessment when:

1. **New university added** with significantly different HTML structure
2. **Claude Agent SDK updated** (WebFetch behavior may change)
3. **Playwright version upgraded** (memory footprint may change)
4. **User reports low discovery coverage** (< 60%)
5. **Performance issues observed** (phase takes > 15 minutes)
6. **Security vulnerability discovered** in web scraping dependencies

---

## Recommendations

### Immediate Actions (Before Implementation)

1. **Expand HTML selector pattern library** to 15-20 patterns based on university CMS research
2. **Implement Playwright-specific concurrency limit** (2-3 max concurrent browsers)
3. **Add pre-filter to deduplication** (string similarity before LLM calls)
4. **Define coverage gate** (halt at < 70%) and gap report format

### Development Phase Actions

1. **Test against 10 real university websites** from diverse CMS systems
2. **Benchmark deduplication performance** with 100-200 professor datasets
3. **Implement comprehensive error handling** for all web scraping failures
4. **Add detailed logging** for debugging scraping failures

### Pre-Production Actions

1. **Load test parallel execution** with 20+ departments
2. **Security review** of URL validation and HTML sanitization
3. **Performance validation** on slowest/largest university in test set
4. **Document known limitations** and failed university sites

---

## Conclusion

Story 3.1 presents **significant technical and operational risk** (Risk Score: 37/100) due to the inherent variability of web scraping, resource management challenges with concurrent browser automation, and performance concerns with LLM-based deduplication.

**Critical Success Factors:**
- Robust HTML parser with extensive selector pattern library
- Careful Playwright resource management with separate concurrency limits
- Performance-optimized deduplication with pre-filtering
- Coverage validation gates to ensure user value

**Recommended Approach:**
- Treat this as a **high-risk, high-value story** requiring extra testing attention
- Allocate 30-40% of development time to testing and performance optimization
- Build iterative improvement path: Start with conservative limits, tune based on real usage
- Plan for manual fallback mechanisms (custom selectors, CSV import) for edge cases

With proper risk mitigation and comprehensive testing, this story can deliver substantial value while maintaining acceptable quality levels.

---

**Risk Assessment Confidence:** High (based on detailed story analysis and architectural understanding)

**Next Steps:**
1. Share risk profile with dev team before implementation starts
2. Incorporate mitigation strategies into task breakdown
3. Add risk-focused test scenarios to test plan
4. Schedule mid-implementation risk review after initial scraping tests
